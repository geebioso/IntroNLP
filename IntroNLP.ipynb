{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "**Garren Gaut with contributions from Avishek Kumar**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Table of Contents\n",
    " - [Data Source](#Data-Source:-Reddit-Comments-from-May-2015-in-JSON-format)\n",
    " - [Preprocess](#Preprocess-the-data)\n",
    " - [Supervised Learning](#Comment-Classification:-Supervised-Learning)\n",
    " - [Unsupervised Learning](#Comment-Similarity-and-Clustering:-Unsupervised-Learning)\n",
    " - [Dimensionality Reduction](#Topic-Modeling:-Dimensionality-Reduction )\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Text Analysis is used for summarizing or extracting useful information from large amounts of  unstructured text. Text analysis encompases a large array of tasks: \n",
    "\n",
    "<img src=\"figs/nlp_tasks.png\">\n",
    "\n",
    "<!--\n",
    "Text Analysis can help with the following tasks:\n",
    "\n",
    "* **Searches and information retrieval**: Help find relevant information in large databases such as a systematic literature review. \n",
    "\n",
    "* **Clustering**: Text clustering is organizing text documents into groups that share properties such as theme, author, or type of document. \n",
    "\n",
    "* **Dimensionality Reduction**: Techniques such as topic modeling can summarize a large corpus of text by representing documents as mixtures of topics. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate from one language to another. \n",
    "\n",
    "In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post origniated from and also do topic modeling to categorize posts. \n",
    "-->\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Demos \n",
    "\n",
    " - [Stanford NLP](http://nlp.stanford.edu:8080/corenlp/process)    \n",
    " - [Illinois Cognitive Computation Group](http://cogcomp.cs.illinois.edu/page/demos/)    \n",
    " - [Event Search](http://eventregistry.org/searchEvents)    \n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why is Language Analysis Hard?\n",
    "\n",
    "- Ambiguity of Language\n",
    "- Commonsense knowledge is typically omitted from social communications\n",
    "- Abduction\n",
    "- Language is dynamic \n",
    "- Symbol grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a word? \n",
    "\n",
    "### One-hot Encoding\n",
    "\n",
    "`[0,0,0,...,0,1,0,...,0]`\n",
    "\n",
    "### Distributed Representation \n",
    "\n",
    "`[0.34, 0.51,...,0.49, 0.56, 0.72]`\n",
    "\n",
    "<!-- LSA, word embeddings, vector space embedding --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a document? \n",
    "\n",
    "### Bag-of-words \n",
    "\n",
    "A bag of words is created by summing the one-hot encodings for all words in a document.     \n",
    "\n",
    "\n",
    "<img src=\"figs/scrabble-2133.jpg\" style=\"width: 50%; height: 50%\"/>\n",
    "\n",
    "\n",
    "### Document Embedding \n",
    "\n",
    "Document embeddings can be created by averaging over word embeddings. This assumes that words and document are 'occupying' the same semantic vector space. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "\n",
    "* **Document Classification**: Predicting which class label belongs to a document (in this tutorial comments are documents). \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ingnored. \n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. For instance, separating sentences using punctuation. \n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **Word (type) vs word token**: In text analysis, *word (type)* refers to a unique set of characters. A *word token* is any occurance of a *word (type)*. For example, the sentence 'Oh boy, that boy is impressive!' contains 6 *word tokens*, but only 5 *word (types)*.   \n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "- Cleaning & Tokenization\n",
    "- Stop Word Removal\n",
    "- Normalization: Stemming & Lemmatization\n",
    "- N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Processing Example \n",
    "\n",
    "**Original Sentence**    \n",
    "`Dude! I’m dying to go surfing. Let’s shred the gnar!` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Clearning and Tokenizing**    \n",
    "`[dude,im,dying,to,go,surfing,lets,shred,the,gnar]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stop Word Removal**    \n",
    "`[dying,surfing,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stemming and Lemmatization**    \n",
    "`[dy,surf,shred,gnar]   [die,surf,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams**    \n",
    "`[die,die_surf,surf,surf_shred,shred,shred_gnar,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams using POS**    \n",
    "`[die,surf,shred,shred_gnar,gnar] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing In Scikit Learn \n",
    "\n",
    "<img src=\"figs/sklearn_preprocessing.png\" style=\"width: 75%; height: 75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sci-kit Learn and Nltk\n",
    "\n",
    "We are going to use sci-kit learn and nltk to process a dataset of comments from reddit. We will train machine learning classifiers to predict which subreddit a comment came from, and will cluster comments. First, we process comment text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering, spectral_clustering\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords') # download the latest stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def load_reddit(fname, ls_subreddits=[], MIN_CHAR=30):\n",
    "    \"\"\"\n",
    "    Loads Reddit Comments from a json file based on \n",
    "    whether they are in the selected subreddits and \n",
    "    have more than the MIN_CHARACTERS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        filename\n",
    "    ls_subreddits: ls[str]\n",
    "        list of subreddits to select from \n",
    "    MIN_CHAR: int\n",
    "        minimum number of characters necessary to select\n",
    "        a comment\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    corpus: ls[str]\n",
    "        list of selected reddit comments\n",
    "    subreddit_id: array[int]\n",
    "        np.array of indices that match with the ls_subreddit\n",
    "        index \n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    subreddit_id = []\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile:\n",
    "            dict_reddit_post =  ujson.loads(line)\n",
    "            subreddit = dict_reddit_post['subreddit']\n",
    "            n_characters = len( dict_reddit_post['body'] )\n",
    "            \n",
    "            if ls_subreddits: # check that the list is not empty\n",
    "                in_ls_subreddits = subreddit in ls_subreddits\n",
    "            else:\n",
    "                in_ls_subreddits = True\n",
    "            \n",
    "            grter_than_min = n_characters > MIN_CHAR\n",
    "            \n",
    "            if ( grter_than_min and in_ls_subreddits ):\n",
    "                corpus.append(dict_reddit_post['body'])\n",
    "                subreddit_id.append(subreddit)\n",
    "                \n",
    "    return np.array(corpus), np.array(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Plots the precision and recall as a function \n",
    "    of the percent of data for which we calculate \n",
    "    precision and recall \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: \n",
    "\n",
    "    yprob_: \n",
    "\n",
    "    model_name: \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = roc_auc_score( y_true, y_score)\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "### Data Source: Reddit Comments from May 2015 in JSON format\n",
    "\n",
    "For the superivised learning portion of the tutorial we will being attempting to classify whether reddit comments have come from /r/SucideWatch or /r/depression. These two subreddits are somewhat similiar so it poses a non-trivial challenge for a classifier.\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# grab data from the following subreddits\n",
    "ls_subreddits = ['SuicideWatch', 'depression']\n",
    "[corpus, subreddit_id] = load_reddit('./data/RC_2015-05.json', ls_subreddits, MIN_CHAR=30)\n",
    "\n",
    "# count the number of comments in each subreddit \n",
    "Counter(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "In order to quantify our text, we will have to remove characters that we don't consider parts of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create an expression to remove from data \n",
    "RE_PREPROCESS = re.compile(r\"\"\" \\W + # one or more nonword characters\n",
    "                                |    # the or operator\n",
    "                                \\d+  # the decimal point\"\"\", re.VERBOSE)\n",
    "\n",
    "# remove anything matching RE_PREPROCESS and make lowercase \n",
    "processed_corpus = np.array( [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )\n",
    "#processed_corpus2 = np.array( [ re.sub(RE_PREPROCESS2, ' ', comment).lower() for comment in corpus] )\n",
    "\n",
    "# check out original and preprocessed data \n",
    "num_comment_to_view = 0\n",
    "for i in range(0, num_comment_to_view):\n",
    "    print(\"Original: subreddit: {}, comment: {}\".format(subreddit_id[i],corpus[i]))\n",
    "    print()\n",
    "    print(\"Processed: subreddit: {}, comment: {}\".format(subreddit_id[i],processed_corpus[i]))\n",
    "    \n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 1: Internet Stoic\n",
    "**10 mins**\n",
    "\n",
    "Write your own preprocessing routine that removes **only happy and sad emojis** from each comment in the corpus. Write a loop that prints comments `[1788, 2360, 9679, 18096]` to check that your choice of regular expression preprocess the text as you intended. [Python regular expression documentation](https://docs.python.org/2/library/re.html) will be helpful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "comments_to_check = [1788, 2360, 9679, 18096]\n",
    "\n",
    "RE_PREPROCESS_EMOJI = re.compile(r'(:\\))|(:\\()')\n",
    "\n",
    "emojiless_corpus = np.array( [ re.sub(RE_PREPROCESS_EMOJI, ' ', comment) for comment in corpus] )\n",
    "for i in comments_to_check:\n",
    "    print(\"\\nComment {}:\\n \\n{}\\n{}\".format(i, corpus[i], emojiless_corpus[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Classification: Supervised Learning\n",
    "\n",
    "In this section we are going to train a classifier to properly tag the original subreddit in which the comment appeared. First we split our data into a testing and training set using the first 80% of the data as the training set and the remaining 20% as the testing set. \n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create Training/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#split the data into training and testing sets. \n",
    "#refactor this in the test train-split\n",
    "train_set_size = int(0.8*len(subreddit_id))\n",
    "train_idx = np.arange(0,train_set_size)\n",
    "test_idx = np.arange(train_set_size, len(subreddit_id))\n",
    "\n",
    "train_subreddit_id = subreddit_id[train_idx]\n",
    "train_corpus = processed_corpus[train_idx]\n",
    "\n",
    "test_subreddit_id = subreddit_id[test_idx]\n",
    "test_processed_corpus = processed_corpus[test_idx]\n",
    "test_corpus = corpus[test_idx]\n",
    "\n",
    "print('Training Labels', Counter(subreddit_id[train_idx]))\n",
    "print('Testing Labels', Counter((subreddit_id[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenize and stem to create features\n",
    "\n",
    "Now that we have the data and we have done a bit of preprocessing, we want to create features. We quantify words as **one-hot encodings** and create document word counts by summing encodings over words in a documents. Scikit-learn as a [CountVectorizer method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that makes this step simple. \n",
    "\n",
    "We first create a vectorizer object that stores the frequency of words in each of the documents in a **document-term count matrix**. We will discard very high and low frequency words. For example, the words *the* or *for* may appear often throughout a corpus but contain very little semantic information. Conversely a document may contain obscure words that do not occur anywhere in else in the corpus which could cause models to overfit. These cases are managed by setting a threshold for the Min and Max Document Frequency(DF).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for vectorizer \n",
    "ANALYZER = \"word\" # unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode' \n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) # Range for n-grams \n",
    "MIN_DF = 0.01 # Exclude words that are contained in less that x percent of documents \n",
    "MAX_DF = 0.8  # Exclude words that are contained in more than x percent of documents \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "                            # could add 'token_pattern' argument to specify what denotes a token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**TFIDF (Term Frequency Inverse Document Frequency)** transforms a count matrix--what we created above--into a TFIDF represenation. This is done by reweighting words that occur throughout the entire corpus to a lower weight due to empirically being found to be less discriminative. Intuitively, TFIDF counts the number of times each term occurs in each document and weights each term frequency by how rare that term is in the corpus. Rare words receive higher weight to quantify that rare words are more discriminative. \n",
    "\n",
    "Term frequency (TF) for a word $t$ and document $d$ is computed as:\n",
    "\n",
    "$$\n",
    "\\textrm{tf} (t,d)= \\frac{n_{t,d}}{\\sum\\limits_{t^\\prime \\in \\mathbb{T}} { n_{t^\\prime,d}}}\n",
    "$$\n",
    "\n",
    "where $n_{t,d}$ is the number of times term $t$ occurs in document $d$ and $\\mathbb{T}$ is the set of all terms in the corpus. The inverse document frequency (IDF) for a term and a set of documents $D$ is computed as:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathrm{idf}(t, D) &=  \\log \\frac{|D|}{1 + df(D,t)} \\\\ \n",
    " df(D,t) &= |\\{d \\in D: t \\in d\\}| \n",
    "\\end{align}\n",
    "\n",
    "where $|\\{d \\in D: t \\in d\\}|$ is the number of documents where term $t$ appears. The 1 added in the demoninator prevents division-by-zero. Then TFIDF is just the weighting of each term frequency by the inverse document frequency:\n",
    "\n",
    "$$ \n",
    "{\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)}\n",
    "$$ \n",
    "\n",
    "[TFIDF in sci-kit learn](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) differs from the textbook definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Plot different IDF options\n",
    "document_freq = np.arange(0,10,1)\n",
    "n_d = 10\n",
    "\n",
    "default = np.log((1 + n_d)/(1 + document_freq)) + 1\n",
    "textbook = np.log( n_d / (1 + document_freq))\n",
    "smooth_idf_false = np.log( n_d/document_freq)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(document_freq, default, color='blue',\n",
    "     lw=lw, label='default')\n",
    "plt.plot(document_freq, textbook, color='red',\n",
    "     lw=lw, label='textbook')\n",
    "plt.plot(document_freq, smooth_idf_false, color='green',\n",
    "     lw=lw, label='smooth_idf_false')\n",
    "\n",
    "plt.xlabel('document frequency')\n",
    "plt.ylabel('inverse document frequency')\n",
    "plt.title('IDF')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to weight the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words = vectorizer.transform( test_processed_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 2: Words are *Just Too Long*\n",
    "\n",
    "**20 mins**\n",
    "\n",
    "In text analysis, words are frequently **stemmed** or **lemmatized** in order to reduce the size of the vocubalary and treat words with similar semantic meaning as the same word. Stemming consists of removing parts of words to create a stem. For example, the word tokens `eat, eating, eatery, eaten, eater` would all be stemmed to the word `eat`. The CountVectorizer provides an easy way incorporate stemming into preprocessing via the *tokenizer* parameter which takes a function handle as an argument. For this activity, create a function to pass to the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will stem all words in your corpora. Use the CountVectorizer with this function as an argument to preprocess the corpus (this may take some time). Store the result in a variable called `stemmed_corpus`. Below is an example of a stemmer from the nltk toolkit that will be helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "x = 'eating'\n",
    "stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER \n",
    "def tokenize_and_stem(comment, stemmer = stemmer):\n",
    "    \"\"\"\n",
    "    Takes a reddit comment and stemmer as input\n",
    "    and returns a list of stemmed words\n",
    "    \n",
    "    params: \n",
    "        str comment: comment to stem \n",
    "        nltk.stemmer stemmer: nltk stemmer object \n",
    "        \n",
    "    rtype: ls[str]\n",
    "    \"\"\"\n",
    "    stemmed_comment = [stemmer.stem(word) for word in comment]\n",
    "    return stemmed_comment\n",
    "    \n",
    "vectorizer_stem = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=tokenize_and_stem, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "start_time = time.time()\n",
    "train_bag_of_words_stem = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words_stem = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf_stem = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf_stem = transformer.transform(test_bag_of_words)\n",
    "features_stem = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "# depression corresponds to a 1 \n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(subreddit_id)\n",
    "subreddit_id_binary = le.transform(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit and Evaluate a Supervised Model\n",
    "\n",
    "**Fit** \n",
    "We will fit a regularized logistic regression model that takes a comment as input and produces a subreddit classification (/r/SuicideWatch or /r/depression) as output. \n",
    "\n",
    "### Regularized Logistic Regression \n",
    "\n",
    "Regularized logistic regression solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\bf w}{\\frac{1}{2}||{\\bf w}||_d + C \\sum_{i=1}^{N}{\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})} }$$\n",
    "\n",
    "where $(x_i, y_i)$ are the features and label for the $i$th datapoint, $\\bf w$ is a vector of model coefficients (or weights), $C$ is a regularization parameter, and $d$ specifies a norm (usually l1 or l2). The l1 norm will force coefficients that are not useful for class dicrimination to zero. The loss function $\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})$ is derived from a probabilistic model and is referred to as the logistic loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a regularized logistic regression model  \n",
    "clf = LogisticRegression(penalty='l2')\n",
    "mdl = clf.fit(train_tfidf, \n",
    "              subreddit_id_binary[train_idx])\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Evaluate\n",
    "To evalute how our classifer performed we plot the Receiver Operating Characteristic (ROC) curve. The ROC curve is a way of lookin at overall classifier performance. For each test comment, the classifier produces a score indicating how confident the classifier is that the comment was posted in the depression subreddit. In order to produce classifications from these scores, we need to set a threshold above which a score is converted into a positive classificaiton. The ROC curve plots the false positive and true positive rate for every possible threshold we could set. We find the Area Under the Curve (AUC) which ranges from 0.5 (chance) to 1 (perfect prediction). The dashed line plots chance ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc(subreddit_id_binary[test_idx], y_score[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Activity 3: \n",
    "\n",
    "**20mins** \n",
    "\n",
    "To understand what the model is doing, we want to see which features make the model more likely to classify a comment as a certain class. The CountVectorizer provides a list of features (using the `get_feature_names()` method) and model coefficients can be found in using the `coef_` method. For this activity, make a sorted list or dictionary of features and their importances (i.e., the corresponding coefficient value). Print the 5 most important features for classifying a comment as each of our class labels, SuicideWatch and depression. \n",
    "\n",
    "Note, this is pretty easy since we are using Logistic Regression. Feature importance may require more complicated analysis for other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER \n",
    "coef = mdl.coef_.ravel()\n",
    "\n",
    "dict_feature_importances = dict( zip(features, coef) )\n",
    "orddict_feature_importances = OrderedDict( \n",
    "                                sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "                                # lambda is an anonymous function. in this case, we take the first \n",
    "                                # element of each item of the dictionary as the key to sort by\n",
    "        \n",
    "\n",
    "ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "num_features = 5\n",
    "suicidewatch_features = ls_sorted_features[:5] #SuicideWatch\n",
    "depression_features = ls_sorted_features[-5:] #depression\n",
    "print('SuicideWatch: ',suicidewatch_features)\n",
    "print('depression: ', depression_features[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examine Confident Predictions \n",
    "\n",
    "We want to see which comments the model scores as highly probable to belong to either class. This step can help to look for patterns in model classification and to check for any obvious errors. The predict probability refers to the probablity of a comment belonging to the depression subreddit. Therefore, comments belonging to the SucideWatch subreddit will have a low probablity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# maybe do something with this crazy indexing: this is python not C!\n",
    "num_comments = 5\n",
    "subreddit0_comment_idx = y_score[:,1].argsort()[:num_comments] #SuicideWatch\n",
    "subreddit1_comment_idx = y_score[:,1].argsort()[-num_comments:] #depression\n",
    "\n",
    "# convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([subreddit0_comment_idx, \n",
    "                                               subreddit1_comment_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_subreddit_id[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As can be seen from the comments for the three highest probablities for SucideWatch and depression, the classifier does a good job. Note the last entry in the list of SuicideWatch top comments is miscategorized. This is most likely due to references to depression, depression medication, and psychological treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Similarity and Clustering: Unsupervised Learning\n",
    "\n",
    "The first step in grouping documents is to define a measure of similarity between documents. A common measure of similarity for non-text data is euclidean distance. However the euclidean distance between document-term vectors ignore the length of the document. For example, if document one consists of a single sentence and document two consists of that same sentence repeated multiple times, the euclidean distance will be nonzero. To overcome this issue documents are often compared using cosine similarity, which measures the angle between two count vectors. \n",
    "\n",
    "### Cosine Similarity \n",
    "\n",
    "The cosine similarity between two vectors ${\\bf x}$ and ${\\bf y}$ is defined as: \n",
    "\n",
    "$$\\textrm{cos}({\\bf x},{\\bf y}) = \\frac{{\\bf x} \\bullet {\\bf y}}{||\\bf x|| \\; || \\bf y ||}$$. \n",
    "\n",
    "The common conceptual interpretations of the cosine similarity is the angle between two vectors. In text analysis each document is represented as a point in a $V$-dimensional, where $V$ is the size of the vocabulary. Thus by normalizing we are looking at the percent of a document that each word compromises instead of the raw counts of word tokens. \n",
    "\n",
    "\n",
    "Below is an example illustrating why we use cosine distance. The example computes the euclidean and cosine distances between repeated sentences. \n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['The girl went to the store', \n",
    "             'The girl went to the store. The girl went to the store',\n",
    "             'The girl went to the store. The girl went to the store. The girl went to the store']\n",
    "\n",
    "vectorizer_sim = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=(0,1)\n",
    "                            )\n",
    "\n",
    "sentence_count = vectorizer_sim.fit_transform( sentences ) \n",
    "\n",
    "\n",
    "euclidean_distance = euclidean_distances(sentence_count, Y = None)# sentence_count, Y=None, dense_output=True, 'l2')\n",
    "cosine_distance = cosine_distances(sentence_count, Y=None )\n",
    "\n",
    "print(\"Euclidean Distance\\n\", euclidean_distance.round(2))\n",
    "print(\"\\nCosine Distance\\n\", cosine_distance.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define count vectorizer for clustering \n",
    "#MIN_DF = 0.001\n",
    "#MAX_DF = 0.8\n",
    "vectorizer_clust = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "num_comments = 1000\n",
    "bag_of_words_clust = vectorizer_clust.fit_transform( processed_corpus[0:(num_comments-1)] ) \n",
    "processed_corpus_clust = processed_corpus[0:(num_comments-1)]\n",
    "corpus_clust = corpus[0:(num_comments-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #del corpus_clust[396]\n",
    "# def delete_row_csr(mat, i):\n",
    "#     \"\"\"\n",
    "#     Function to remove a row from a scipy.sparse.csr_matrix. \n",
    "#     Taken from \n",
    "#     http://stackoverflow.com/questions/13077527/is-there-a-numpy-delete-equivalent-for-sparse-matrices\n",
    "    \n",
    "#     params: \n",
    "#         scipy.sparse.csr_matrix mat\n",
    "#         int i: row to delete \n",
    "#     \"\"\"\n",
    "#     if not isinstance(mat, scipy.sparse.csr_matrix):\n",
    "#         raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "#     n = mat.indptr[i+1] - mat.indptr[i]\n",
    "#     if n > 0:\n",
    "#         mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "#         mat.data = mat.data[:-n]\n",
    "#         mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "#         mat.indices = mat.indices[:-n]\n",
    "#     mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "#     mat.indptr[i:] -= n\n",
    "#     mat.indptr = mat.indptr[:-1]\n",
    "#     mat._shape = (mat._shape[0]-1, mat._shape[1])\n",
    "    \n",
    "# corpus_clust = np.delete(corpus_clust, 396)\n",
    "# processed_corpus_clust = np.delete(corpus_clust, 396)\n",
    "# delete_row_csr(bag_of_words_clust, 396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering \n",
    "\n",
    "The idea behind agglomerative clustering is to cluster nearby data points together in a bottom up fashion. Suppose we have $n$ data points $\\{x_1, ..., x_N\\}$. First, we pre-compute the similarity between all data points using our choice of distance metric (affinity in sci-kit learn). We start with a collection of clusters $C$ that contains $N$ singleton clusters (cluster $c_i = {x_i}$). We repeat the following process until we have $K$ clusters: \n",
    "\n",
    "1. Find the closest pair of clusters ( $\\min_{i,j} D(c_i, c_j)$ ). $D$ depends on the similarity values between each element of each cluster.     \n",
    "2. Add a new cluster $c_{i+j}$ to collection $C$ such that $c_{i+j}= c_i \\cup c_j$.    \n",
    "3. Remove clusters $c_i$ and $c_j$ from $C$.    \n",
    "\n",
    "The distance $D$ is defined between clusters. The following distances are supported in sci-kit learn: \n",
    "\n",
    "* The *ward distance* minimizes the variance of the clusters being merged. I.e., it computes the sum of squares error. \n",
    "* The *average* distance uses the average of the distances of each observation of the two sets.\n",
    "* The *complete or maximum* distance uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "\n",
    "\\* Material is from this [tutorial](https://www.youtube.com/watch?v=XJ3194AmH40). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters = 4, affinity=\"cosine\", linkage =\"average\")\n",
    "cluster.fit(bag_of_words_clust.toarray())\n",
    "labels = cluster.labels_\n",
    "\n",
    "label_counter = Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 4: Cosine Deception \n",
    "**20 mins**\n",
    "\n",
    "Use the code above to find clusters in the data using [Agglomerative Clusting](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Examine the comments that are assigned to each cluster using the function `examine_comment_labels` defined below. Notice that the comments in Group 0 are all short comments. Wasn't the point of using cosine distance to prevent short comments from being similar?  \n",
    "\n",
    "See if you can figure out why these comments are being grouped together. What are some ways to fix this error? \n",
    "\n",
    "**Hint**: look at the comments after converting them into bags of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def examine_comment_labels(num_comments_to_view, label_counter, corpus_clust, num_comments):\n",
    "    \"\"\"\n",
    "    Function that prints comments and their cluster labels. The function can print\n",
    "    the raw comments, processed comments, or bag of words. \n",
    "    \n",
    "    params: \n",
    "        int num_comments_to_view: the number of comments to view for \n",
    "            each group\n",
    "        Counter label_counter: contains the cluster labels for each item \n",
    "            in the dataset\n",
    "        numpy.ndarray or scipy.sparse.csr.csr_matrix corpus: the corpus \n",
    "            processed corpus, or bag of words representation of our data \n",
    "        int num_comments: number of comments contained in corpus\n",
    "    \"\"\"\n",
    "    for key, value in label_counter.items():\n",
    "        count = 0\n",
    "        print(\"\\n-------Group{}-------\\n\\n\".format(key))\n",
    "        for i in range(0, num_comments-1):\n",
    "            if (labels[i] == key) & (count < num_comments_to_view):\n",
    "                print(\"Comment {0}: {1}\\n\".format(i, corpus_clust[i]))\n",
    "                #print(\"Comment {0}: {1}\\n\".format(i, processed_corpus[i]))\n",
    "                count += 1\n",
    "\n",
    "                \n",
    "# examine the comments that fall in each group\n",
    "# what defines each group?\n",
    "print(label_counter)\n",
    "num_comments_to_view = 4\n",
    "print('prohibition' in stopwords.words('english'))\n",
    "examine_comment_labels(num_comments_to_view, label_counter, corpus_clust, num_comments-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Dimensionality Reduction\n",
    "\n",
    "In this portion of the tutorial we will be extracting topics in the form of commonly co-occuring words from the corpus of data. A topic model is a generative model of document creation used for automatic summarization of text into topics, or mixtures of words. While many variants exist (e.g., supervised, unsupervised, dynamic, hierarchical, structural), this tutorial will cover the standard topic model. The standard topic model makes three assumptions: \n",
    "\n",
    "- Bag-of-words: semantic meaning can be extracted from word co-occurance\n",
    "- Dimensionality reduction is essential to understanding semantic information \n",
    "- Topics (lower dimensional units) can be expressed as a probability distribution over words \n",
    "\n",
    "\n",
    "<img src=\"figs/topic_model.png\" style=\"width :85%; height: 85%\"/>\n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topic Model IO\n",
    "\n",
    "### Input     \n",
    "\n",
    "Document term count matrix    \n",
    "\n",
    "### Output \n",
    "\n",
    "**Topics**--the probability of each word being expressed in a given topic    \n",
    "**Document Mixtures**--the probability of each topic being expressed in a document  \n",
    "\n",
    "<img src=\"figs/topic_model_io.png\" style=\"width:85%; height:85%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Topics \n",
    "\n",
    "<img src=\"figs/example_topics.png\" style=\"width:85%; height:85%\"/>\n",
    "\n",
    "\\* These topics were extracted from a *supervised* topic model. In this case, the topics correspond to each subreddit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load and Preprocess Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "corpus_all, subreddit_id_all = load_reddit('./data/RC_2015-05.json',MIN_CHAR=250)\n",
    "end = time.time()\n",
    "print('Loading takes {0:2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get rid of punctuation and set to lowercase  \n",
    "start = time.time()\n",
    "processed_corpus_all = [ re.sub( RE_PREPROCESS, ' ', comment).lower() for comment in corpus_all]\n",
    "\n",
    "#tokenzie the words\n",
    "bag_of_words_all = vectorizer.fit_transform( processed_corpus_all ) \n",
    "end = time.time() \n",
    "#grab the features/vocabulary\n",
    "features_all = vectorizer.get_feature_names()\n",
    "print(\"Processing took {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print subreddits and number of topics \n",
    "print(Counter(subreddit_id_all), len(subreddit_id_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit a Topic Model\n",
    "\n",
    "To create our topics we will use Latent Dirichlet Allocation (LDA, i.e., the standard topic model). Both [scikit learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and [gensim](https://radimrehurek.com/gensim/) provide software for fitting LDA. Both modules use [online variational inference](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf) to fit the model, but methods such as [Gibbs sampling](https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/steyvers06probabilistic.pdf) are also popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "N_TOPICS = 50\n",
    "N_TOP_WORDS = 10\n",
    "\n",
    "start = time.time()\n",
    "lda = LatentDirichletAllocation( n_topics = N_TOPICS )\n",
    "doctopic = lda.fit_transform( bag_of_words_all )\n",
    "end = time.time() \n",
    "print(\"Processing took {}s\".format(end- start)) # takes ~72s for 1 file, ~445s for 5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Display the top ten words for each topics \n",
    "\n",
    "Qualitatively examining the topic output is the most common method for assessing fit of a topic model. Ideally, most topics will be interpretable. Topic interpretability tends to increase with the number of documents used during fitting. Since this topic model is unsupervised, topic names (in this case integers $1,...,T$) are arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "    keywords = ', '.join( features_all[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examine Comments\n",
    "\n",
    "We will examine some of the comments and list which topic comprises the largest portion of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_comments = 5\n",
    "for comment_id in range(num_comments):\n",
    "    topic_id = np.argsort(doctopic[comment_id])[::-1][0]\n",
    "    \n",
    "    comment_color = []\n",
    "    comment = corpus_all[comment_id].split()\n",
    "    for word in comment: \n",
    "        if word in top_features:\n",
    "            comment_color.append('\\x1b[31m' + word + '\\x1b[0m')\n",
    "        else:\n",
    "            comment_color.append(word)\n",
    "    print('comment_id:',\n",
    "          comment_id,\n",
    "          subreddit_id_all[comment_id],\n",
    "          'topic_id:',\n",
    "          topic_id,\n",
    "          'keywords:',\n",
    "           ls_keywords[topic_id])\n",
    "    print('---')\n",
    "    print(' '.join(comment_color))\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cool Visualizations \n",
    "    \n",
    "<img src = \"figs/supreme_court_topics.png\" style = \"width:100%;height:100%\"/>\n",
    "\n",
    "[Mental Health Subreddit Topics](http://geebioso.github.io/Subreddit-Topic-Visualization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Return to [TOC](#Table-of-Contents)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
