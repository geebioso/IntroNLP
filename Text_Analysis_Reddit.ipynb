{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "**Garren Gaut with contributions from Avishek Kumar**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Table of Contents\n",
    " - [Data Source](#Data-Source:-Reddit-Comments-from-May-2015-in-JSON-format)\n",
    " - [Preprocess](#Preprocess-the-data)\n",
    " - [Supervised Learning](#Comment-Classification:-Supervised-Learning)\n",
    " - [Unsupervised Learning](#Comment-Similarity-and-Clustering:-Unsupervised-Learning)\n",
    " - [Dimensionality Reduction](#Topic-Modeling:-Dimensionality-Reduction )\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Text Analysis is used for summarizing or extracting useful information from large amounts of  unstructured text. Text analysis encompases a large array of tasks: \n",
    "\n",
    "<img src=\"figs/nlp_tasks.png\">\n",
    "\n",
    "<!--\n",
    "Text Analysis can help with the following tasks:\n",
    "\n",
    "* **Searches and information retrieval**: Help find relevant information in large databases such as a systematic literature review. \n",
    "\n",
    "* **Clustering**: Text clustering is organizing text documents into groups that share properties such as theme, author, or type of document. \n",
    "\n",
    "* **Dimensionality Reduction**: Techniques such as topic modeling can summarize a large corpus of text by representing documents as mixtures of topics. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate from one language to another. \n",
    "\n",
    "In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post origniated from and also do topic modeling to categorize posts. \n",
    "-->\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Demos \n",
    "\n",
    " - [Stanford NLP](http://nlp.stanford.edu:8080/corenlp/process)    \n",
    " - [Illinois Cognitive Computation Group](http://cogcomp.cs.illinois.edu/page/demos/)    \n",
    " - [Event Search](http://eventregistry.org/searchEvents)    \n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why is Language Analysis Hard?\n",
    "\n",
    "- Ambiguity of Language\n",
    "- Commonsense knowledge is typically omitted from social communications\n",
    "- Abduction\n",
    "- Language is dynamic \n",
    "- Symbol grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a word? \n",
    "\n",
    "### One-hot Encoding\n",
    "\n",
    "`[0,0,0,...,0,1,0,...,0]`\n",
    "\n",
    "### Distributed Representation \n",
    "\n",
    "`[0.34, 0.51,...,0.49, 0.56, 0.72]`\n",
    "\n",
    "<!-- LSA, word embeddings, vector space embedding --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a document? \n",
    "\n",
    "### Bag-of-words \n",
    "\n",
    "A bag of words is created by summing the one-hot encodings for all words in a document.     \n",
    "\n",
    "\n",
    "<img src=\"figs/scrabble-2133.jpg\" style=\"width: 50%; height: 50%\"/>\n",
    "\n",
    "\n",
    "### Document Embedding \n",
    "\n",
    "Document embeddings can be created by averaging over word embeddings. This assumes that words and document are 'occupying' the same semantic vector space. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "\n",
    "* **Document Classification**: Predicting which class label belongs to a document (in this tutorial comments are documents). \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ingnored. \n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. For instance, separating sentences using punctuation. \n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **Word (type) vs word token**: In text analysis, *word (type)* refers to a unique set of characters. A *word token* is any occurance of a *word (type)*. For example, the sentence 'Oh boy, that boy is impressive!' contains 6 *word tokens*, but only 5 *word (types)*.   \n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "- Cleaning & Tokenization\n",
    "- Stop Word Removal\n",
    "- Normalization: Stemming & Lemmatization\n",
    "- N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Processing Example \n",
    "\n",
    "**Original Sentence**    \n",
    "`Dude! I’m dying to go surfing. Let’s shred the gnar!` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Clearning and Tokenizing**    \n",
    "`[dude,im,dying,to,go,surfing,lets,shred,the,gnar]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stop Word Removal**    \n",
    "`[dying,surfing,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stemming and Lemmatization**    \n",
    "`[dy,surf,shred,gnar]   [die,surf,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams**    \n",
    "`[die,die_surf,surf,surf_shred,shred,shred_gnar,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams using POS**    \n",
    "`[die,surf,shred,shred_gnar,gnar] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing In Scikit Learn \n",
    "\n",
    "<img src=\"figs/sklearn_preprocessing.png\" style=\"width: 75%; height: 75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import Packages and Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Garren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering, spectral_clustering\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords') # download the latest stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def load_reddit(fname, ls_subreddits=[], MIN_CHAR=30):\n",
    "    \"\"\"\n",
    "    Loads Reddit Comments from a json file based on \n",
    "    whether they are in the selected subreddits and \n",
    "    have more than the MIN_CHARACTERS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        filename\n",
    "    ls_subreddits: ls[str]\n",
    "        list of subreddits to select from \n",
    "    MIN_CHAR: int\n",
    "        minimum number of characters necessary to select\n",
    "        a comment\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    corpus: ls[str]\n",
    "        list of selected reddit comments\n",
    "    subreddit_id: array[int]\n",
    "        np.array of indices that match with the ls_subreddit\n",
    "        index \n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    subreddit_id = []\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile:\n",
    "            dict_reddit_post =  ujson.loads(line)\n",
    "            subreddit = dict_reddit_post['subreddit']\n",
    "            n_characters = len( dict_reddit_post['body'] )\n",
    "            \n",
    "            if ls_subreddits: # check that the list is not empty\n",
    "                in_ls_subreddits = subreddit in ls_subreddits\n",
    "            else:\n",
    "                in_ls_subreddits = True\n",
    "            \n",
    "            grter_than_min = n_characters > MIN_CHAR\n",
    "            \n",
    "            if ( grter_than_min and in_ls_subreddits ):\n",
    "                corpus.append(dict_reddit_post['body'])\n",
    "                subreddit_id.append(subreddit)\n",
    "                \n",
    "    return np.array(corpus), np.array(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Plots the precision and recall as a function \n",
    "    of the percent of data for which we calculate \n",
    "    precision and recall \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: \n",
    "\n",
    "    yprob_: \n",
    "\n",
    "    model_name: \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = roc_auc_score( y_true, y_score)\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "### Data Source: Reddit Comments from May 2015 in JSON format\n",
    "\n",
    "For the superivised learning portion of the tutorial we will being attempting to classify whether reddit comments have come from /r/SucideWatch or /r/depression. These two subreddits are somewhat similiar so it poses a non-trivial challenge for a classifier.\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'SuicideWatch': 12609, 'depression': 24683})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab data from the following subreddits\n",
    "ls_subreddits = ['SuicideWatch', 'depression']\n",
    "[corpus, subreddit_id] = load_reddit('./data/RC_2015-05.json', ls_subreddits, MIN_CHAR=30)\n",
    "\n",
    "# count the number of comments in each subreddit \n",
    "Counter(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "In order to quantify our text, we will have to remove characters that aren't parts of words. \n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create an expression to remove from data \n",
    "RE_PREPROCESS = re.compile(r\"\"\" \\W + # one or more nonword characters\n",
    "                                |    # the or operator\n",
    "                                \\d+  # the decimal point\"\"\", re.VERBOSE)\n",
    "\n",
    "# remove anything matching RE_PREPROCESS and make lowercase \n",
    "processed_corpus = np.array( [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )\n",
    "#processed_corpus2 = np.array( [ re.sub(RE_PREPROCESS2, ' ', comment).lower() for comment in corpus] )\n",
    "\n",
    "# check out original and preprocessed data \n",
    "num_comment_to_view = 0\n",
    "for i in range(0, num_comment_to_view):\n",
    "    print(\"Original: subreddit: {}, comment: {}\".format(subreddit_id[i],corpus[i]))\n",
    "    print()\n",
    "    print(\"Processed: subreddit: {}, comment: {}\".format(subreddit_id[i],processed_corpus[i]))\n",
    "    \n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 1: Digital Stoicism\n",
    "**10 mins**\n",
    "\n",
    "Write your own preprocessing routine that removes **only happy and sad emojis** from each comment in the corpus. Write a loop that prints comments `[1788, 2360, 9679, 18096]` to check that your choice of regular expression preprocess the text as you intended. [Python regular expression documentation](https://docs.python.org/2/library/re.html) will be helpful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comment 1788:\n",
      " \n",
      ":/ this loneliness must be so hard then...I do hope you'll think about calling a friend if these feelings get stronger.  I don't want you to be scared. \n",
      ":/ this loneliness must be so hard then...I do hope you'll think about calling a friend if these feelings get stronger.  I don't want you to be scared. \n",
      "\n",
      "Comment 2360:\n",
      " \n",
      ":( You need to call someone...I had a panic attack right before I swallowed around 10 pills and saw the fucking police coming up the steps for me...I know that feeling.\n",
      "\n",
      "You can do serious damage and please don't take anymore pills. I care about you. :)\n",
      "\n",
      "~LG\n",
      "  You need to call someone...I had a panic attack right before I swallowed around 10 pills and saw the fucking police coming up the steps for me...I know that feeling.\n",
      "\n",
      "You can do serious damage and please don't take anymore pills. I care about you.  \n",
      "\n",
      "~LG\n",
      "\n",
      "Comment 9679:\n",
      " \n",
      ":| seriously? I wanted to get into building this pc as a **hobby**, something to get myself interested in. I already go for jogs and stuff, take pills, but I often feel I'm not doing anything productive with my life\n",
      ":| seriously? I wanted to get into building this pc as a **hobby**, something to get myself interested in. I already go for jogs and stuff, take pills, but I often feel I'm not doing anything productive with my life\n",
      "\n",
      "Comment 18096:\n",
      " \n",
      ":) thank you, Im working on it for sure or at least trying to\n",
      "  thank you, Im working on it for sure or at least trying to\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "comments_to_check = [1788, 2360, 9679, 18096]\n",
    "\n",
    "RE_PREPROCESS_EMOJI = re.compile(r'(:\\))|(:\\()')\n",
    "\n",
    "emojiless_corpus = np.array( [ re.sub(RE_PREPROCESS_EMOJI, ' ', comment) for comment in corpus] )\n",
    "for i in comments_to_check:\n",
    "    print(\"\\nComment {}:\\n \\n{}\\n{}\".format(i, corpus[i], emojiless_corpus[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Classification: Supervised Learning\n",
    "\n",
    "In this section we are going to train a classifier to properly tag the original subreddit in which the comment appeared. First we split our data into a testing and training set using the first 80% of the data as the training set and the remaining 20% as the testing set. \n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create Training/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Counter({'depression': 19382, 'SuicideWatch': 10451})\n",
      "Testing Labels Counter({'depression': 5301, 'SuicideWatch': 2158})\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and testing sets. \n",
    "#refactor this in the test train-split\n",
    "train_set_size = int(0.8*len(subreddit_id))\n",
    "train_idx = np.arange(0,train_set_size)\n",
    "test_idx = np.arange(train_set_size, len(subreddit_id))\n",
    "\n",
    "train_subreddit_id = subreddit_id[train_idx]\n",
    "train_corpus = processed_corpus[train_idx]\n",
    "\n",
    "test_subreddit_id = subreddit_id[test_idx]\n",
    "test_processed_corpus = processed_corpus[test_idx]\n",
    "test_corpus = corpus[test_idx]\n",
    "\n",
    "print('Training Labels', Counter(subreddit_id[train_idx]))\n",
    "print('Testing Labels', Counter((subreddit_id[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenize and stem to create features\n",
    "\n",
    "Now that we have the data and we have done a bit of preprocessing, we want to create features. We quantify words as **one-hot encodings** and create document word counts by summing encodings over words in a documents. Scikit-learn as a [CountVectorizer method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that makes this step simple. \n",
    "\n",
    "We first create a vectorizer object that stores the frequency of words in each of the documents in a **document-term count matrix**. We will discard very high and low frequency words. For example, the words *the* or *for* may appear often throughout a corpus but contain very little semantic information. Conversely a document may contain obscure words that do not occur anywhere in else in the corpus which could cause models to overfit. These cases are managed by setting a threshold for the Min and Max Document Frequency(DF).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for vectorizer \n",
    "ANALYZER = \"word\" # unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode' \n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) # Range for n-grams \n",
    "MIN_DF = 0.01 # Exclude words that are contained in less that x percent of documents \n",
    "MAX_DF = 0.8  # Exclude words that are contained in more than x percent of documents \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "                            # could add 'token_pattern' argument to specify what denotes a token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**TFIDF (Term Frequency Inverse Document Frequency)** transforms a count matrix--what we created above--into a TFIDF represenation. This is done by reweighting words that occur throughout the entire corpus to a lower weight due to empirically being found to be less discriminative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 17.81s\n"
     ]
    }
   ],
   "source": [
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to limit the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words = vectorizer.transform( test_processed_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 2: Words are *Just Too Long*\n",
    "\n",
    "**20 mins**\n",
    "\n",
    "In text analysis, words are frequently **stemmed** or **lemmatized** in order to reduce the size of the vocubalary and treat words with similar semantic meaning as the same word. Stemming consists of removing parts of words to create a stem. For example, the word tokens `eat, eating, eatery, eaten, eater` would all be stemmed to the word `eat`. The CountVectorizer provides an easy way incorporate stemming into preprocessing via the *tokenizer* parameter which takes a function handle as an argument. For this activity, create a function to pass to the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will stem all words in your corpora. Use the CountVectorizer with this function as an argument to preprocess the corpus (this may take some time). Store the result in a variable called `stemmed_corpus`. Below is an example of a stemmer from the nltk toolkit that will be helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "x = 'eating'\n",
    "stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 14.77s\n"
     ]
    }
   ],
   "source": [
    "# ANSWER \n",
    "def tokenize_and_stem(comment, stemmer = stemmer):\n",
    "    \"\"\"\n",
    "    Takes a reddit comment and stemmer as input\n",
    "    and returns a list of stemmed words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    comment: str\n",
    "    stemmer: nltk stemmer object \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    stemmed_comment: ls[str]\n",
    "        list of stemmed words\n",
    "    \"\"\"\n",
    "    stemmed_comment = [stemmer.stem(word) for word in comment]\n",
    "    return stemmed_comment\n",
    "    \n",
    "vectorizer_stem = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=tokenize_and_stem, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "start_time = time.time()\n",
    "train_bag_of_words_stem = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words_stem = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf_stem = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf_stem = transformer.transform(test_bag_of_words)\n",
    "features_stem = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(subreddit_id)\n",
    "subreddit_id_binary = le.transform(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit and Evaluate a Supervised Model\n",
    "\n",
    "**Fit** \n",
    "We will fit a regularized logistic regression model that takes a comment as input and produces a subreddit classification (/r/SuicideWatch or /r/depression) as output. \n",
    "\n",
    "### Regularized Logistic Regression \n",
    "\n",
    "Regularized logistic regression solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\bf w}{\\frac{1}{2}||{\\bf w}||_d + C \\sum_{i=1}^{N}{\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})} }$$\n",
    "\n",
    "where $(x_i, y_i)$ are the features and label for the $i$th datapoint, $\\bf w$ is a vector of model coefficients (or weights), $C$ is a regularization parameter, and $d$ specifies a norm (usually l1 or l2). The l1 norm will force coefficients that are not useful for class dicrimination to zero. The loss function $\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})$ is derived from a probabilistic model and is referred to as the logistic loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a regularized logistic regression model  \n",
    "clf = LogisticRegression(penalty='l2')\n",
    "mdl = clf.fit(train_tfidf, \n",
    "              subreddit_id_binary[train_idx])\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "** Evaluate** \n",
    "To evalute how our classifer performed we find the Area Under the Curve (AUC) of a Receiver Operating Characteristic (ROC) curve. The ROC curve plots the true positive rate versus the false positive rate for each threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njyxBGGOM8cgShDHGGI8sQRhjjPHo/wFcAYlqi5UvjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1097ae2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(subreddit_id_binary[test_idx], y_score[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Activity 3: \n",
    "\n",
    "**20mins** \n",
    "\n",
    "To understand what the model is doing, we want to see which features make the model more likely to classify a comment as a certain class. The CountVectorizer provides a list of features (using the `get_feature_names()` method) and model coefficients can be found in using the `coef_` method. For this activity, make a sorted list or dictionary of features and their importances (i.e., the corresponding coefficient value). Print the 5 most important features for classifying a comment as each of our class labels, SuicideWatch and depression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuicideWatch:  ['suicide', 'please', 'talked', 'suicidal', 'hospital']\n",
      "depression:  ['helps', 'psychiatrist', 'depressed', 'effects', 'depression']\n"
     ]
    }
   ],
   "source": [
    "# ANSWER \n",
    "coef = mdl.coef_.ravel()\n",
    "\n",
    "dict_feature_importances = dict( zip(features, coef) )\n",
    "orddict_feature_importances = OrderedDict( \n",
    "                                sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "num_features = 5\n",
    "suicidewatch_features = ls_sorted_features[:5] #SuicideWatch\n",
    "depression_features = ls_sorted_features[-5:] #depression\n",
    "print('SuicideWatch: ',suicidewatch_features)\n",
    "print('depression: ', depression_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examine Confident Predictions \n",
    "\n",
    "We want to see which comments the model scores as highly probable to belong to either class. This step can help to look for patterns in model classification and to check for any obvious errors. The predict probability refers to the probablity of a comment belonging to the depression subreddit. Therefore, comments belonging to the SucideWatch subreddit will have a low probablity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# maybe do something with this crazy indexing: this is python not C!\n",
    "num_comments = 5\n",
    "subreddit0_comment_idx = y_score[:,1].argsort()[:num_comments] #SuicideWatch\n",
    "subreddit1_comment_idx = y_score[:,1].argsort()[-num_comments:] #depression\n",
    "\n",
    "# convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([subreddit0_comment_idx, \n",
    "                                               subreddit1_comment_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuicideWatch:0.009617024344665628\n",
      "---\n",
      "It sounds like you're exhausted on many levels and are losing hope or have lost hope. I'm so sorry. \n",
      "\n",
      "For what it's worth, it's striking that you give equal weight in your post to your personal struggles and your existential despair at the state of the world. That's pretty remarkable - it speaks to your empathy. You are not worthless. I think that you want to help people. You say that you don't even know why you try - but it sounds like you do know. You care about the world so much that it hurts. Helping people gives life meaning, right? And sometimes jerks may deny that or try to take that from you, but we all have to fight them. \n",
      "\n",
      "I know that I can't imagine the added pain of being trans in a hostile world, though some of this, I get. I'm queer too and not in a good situation right now and it sounds like you've got something eating disordered type stuff going on - that certainly makes the world a harder place to live in. There is help out there though. Personally, I found support groups where people help me and I help them and they make my life not just survivable, but meaningful. Those may not be for everyone, but please don't stop believing that you deserve help. You are worth it and I'll be thinking about you.\n",
      "===\n",
      "SuicideWatch:0.022749824384601065\n",
      "---\n",
      "If life were just, heartbreak would kill you on the spot. It hurts so bad and so deep in places only that person could touch. And when the hurt is gone there is a big hole for a long time that can get filled up any number of ways. Some people fill it with anger - anger about a lot of things; their life, their ex, themselves. But anger only makes the hole bigger and deeper. Some people fill it with other things - sometimes good (family, friends, new relationships), sometimes not so good (one night stands, drugs, alcohol). But what really goes in that hole is you. You have to patch yourself back up with bits and pieces you find of yourself through some uncomfortable self-discovery. It takes a long time, and it hurts, but one day, without even realizing it's working, you'll look around and think, \"I feel whole again.\" And then your heart won't be broken and you'll have room inside for someone else. Someone who won't burn holes into your heart, but fill it up to the brim with strong, confident, love. \n",
      "\n",
      "I had my heart broken so badly I wanted to die a little over a year ago. I said \"fuck it\" to the life I thought I was going to lead (but couldn't because of the break up) and did something absolutely crazy. I sailed across the Indian Ocean. Trust me, nothing makes you dig deeper down into yourself and confront uncomfortable and varying truths like pitting yourself against a fierce ocean. Now obviously not everyone can do what I did. But I encourage you to take this time to dig deep into yourself and do something crazy that you never thought you could do. Travel, even if it's just outside your city. Learn about yourself, push your body, your mind, your soul hard. You'll find someone amazing worth loving inside. \n",
      "\n",
      "I hope this didn't seem trite. I really empathize with your pain. Please let me know if you need someone to talk to. -- you can do anything in the world when you have nothing to lose.\n",
      "===\n",
      "SuicideWatch:0.028515195298273954\n",
      "---\n",
      "The reason why hotlines are the norm is that a lot of friends and family, even if they have all the love in the world, just aren't emotionally equipped to deal with our thoughts of suicide.  So I think you're being wise and realistic to put some thought into how you present this to your family.  \n",
      "\n",
      "I also answer a hotline IRL and one of the things we end up doing quite often is helping people figure out how to break things to their families.  We also always help people work out a \"Plan B\" for what to do if telling the family doesn't go well.  So talking to your local or national hotline first might be an option for you if you feel comfortable with that.\n",
      "\n",
      "Either way, I think we might be able to help you more here in framing the conversation if you could tell us a bit more about yourself and your family, how long you've been feeling this way, and what your relationship's been like up to this point.  \n",
      "\n",
      "It's never an easy conversation to have, no matter how well prepared we are, but it sounds like you need to have it soon.  You said that you've been really struggling with thoughts of suicide for a couple of weeks - were there any changes in your life or events that precipitated this?   \n",
      "\n",
      "Hugs. \n",
      "===\n",
      "SuicideWatch:0.028971064279539174\n",
      "---\n",
      "Hey OP, just letting you know I'm here, so I'll just carry over what you posted in your last post over here. I'll respond to that a little later, please give me a little moment. \n",
      "\n",
      "&gt; Original Post:\n",
      "This is my little way of venting and hopefully making myself feel better. Sort of my way of telling the whole world how I feel but not necessarily anyone will read it, if that makes sense. Well it does to me. The best way to describe it is that I prefer to have it here on reddit with the possibility of someone reading it than on microsoft word where I know no one will read it. I'm also typing what I feel none of this is planned so if it seems all over the place this is why. Like I said I just wrote this in attempt to make myself feel better. I'm tired and it's 5:51 am as im writing this so that explains any grammatical errors. Feel free to leave responses. I don't care. If I'm breaking any rules I'd greatly appreciate a PM mods, so I can edit it or if I have to copy it so I can post it somewhere else. Anyway....\n",
      "\n",
      "&gt; In short I want to die. That's it really. I just want to stop living. Not because I'm depressed from sadness, but because I'm tired. I'm only 19 years old and I feel like I'm 80. In high school teachers, counselors, etc. always use to ask my peers and I where we saw ourselves in ten years. I never knew the answer. I still don't. I can't picture my life in 10 years, or 5 years, or even next year. I hope I'm dead. Earlier I said I'm not depressed. That's not entirely true. Let me explain. I don't want to do anything with my life. The thought of going to college and having a career saddens me. Even the thought of having a simple job saddens me. The thought of interacting with strangers frightens me. The thought of living saddens me. I'm scum. I have the possibility to do anything with my life, and I don't have a hard life. My parent's didn't hit me. They didn't divorce. I have wonderful brothers and sisters, and yet I still feel this way. If I had the opportunity to not exist and somehow give my position to somebody who has had a hard life and suffered to them I would because at least they'd do something with it. I'd do it in a heartbeat. This is what depresses me. I am the definition of scum. I shouldn't be depressed, but I am. I don't feel like I deserve to be depressed but I am. And I hate myself even more because I am. I hold a small grudge towards my parents for bringing me into this world. I often find myself at night wishing they never met. Wishing I was never born. The thought of me not existing brings me such joy. But sadly I exist so the next best thing is death. And I would of killed myself by now but the thought of pain stops me. I'm too scared. Death is the only thing I want in life and once again the only person stopping me is me. I'm my own worst enemy. Sometimes I wonder if that's why I wanna kill myself. If somehow by committing suicide, I have killed the person who's made me feel the way I feel. I haven't told anyone the way I feel. No friends or family. I often joke to my friends about suicide but I'm positive they think I'm joking and have no idea how badly I want to die. This one time my friend casually told me at a party that he was kind of depressed, and he was describing exactly how I felt, and what did I do? I basically told him to shut up because no one wanted to hear that. I don't know why I told him that. I wonder if it was because he described my feelings and I felt like I couldn't accept it at the time. I regret it everyday of my life. It's another reason why I hate myself. My friend didn't commit suicide, and we've hung out since then, but he's never talked about it again and I haven't asked him. I'm not even good enough to be his friend. I've never felt connected to my family. At least not enough to ever tell them my feelings. I feel like my brothers and sisters were made for each other, if that makes sense. Like when you learn that they're related you think \"yeah, that makes sense\" but me, not so much. I'm nothing like them. We don't like the same things, and our personalities are so different that when we get together, like at a family gathering, I find that I don't talk to them much. and when we do they generally ask how's it going and I just say \"fine\". I've felt so lonely my entire life. Ever since I was a child. I've never had a girlfriend and at this point I don't even want one because I'm worried if I start dating someone and I tell her how I feel that she'll feel like she now has to stay in the relationship so I don't commit suicide. I often day dream of hanging out with this chick I had a crush on in one of my high school classes. We both love reggae music and we'd listen to it in class every now and then. I kind of gave signals that I was interested, but she never did anything. When I was attending community college she had the professor I had before me, and we'd talk for a few minutes. The first couple times was just catching up cause we didn't see each other for a while, and after that it was just me asking what we did in class. Now that I've dropped out of college I still think about her. I've even had dreams about her where we were just hanging out. I don't think that's healthy. I think I love her, but at the same time I just think I feel this way because I feel shitty. But when I dream of her I don't hurt so bad. But I'm just her friend, and I think I'm ok with that but it still hurts because she'll never know how I feel, but at the same time I know she doesn't feel the same. At this point I feel like I'm carrying a heavy weight and it just gets heavier and heavier. But I keep carrying it, and I want to drop it. But somehow I put a smile on my face, not because I'm happy but because the thought of losing this weight makes me laugh and the thought of me killing myself makes me laugh, because I know I'll carry it forever and I know I won't ever have the balls to kill myself, so I feel I'll always feel this way, but who knows. To me death sounds good all the time so maybe when I'm at my breaking point I'll do it. I have a lot more to say, but at this point I can't really think of a reason to keep going.\n",
      "===\n",
      "SuicideWatch:0.029879756139131398\n",
      "---\n",
      "I am sorry that you want to kill yourself. This is not a place to ask for methods, but let me tell you one thing - there is no fail safe method to kill yourself. Anything you would do, there's no guarantee that you wouldn't end up in some really fucked up state. Trust me, I researched it a lot in my life.\n",
      "\n",
      "Can you please tell us what's going on in your life that you consider suicide? Did something happened lately, or are you in permanent pain from something?\n",
      "===\n",
      "depression:0.9987981489018872\n",
      "---\n",
      "Some antidepressants do have that side effect, because what you have to realize is the way they work is they \"balance\" out and change the levels of hormones and neurochemicals in our bodies. Citalopram specifically increases serotonin which surprisingly enough is a neurotransmitter inhibitor that calms and relaxes the brain. Which is possibly one of the reasons. \n",
      "\n",
      "My best suggestion for you is to speak with you psychiatrist or whoever is prescribing you the medications, and bring it up. There's a possibility if the citalopram is working and it keep your depression in check, your psychiatrist might be able to prescribe you a secondary med for days when you expect to have some form of romantic or intimate contact with you and your significant other.\n",
      "\n",
      "This is what folks with ED do with viagra. They only take it when they need it. Now I'm not saying go get viagra, but there are other drugs specifically NDRIs, where instead of changing serotonin levels they increase dopamine and norepinephrine, which for some people show very little side effects in terms of sexual drive.\n",
      "\n",
      "I'm not a psychiatrist or a pharmacist but definitely discuss it with whoever is prescribing your medications.\n",
      "===\n",
      "depression:0.9990681957555436\n",
      "---\n",
      "&gt; Edit: Also, if there is one thing I've learned in life it's no matter how much you ask or scream for help, nobody is going to come. You're always going to be on your own.\n",
      "\n",
      "You posted on this subreddit for a reason, and from one depressed to another, I want you to know this post can help you see that you truly aren't alone :)\n",
      "\n",
      "Job and money is... tricky. Our culture has esteemed the kind of job we have and the amount of money in our accounts to be the measures of success, and this puts a ton of pressure to adhere to that standard to find self-worth in this cruel life where not everyone starts on the same level. It's scary, because I don't have everything figured out, still trying to search for my career, and I have just enough to meek out a living with just enough to save and spend a little to have some fun. And guess what? I'm 27. I have a bachelor's which amounted to no career, and I'm trying to get back into school for a different bachelor's with a much better job pool; you're 21, and not to come off patronizing but there is plenty of time for you to keep searching and find your calling. If money is an issue regarding school, there is FAFSA you can apply for (assuming you're in the U.S.); you can start building a solid credit score if you haven't already by opening a credit line and treating it as you would a debit card to spend what you can afford while paying off the debt in full; you can work on getting certified or start community college to knock out general ed for much cheaper than you would at a uni if you haven't started school yet; you can bite the bullet and work part time somewhere to start your collection of job experience, or even consult a temp agency to get your foot in somewhere. There are options, and as I often say to others on this subreddit *it all begins with small, manageable steps*.\n",
      "\n",
      "Parties are overrated, especially if it's the kind of parties you think should be expected of from shows and movies and shit. Parties in general, however, can be pretty fun if you allow yourself to enjoy them :) You don't always need blowout parties, and if getting access to them is the issue, again... start small. Try Meetup.com. You can search based on your interests and hobbies, and if you don't have much hobbies to speak of try picking up something simple and small. It's daunting to meet new people, believe me I know. It took me years to develop extroverted habits, and even then I'm still an introvert at heart and need to recluse from social things to recharge my batteries. It's disheartening to put yourself out there and deal with rejection, but when it comes to social aspects in life it's what it takes, and the more you expose yourself to it the less daunting it will seem because you give yourself the opportunities to learn from what did and didn't work, and this most definitely applies to the dating world... which leads to:\n",
      "\n",
      "Your health, shape, whatever label that goes on it. Check out /r/fitness, get situated with the FAQs, and ask away if you have questions. It's a very helpful subreddit, and I refined my workout routines as a result of consulting there often. The thing with fitness is, it's difficult. It's really tough to get started, and it's a whole new battle to maintain it. **However**, once you start, it only gets better from there. It's so easy to sit down and look back to think \"If I only started __ years ago, I'd be so much more fit now...\", but with fitness your best time to start is NOW. It doesn't even have to start at the gym if money is an issue, because weight loss comes from the kitchen. If you're drinking 10 cans of soda a week, cut it down to 8 a week; then 6; then 4; then 2; then ween yourself from it and replace it with the occasional 100% juice. If you're eating fast food 3 meals a day, cut it down to 2, then 1, all the while making a trip or two to the market and avoid the aisles; slowly replace a meal or snack with something you can simply whip up in the kitchen. Even just grilling chicken and stir frying your veggies is a big step up from ordering your usual from the take out joint.\n",
      "\n",
      "Gym-wise, I recommend Planet Fitness: it's $10 a month to attend the gym you sign up at (or $20/mo. to gain access to any PF in the country) There's a lot of circlejerk hatred for PF, especially in /r/Fitness because of the general lack of access to squat racks or deadlift stations and weights that cap out at like 60-75 pounds, but if you're just starting out you don't need all that super heavy equipment (and most of the people crying about PF aren't olympic lifters either, so they're just joining in on the hate). Should you start going to the gym, don't make the wild expectation that you're going to go every day every morning if it's not your routine yet; start small. Try every two days, every other day, and once you get the hang of it you should add more gym days. I'm currently 205lbs at 6'3\", maybe at about 16-17% body fat. Not bad, but considering I used to be 250+lbs and struggled with eating right and maintaining good exercising and eating habits over the past 10+ years until now, it's a milestone for me. It doesn't have to take as long for you to get where you want to be, but it all starts with you.\n",
      "\n",
      "A healthy body image is important to building your confidence in many aspects, but figuring out who you are and allowing yourself to embrace all that is actually good, no matter how small, is where it should begin. You can look like Adonis himself and still have poor self-image, so know that looks aren't as important as you being confident with who you are, even with faults. When you find the things you know you can work on, whether it's eating habits or even something more abstract like having a hard time engaging people in conversation, it's a lot easier to break down the issues into smaller, realistic quest lines to complete. Before you know it, you'll have completed a full arc for you to gain XP on the next one.\n",
      "\n",
      "Notice the trend here? It's *all about small steps*.\n",
      "\n",
      "My biggest tip to you is to work on yourself with manageable goals, and give yourself time to let it all manifest into something greater than what you originally started with. You working on yourself will steadily give you the confidence and capacity to love yourself more, and until you love yourself you won't get yourself into a fulfilling position of someone else loving you back.\n",
      "\n",
      "You can do this.\n",
      "===\n",
      "depression:0.999280948891097\n",
      "---\n",
      "As the other commenter said, it works differently for everyone.\n",
      "\n",
      "That being said, it is a perfectly good med. AFAIK research hasn't shown it to be any better or any worse than other antidepressants. I own quite a few psychopharmacology books and one of them says that virtually all antidepressants have a 68% response rate (in other words, 2 out of 3 people who take it will respond in some way). I also recall reading about something called \"the rule of thirds\": this refers to how in clinical studies for a particular antidepressant, the pattern tends to be that 1/3 of patients will go into partial remission (i.e., they are almost completely better but have residual symptoms), 1/3 will go into full remission, and 1/3 will not show any response.\n",
      "\n",
      "Celexa (citalopram) is a very commonly used antidepressant. It's an SSRI (selective serotonin reuptake inhibitor), which is a safe and pretty effective class of meds. SSRIs are commonly used because they don't cause side effects as much or as severely as other types of antidepressants like tricyclics (TCAs) or monoamine oxidase inhibitors (MAOIs).\n",
      "\n",
      "If you'd like to read more about Celexa, I would recommend looking at this site: http://cdn.neiglobal.com/content/pg/live/citalopram.pdf\n",
      "\n",
      "That site has the same info that is in the book my doctor uses for prescribing info (it's called Stahl's Essential Psychopharmacology: The Prescriber's Guide). It explains what it's used for, how it works, notable side effects, how it causes side effects, what to do about side effects, best augmenting agents, dosing, best/worse candidates for taking this med, etc.\n",
      "\n",
      "Good luck!\n",
      "===\n",
      "depression:0.9993763862311915\n",
      "---\n",
      "This has not been said here, and I feel like it really needs to be.  Most anti-depressants will not make you less depressed. Period.  Finding one that works is difficult.  Some of the positive effects can be placebo or just your life circumstances changing.  They take 6-8 weeks to work with each medication.  It is normal to go through 2 or 3 before finding one that works for you and doesn't have nasty side effects.  From the first appointment until you find something you like might be 5-6 months.  \n",
      "\n",
      "I feel like you have this idea in your head that by getting anti-depressants your depression will go away, but that's not really how it works at all.  That being said, find a decent therapist or psychiatrist.  Sadly, in my experience, good ones are expensive.  \n",
      "\n",
      "Finally, your parents sound like they have no idea how anti-depressants actually work or what they do.  Do some of your own research.  The things that make you feel nothing are maybe anti-psychotics or like xanax (I don't actually know as much about those drug categories) and they are different from an anti-depressant.  \n",
      "===\n",
      "SuicideWatch:0.999719597566411\n",
      "---\n",
      "##Looking for help?\n",
      "\n",
      "Mid twenties paedophile here.\n",
      "\n",
      "There **are** ethical options:\n",
      "- - -\n",
      "\n",
      "##Therapy &amp; Social Support\n",
      "\n",
      "[Various therapies](http://en.wikipedia.org/wiki/Sex_offender#Therapies) have been shown to reduce (often drastically) the instance of re-offense for sex offenders and (in my opinion) there is no reason or evidence to suggest it would be any different for those with difficult/perilous attractions, paedophilic or otherwise who haven't committed a crime.\n",
      "\n",
      "There are support groups for innocent paedophiles seeking to avoid dangerous or criminal behaviour, some being run by other self-titled \"[Virtuous Pedophiles](http://en.wikipedia.org/wiki/Virtuous_Pedophiles)\" and others by community outreach type groups like \"[Circles of Support and Accountability](http://en.wikipedia.org/wiki/Circles_of_Support_and_Accountability)\" and \"[B4U-ACT](http://www.b4u-act.org)\".\n",
      "\n",
      "\"[Stop It Now](http://www.stopitnow.org)\" is a good resource for survivors and reporting crimes in a general sense but also has [a page](http://www.stopitnow.org/treatment-resources-adults-risk-abuse-and-who-have-abused-0) dedicated specifically to helping us find help.\n",
      "\n",
      "On Reddit there's /r/pedtalk, /r/pedohelp , /r/pedophilia and the generic abuse prevention sub /r/abuseinterrupted that I've found or been referred to so far.\n",
      "\n",
      "In addition to such support, actual psychological treatment in the form of [behaviour modification](https://en.wikipedia.org/wiki/Behavior_modification) (whether through [operant conditioning](http://en.wikipedia.org/wiki/Operant_conditioning) or [respondent conditioning](http://en.wikipedia.org/wiki/Respondent_conditioning) including the ethically controversial [aversion therapy](http://en.wikipedia.org/wiki/Aversion_therapy)) has some substantial positive effect.\n",
      "- - -\n",
      "\n",
      "##Pharmaceutical Aid\n",
      "\n",
      "There is also [chemical castration](http://en.wikipedia.org/wiki/Chemical_castration) which attempts to reduce risk by lessening or eliminating sex drive entirely with varied (though often positive) results.\n",
      "\n",
      "Additionally, several other prescription medications (such as anti-depressants for example) list reduced or eliminated libido as a side effect (some without the more distressing side effects of standard chemical castration drugs) the use of which could allow someone to seek medication without \"outing\" themselves under the guise of someone suffering from something like depression, which is so common as to draw little or no attention, and has the side benefit of reducing the depression that often accompanies low libido and the social isolation associated with having a sexual identity that is incompatible with ethical sexual expression.\n",
      "\n",
      "I'm not certain which work on women but men at least should (very cautiously) research:\n",
      "\n",
      "**Antacids** (famotidine, ranitidine) can cause erectile dysfunction.\n",
      "\n",
      "**Anti-anxiety drugs** (alprazolam, clonazepam, lorazepam, diazepam) may result in lower libido, delayed ejaculation, and erectile dysfunction.\n",
      "\n",
      "**Antidepressants** (selective serotonin reuptake inhibitors [SSRIs], serotonin-norepinephrine reuptake inhibitor [SNRIs], monoamine oxidase inhibitors [MAOIs], tricyclic antidepressants [TCAs]) can lower libido and cause erectile dysfunction, delayed ejaculation, and sometimes painful ejaculation.\n",
      "\n",
      "**Antifungal drugs** (ketoconazole) can lower libido and cause erectile dysfunction.\n",
      "\n",
      "**Antipsychotics** (haloperidol, risperidone, fluphenazine, quetiapine, olanzapine, ziprasidone, clozapine) can result in lower libido, erectile dysfunction, and difficulty ejaculating.\n",
      "\n",
      "**Blood pressure medication** (beta-blockers, antiarrhythmic drugs, diuretics) can cause low libido, erectile dysfunction, and delayed ejaculation.\n",
      "\n",
      "**Cholesterol-lowering drugs** (statins, fibrates) can cause sexual side effects like low libido, and erectile dysfunction since cholesterol is needed to produce testosterone.\n",
      "\n",
      "**Chemotherapy drugs** can cause low libido and ejaculatory problems.\n",
      "\n",
      "**Prostate drugs** (finasteride, prazosin, tamsulosin) can cause erectile dysfunction. Terazosin may result in priapism. Anti-androgen drugs used to treat prostate cancer can also cause side effects because they are made to lower testosterone.\n",
      "- - -\n",
      "##Other Info\n",
      "\n",
      "**Additional Sources:** [I'm a Paedophile. Over ten years since my release without hurting anyone](http://www.reddit.com/r/IAmA/comments/1kpw5g/iama_paedophile_who_has_been_inactive_since_my/).\n",
      "\n",
      "**Other things I've found personally helpful in understanding and regulating my own behaviour in no particular order.**\n",
      "\n",
      "[Cognitive Biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases) are what our minds do to protect our opinions from being challenged.\n",
      "\n",
      "[Logical Fallacies](https://en.wikipedia.org/wiki/List_of_logical_fallacies) are the failures of reasoning that we use intentionally or accidentally to prop up ill-thought-out positions.\n",
      "\n",
      "[YouAreNotSoSmart](http://youarenotsosmart.com/) is a site that has articles on some of each group above.\n",
      "\n",
      "[Mental Disorder.](https://en.wikipedia.org/wiki/Mental_illness) Focus less on these as directly indicative of a mental disorder and more on the potential for someone's brain/mind to just fail to function in some fashion. Understanding this can give you the will to recognize when yours might do the same even if you don't have some specific illness.\n",
      "\n",
      "[Maslow's Hierarchy of needs](https://en.wikipedia.org/wiki/Maslows_hierarchy_of_needs) is a good framework from which to build understanding of your own needs and to strive for their responsible fulfillment and to help others do the same for themselves.\n",
      "\n",
      "[Mindfulness Meditation](http://marc.ucla.edu/) is a particular type of meditation that eschews the metaphysical bullshit in favor of common, secular practices and scientifically studied and evidenced medical and psychological benefits.\n",
      "\n",
      "[Here](http://marc.ucla.edu/body.cfm?id=22) are some links to guided mindfulness meditation sessions from the study of it at UCLA.\n",
      "\n",
      "[TheraminTrees](https://www.youtube.com/user/TheraminTrees) is a YouTube channel that makes some pretty interesting videos about thought, faith, ethics and such. I should warn you it is from a quite secular viewpoint which may be unsettling to you if you are religious.\n",
      "\n",
      "[QualiaSoup](https://www.youtube.com/user/QualiaSoup) is quite similar and works frequently with TheraminTrees.\n",
      "\n",
      "QualiaSoup's [video on a logical basis for morality](https://www.youtube.com/watch?v=T7xt5LtgsxQ&amp;list=SP0EFCB22DFCD4F2E7) is quite helpful in divorcing the notion of morality from your own personal feelings as is [Sam Harris' TED talk on the subject](https://www.youtube.com/watch?v=Hj9oB4zpHww).\n",
      "\n",
      "I'd also recommend Sam's video on [free will](https://www.youtube.com/watch?v=pCofmZlC72g) as it outlines some pretty interesting points.\n",
      "\n",
      "[Phil Hellenes](https://www.youtube.com/user/philhellenes) has some food for thought/secular inspirational videos and is well worth checking out.\n",
      "\n",
      "[This](http://youtu.be/0qDtHdloK44) is a poem titled \"Shake The Dust\" written and performed by \"Anis Mojgani\" at the 2010 \"Heavy and Light\" concert/poetry slam hosted by the suicide/self-harm prevention advocacy group \"To Write Love On Her Arms\" that I've found particularly empowering when I'm struggling.\n",
      "- - -\n",
      "\n",
      "Once again, I derive a lot of my positions from very secular sources (because they're equally available to anyone regardless of belief system) so if you're religious they might put you off a bit.\n",
      "\n",
      "Just remember: You're not alone, you're not evil and you're not doomed.\n",
      "\n",
      "We're all here for you so talk to us.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_subreddit_id[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As can be seen from the comments for the three highest probablities for SucideWatch and depression, the classifier does a good job. Note the last entry in the list of SuicideWatch top comments is miscategorized. This is most likely due to references to depression, depression medication, and psychological treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Similarity and Clustering: Unsupervised Learning\n",
    "\n",
    "The first step in grouping documents is to define a measure of similarity between documents. A common measure of similarity for non-text data is euclidean distance. However the euclidean distance between document-term vectors ignore the length of the document. For example, if document one consists of a single sentence and document two consists of that same sentence repeated multiple times, the euclidean distance will be nonzero. To overcome this issue documents are often compared using cosine similarity, which measures the angle between two count vectors. \n",
    "\n",
    "### Cosine Similarity \n",
    "\n",
    "The cosine similarity between two vectors ${\\bf x}$ and ${\\bf y}$ is defined as: \n",
    "\n",
    "$$\\textrm{cos}({\\bf x},{\\bf y}) = \\frac{{\\bf x} \\bullet {\\bf y}}{||\\bf x|| \\; || \\bf y ||}$$. \n",
    "\n",
    "The common conceptual interpretations of the cosine similarity is the angle between two vectors. In text analysis each document is represented as a point in a $V$-dimensional, where $V$ is the size of the vocabulary. Thus by normalizing we are looking at the percent of a document that each word compromises instead of the raw counts of word tokens. \n",
    "\n",
    "\n",
    "Below is an example illustrating why we use cosine distance. The example computes the euclidean and cosine distances between repeated sentences. \n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance\n",
      " [[ 0.    2.83  5.66]\n",
      " [ 2.83  0.    2.83]\n",
      " [ 5.66  2.83  0.  ]]\n",
      "\n",
      "Cosine Distance\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['The girl went to the store', \n",
    "             'The girl went to the store. The girl went to the store',\n",
    "             'The girl went to the store. The girl went to the store. The girl went to the store']\n",
    "\n",
    "vectorizer_sim = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=(0,1)\n",
    "                            )\n",
    "\n",
    "sentence_count = vectorizer_sim.fit_transform( sentences ) \n",
    "\n",
    "\n",
    "euclidean_distance = euclidean_distances(sentence_count, Y = None)# sentence_count, Y=None, dense_output=True, 'l2')\n",
    "cosine_distance = cosine_distances(sentence_count, Y=None )\n",
    "\n",
    "print(\"Euclidean Distance\\n\", euclidean_distance.round(2))\n",
    "print(\"\\nCosine Distance\\n\", cosine_distance.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_comments = 1000\n",
    "bag_of_words = vectorizer.fit_transform( processed_corpus[0:(num_comments-1)] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/manifold/spectral_embedding_.py:217: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.43s\n",
      "Counter({0: 542, 2: 167, 3: 147, 1: 143})\n"
     ]
    }
   ],
   "source": [
    "#cluster = AgglomerativeClustering(n_clusters = 10, affinity=\"cosine\", linkage =\"average\")\n",
    "#cluster.fit(bag_of_words.toarray())\n",
    "#labels = cluster.labels__\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# calculate cosine similarity and fit clustering algorithm\n",
    "cosine_sim = cosine_similarity(bag_of_words, Y=None )\n",
    "labels = spectral_clustering(affinity=cosine_sim, n_clusters = 4)\n",
    "\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))\n",
    "\n",
    "label_counter = Counter(labels)\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Group0-------\n",
      "\n",
      "\n",
      "Comment 0: Systems of living have been built around us to make us miserable.  Idk where you are but if you can.  Go shooting.   Trust me on this. Something about shooting guns helps with that feeling.  \n",
      "\n",
      "Comment 1: Just logged back onto my alt. Thank you. That means a lot to me, I know I'm late and all but ... thanks.\n",
      "\n",
      "Comment 3: Right there with you. National merit scholar now failing classes. Gpa from 3.7 to 2.5. Depression is a bitch. You just have to find what works for you study wise. For me, empty classrooms help. Space and isolation. You have to be strict about setting study days. It's incredibly difficult and I'm nowhere near succeeding, but it's given me some improvement.  Do it for yourself, not your family. \n",
      "\n",
      "Comment 10: I'm in the exact same boat... Everyday I wake up with such a depressive mindset. I have absolutely no drive and I have no reason to get up in the mornings. I have no friends, no job and I only attend college 3 times a week. \n",
      "\n",
      "Sometimes I have a sudden spurdge of happiness. Usually when I'm in a good mood I have my drive back, I want to do things. I want to make sure that I grasp onto my future and make a name for myself before its too late. I come up with all these plans on how I'm going to do this then booom depression hits me before I execute anything then it's back to being a depressive cu*t again. \n",
      "\n",
      "\n",
      "\n",
      "It's so hard to actually have drive when you have nothing to motivate you. I have 0 irk friends. My days consist of talking to people online who really have no real impact on my real life. \n",
      "I just can't handle it. My life right now is going nowhere, every day gets worse and worse, my grandparents who are basically my parents. They mean they world to me and it's so sad when I see them I know the time is getting closer and closer, their health is going down hill every day and it puts tears in my eyes. I'm typing this with tears running all over me. \n",
      "This post has gone out of hand. This is my feelings talking. I'm sorry if I've missed the objective of this post. I just needed somewhere to throw my emotions \n",
      "\n",
      "Thank you I wish all the best for you xxx\n",
      "\n",
      "\n",
      "-------Group1-------\n",
      "\n",
      "\n",
      "Comment 7: We aren't friends. We don't dislike each other we just keep it professional. I could easily be replaced and life would go on so I guess i don't see why it would be a bad thing.\n",
      "\n",
      "Comment 24: If you fail out of college are you saying that would cause your gf to leave?\n",
      "\n",
      "Comment 42: please read my edit i may have written this a bit hastily, but i assure you i would never try to harm anyone, with my words i know how badly they can hurt.\n",
      "\n",
      "Comment 45: What all is involved in getting the license? I've thought about it. But I Dont think I would want to do it every day though maybe once or twice a week\n",
      "\n",
      "\n",
      "-------Group2-------\n",
      "\n",
      "\n",
      "Comment 4: I was a generally sad child. But I don't feel like my problems stemmed from that. Years ago I decided that I wanted to join the army. It sounded like a good idea. Wrong. I'm not sure what it was that first triggered my major depression episodes, but it happened. I started falling apart. I had a rough experience. The army shuns the weak, both mental and physical. I'm strong but was still developing mentally at 18, still learning the world and who I was. Uncontrollable actions, such as fits of depression and anger made my life hell. The others picked up on it. I felt like they would rather leave me behind then help my up. I was right. I have one very vivid memory that I remember every day. I was laying in my bed (separated from the other beds in the middle of the room), with my two \"guards\" standing watch. Yeah, I had to have at least two people within arms reach of me at all times, even while sleeping. It got that bad. One of them decided he didn't have time for this. All I did was look at him and sigh. He took my pillow from under my head and tried to suffocate me. And I just laid there. Not moving. I didn't care. It didn't last long as the other people in the room (47 people) stepped in. After the event the drill sergeant walked in, unaware of the events that had transpired moments before. He walked up to me as I was laying in bed again. He bent over me and asked my how I was. I said that I was fine and he left. It has taken me a very long time to understand how I felt about this. Turns out I was angry at myself for failing something that I set out to do. I felt like garbage for so long. It took me 5 years to admit to myself that my self directed anger and loathing ment nothing. The past is the past. There is nothing that can be done about it. But my future, my happiness needs not be influenced by past failures. I was angry at myself, and I finally learned why. And I finally forgave myself.\n",
      "\n",
      "Comment 5: Ah. Well, sometimes just him listening can be enough. Just so long as he doesn't tell you to snap out of it or anything like that. Maybe you can also research support groups in your area. Try and find as many resources as you can.\n",
      "\n",
      "Comment 6: Hello friend, I struggle with the hatred as well, it seems like second nature to just scream abuse at myself in my head. I don't know for sure how to stop it from happening but maybe making a point to yourself to notice whenever you feel yourself spiralling downward and see what sort of thing sets you off could be a start? Wishing you the best of lovely times.\n",
      "\n",
      "Comment 12: Okay so you already know that you don't want to be in this rut anymore. Seems like you are currently stuck in this vicious cycle. What if you could take small steps towards getting back out there? I'm not saying completely quit smoking but what if you set little goals each day(every couple of days) and only allow yourself to smoke up after you complete the task? It's obvious that you care and that's huge! Also do you have any close friends that could help you with keeping on track...not like hovering over you but just like checking in? Sorry if this is intrusive-I'm new on reddit😄\n",
      "\n",
      "\n",
      "-------Group3-------\n",
      "\n",
      "\n",
      "Comment 2: Hey, do you know what it is that makes you feel such hatred towards these things? Does the violence make your thoughts go in an unwanted direction? And when you say these things make them bad people, do you mean you think your friends are bad people for enjoying the shows/games, or that the people who make them are bad? It sounds like it could easily be a depression thing tbh.\n",
      "\n",
      "Comment 8: As someone a bit older, I can say this with some authority - pretty much everyone is attractive to some chunk of people.  One thing that happens in school is that people are afraid to appear different, so if they have tastes that lie outside the mainstream, they may very well be afraid to express them. I knew a couple guys who were into bigger girls, but were way too insecure to express it in high school. Not everyone - I'm not even sure most people - find the sort of default media-created concept of what is attractive to actually be attractive.  \n",
      "\n",
      "It sucks that you're on the wrong end of that sort of thing right now, but it will change with time.  And hell, if you're still in school, your body hasn't even finished developing.  You might look far more like how you want to look in a few years.\n",
      "\n",
      "Comment 9: Hey there.\n",
      "\n",
      "I understand a lot of what you're saying. There is some bad advice on this subreddit that could be easily solved by going to the sidebar or directly to this post: http://www.reddit.com/r/SWResources/comments/igh87/concerned_but_dont_know_what_to_say_here_are_some/ .\n",
      "\n",
      "A lot of the people coming here are well intentioned but reddit tends to favor those with loud voices and angry messages so those often rub off onto others. \n",
      "\n",
      "While I can't really point you in a good direction for the defaults, you can find some very kind and supportive subreddits deep down in this website. One of my personal favorites at the moment is /r/TheChurchOfRogers which is based on kindness to others as if they were our neighbors (based on the show Mr. Roger's Neighborhood. Maybe browsing there can give you these better and safer feelings.\n",
      "\n",
      "Reddit is unfortunately a site taken over by the opinions of about 4000 people. If you ever look at the controversial tab of a subreddit or even scrolling far enough down, you'll find those hidden treasures of good comments. I hope you will find those same kinds of good comments under this post as well.\n",
      "\n",
      "Never be afraid to reply OP. I'll be expecting that orange envelope. \n",
      "\n",
      "Comment 15: I'm sure you're a very pretty young girl.  Please don't let the tauntings of small-minded people scar you.\n",
      "Just endure for a little while longer.  These things improve greatly in a few years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the comments that fall in each group\n",
    "# what defines each group?\n",
    "num_comments_to_view = 4\n",
    "for key, value in label_counter.items():\n",
    "    count = 0\n",
    "    print(\"\\n-------Group{}-------\\n\\n\".format(key))\n",
    "    for i in range(0, num_comments-1):\n",
    "        if (labels[i] == key) & (count < num_comments_to_view):\n",
    "            print(\"Comment {0}: {1}\\n\".format(i, corpus[i]))\n",
    "            #print(\"Comment {0}: {1}\\n\".format(i, processed_corpus[i]))\n",
    "            count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Dimensionality Reduction\n",
    "\n",
    "In this portion of the tutorial we will be extracting topics in the form of commonly co-occuring words from the corpus of data. A topic model is a generative model of document creation used for automatic summarization of text into topics, or mixtures of words. While many variants exist (e.g., supervised, unsupervised, dynamic, hierarchical, structural), this tutorial will cover the standard topic model. The standard topic model makes three assumptions: \n",
    "\n",
    "- Bag-of-words: semantic meaning can be extracted from word co-occurance\n",
    "- Dimensionality reduction is essential to understanding semantic information \n",
    "- Topics (lower dimensional units) can be expressed as a probability distribution over words \n",
    "\n",
    "\n",
    "<img src=\"figs/topic_model.png\" style=\"width :85%; height: 85%\"/>\n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topic Model IO\n",
    "\n",
    "### Input     \n",
    "\n",
    "Document term count matrix    \n",
    "\n",
    "### Output \n",
    "\n",
    "**Topics**--the probability of each word being expressed in a given topic    \n",
    "**Document Mixtures**--the probability of each topic being expressed in a document  \n",
    "\n",
    "<img src=\"figs/topic_model_io.png\" style=\"width:85%; height:85%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Topics \n",
    "\n",
    "<img src=\"figs/example_topics.png\" style=\"width:85%; height:85%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load and Preprocess Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading takes 5.641932s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "corpus_all, subreddit_id_all = load_reddit('./data/RC_2015-05.json',MIN_CHAR=250)\n",
    "end = time.time()\n",
    "print('Loading takes {0:2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing took 60.185543060302734s\n"
     ]
    }
   ],
   "source": [
    "# Get rid of punctuation and set to lowercase  \n",
    "start = time.time()\n",
    "processed_corpus_all = [ re.sub( RE_PREPROCESS, ' ', comment).lower() for comment in corpus_all]\n",
    "\n",
    "#tokenzie the words\n",
    "bag_of_words_all = vectorizer.fit_transform( processed_corpus_all ) \n",
    "end = time.time() \n",
    "#grab the features/vocabulary\n",
    "features_all = vectorizer.get_feature_names()\n",
    "print(\"Processing took {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'depression': 12079, 'offmychest': 10661, 'stopdrinking': 6958, 'SuicideWatch': 6098, 'ADHD': 5674, 'GetMotivated': 4799, 'Anxiety': 4466, 'Meditation': 3608, 'stopsmoking': 1867, 'leaves': 1629, 'BPD': 1618, 'BipolarReddit': 1515, 'OpiatesRecovery': 1391, 'socialanxiety': 1221, 'StopGaming': 946, 'schizophrenia': 898, 'mentalhealth': 572, 'ptsd': 566, 'MMFB': 483, 'alcoholism': 378, 'rapecounseling': 283, 'BipolarSOs': 272, 'AlAnon': 267, 'ZenHabits': 237, 'alcoholicsanonymous': 219, 'getting_over_it': 208, 'EatingDisorders': 203, 'Anger': 165, 'selfhelp': 150, 'StopSelfHarm': 136, 'dpdr': 105, 'survivorsofabuse': 96, 'helpmecope': 45, 'AtheistTwelveSteppers': 29, 'problemgambling': 17, 'buddhistrecovery': 12, 'MaladaptiveDreaming': 9, 'psychoticreddit': 8, 'PanicAttack': 8, 'secularsobriety': 6, 'secondary_survivors': 5, 'SMARTRecovery': 5, 'hardshipmates': 4, 'PanicParty': 4, 'afterthesilence': 3, 'feelgood': 2, 'Existential_crisis': 1, 'jessiesparents': 1}) 69927\n"
     ]
    }
   ],
   "source": [
    "print(Counter(subreddit_id_all), len(subreddit_id_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit a Topic Model\n",
    "\n",
    "To create our topics we will use Latent Dirichlet Allocation (LDA, i.e., the standard topic model). Both [scikit learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and [gensim](https://radimrehurek.com/gensim/) provide software for fitting LDA. Both methods use [online variational inference](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf) to fit the model, but alternative methods such as [Gibbs sampling](https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/steyvers06probabilistic.pdf) are also popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing took 245.05619502067566s\n"
     ]
    }
   ],
   "source": [
    "# Options\n",
    "N_TOPICS = 50\n",
    "N_TOP_WORDS = 10\n",
    "\n",
    "start = time.time()\n",
    "lda = LatentDirichletAllocation( n_topics = N_TOPICS )\n",
    "doctopic = lda.fit_transform( bag_of_words_all )\n",
    "end = time.time() \n",
    "print(\"Processing took {}s\".format(end- start)) # takes ~72s for 1 file, ~445s for 5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Display the top ten words for each topics \n",
    "\n",
    "Qualitatively examining the topic output is the most common method for assessing fit of a topic model. Ideally, most topics will be interpretable. Topic interpretability tends to increase with the number of documents used during fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 better, though, even, much, feel, hope, know, sorry, still, life\n",
      "1 good, luck, enough, good luck, tried, suffering, bad, ideas, easy, break\n",
      "2 https, watch, com, music, video, www, youtube, https www, listen, history\n",
      "3 depressed, call, aa, book, suicide, recovery, works, called, lots, addiction\n",
      "4 someone, love, care, else, person, tell, fucking, worth, life, someone else\n",
      "5 another, used, experience, treatment, seriously, reason, sense, realize, one, name\n",
      "6 try, something, find, things, get, help, time, one, might, need\n",
      "7 school, parents, high, college, feels, job, class, life, full, high school\n",
      "8 meds, state, amazing, advice, goal, add, simple, mostly, starting, comes\n",
      "9 adhd, medication, god, walk, early, power, brother, whether, doctors, common\n",
      "10 get, time, work, months, money, take, week, day, two, last\n",
      "11 people, life, like, get, think, things, know, shit, lot, even\n",
      "12 issues, possible, definitely, answer, game, question, asking, however, either, rather\n",
      "13 years, ago, continue, yet, years ago, level, children, almost, met, couple\n",
      "14 doctor, therapy, side, effects, helped, giving, experiences, medical, side effects, inside\n",
      "15 get, work, control, trying, job, stay, honestly, enjoy, time, using\n",
      "16 back, went, go, time, first, thanks, took, got, mom, house\n",
      "17 brain, may, one, step, part, change, make, use, big, way\n",
      "18 new, small, set, anymore, steps, currently, goals, keeping, allow, complete\n",
      "19 sleep, home, normal, made, happened, knew, would, worse, op, got\n",
      "20 anxiety, night, fun, worry, symptoms, front, energy, coming, drive, stand\n",
      "21 support, kids, group, wants, child, hospital, bipolar, respect, watching, contact\n",
      "22 self, far, less, honest, emotional, abuse, cool, asked, possibly, considered\n",
      "23 thoughts, thinking, makes, feelings, reading, think, games, agree, emotions, always\n",
      "24 http, com, http www, www, reddit, message, reddit com, www reddit, please, comments\n",
      "25 year, told, thought, old, girl, weight, truth, word, trouble, together\n",
      "26 let, make, go, sure, away, run, effort, make sure, take, follow\n",
      "27 ask, guess, looking, ones, questions, order, recommend, reasons, hey, quickly\n",
      "28 mind, meditation, body, practice, exercise, heard, attention, room, calm, seeing\n",
      "29 friends, social, takes, almost, found, sucks, one, thing, one thing, basically\n",
      "30 family, never, stuff, really, open, expect, father, put, position, fit\n",
      "31 automatically, please, tags, article, tool, discussion, image, concerns, required, include\n",
      "32 pretty, much, would, guy, talking, eat, eating, healthy, idea, probably\n",
      "33 long, stress, drugs, true, short, term, perhaps, mine, certainly, degree\n",
      "34 fuck, half, morning, hour, mood, angry, entire, changed, became, plenty\n",
      "35 mental, health, job, place, disorder, diagnosed, mental health, illness, wanting, finding\n",
      "36 depression, therapist, thank, help, aware, appreciate, lack, check, hit, quite\n",
      "37 pain, focus, moment, simply, breath, anxious, notice, fall, mindfulness, pick\n",
      "38 gt, amp, org, words, wiki, system, blame, interesting, also, less\n",
      "39 help, want, know, talk, need, live, get, would, alone, people\n",
      "40 like, sounds, relationship, sounds like, really, sound, know, women, situation, close\n",
      "41 stop, seems, best, friend, sort, struggle, wish, mother, seems like, seek\n",
      "42 im, dont, age, sex, wife, young, serious, comment, society, view\n",
      "43 day, every, days, quit, smoking, one, smoke, time, every day, today\n",
      "44 read, man, write, post, great, list, men, husband, lol, books\n",
      "45 many, us, people, anyone, never, life, happy, believe, matter, glad\n",
      "46 free, feel, similar, issue, really, kind, panic, pm, heart, feeling\n",
      "47 done, minutes, drug, sit, literally, wait, sitting, mentioned, progress, proud\n",
      "48 feel, like, know, something, feel like, really, think, would, make, want\n",
      "49 drinking, drink, felt, sober, alcohol, time, long, days, life, sobriety\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "    keywords = ', '.join( features_all[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examine Comments\n",
    "\n",
    "We will examine some of the comments and list which topic comprises the largest portion of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_id: 0 offmychest topic_id: 19 keywords: sleep, home, normal, made, happened, knew, would, worse, op, got\n",
      "---\n",
      "It's not your fault don't think that. Hey if this helps it does then if it doesn't well. At least I can talk about it.\n",
      "\n",
      "I shot my dog. I had him for 10 years. I can home from partying and it was New Years so I was going to shoot off my .40 so...\n",
      "\n",
      "Well I was loading it outside and I shot it off accidentally. Right into my dog. Me having about 5-7 tequila shots into me though if I go to sleep I would wake up and he would be okay. Well if I called a vet when I shot him I could have saved him. Or at least from his pain. So I woke up saw a dead dog in my yard… i cried for hours. That old dog still had a few good years in him. \n",
      "\n",
      "That was my fault and you sitting on a little tiny mouse is more understandable than a man discharging a firearm into a shitzu. Don't feel to bad. \n",
      "===\n",
      "comment_id: 1 offmychest topic_id: 23 keywords: thoughts, thinking, makes, feelings, reading, think, games, agree, emotions, always\n",
      "---\n",
      "You do have a valid point--I've always been a bit of a behind the scenes person. While I'm super bubbly and extraverted IRL, I'm not too big on bringing much attention to myself. I don't even have a Facebook because I just found it so weird over the years. \n",
      "\n",
      "Perhaps I kinda inadvertently set a standard without realizing it, and reading this made me reflect. I reached out to some people today after reading this last night and thinking things through. Some people are still jerks--but that's just who they are and I know to just stay away from them. Others did care. Again, my family and my boyfriend's (he's coming with) don't really, and that cuts real deep, but what are ya gonna do?\n",
      "\n",
      "I do agree with the other commenter though as well. I think it's one thing to suggest something, as in \"Hey maybe we should do this small thing Saturday for my birthday\", but in my experience, I've known way too many people who, when they suggest such things, are very over the top and it just always came to me as tacky, especially (to me) if they haven't really achieved anything.\n",
      "===\n",
      "comment_id: 2 depression topic_id: 11 keywords: people, life, like, get, think, things, know, shit, lot, even\n",
      "---\n",
      "Hey, do you know what it is that makes you feel such hatred towards these things? Does the violence make your thoughts go in an unwanted direction? And when you say these things make them bad people, do you mean you think your friends are bad people for enjoying the shows/games, or that the people who make them are bad? It sounds like it could easily be a depression thing tbh.\n",
      "===\n",
      "comment_id: 3 depression topic_id: 17 keywords: brain, may, one, step, part, change, make, use, big, way\n",
      "---\n",
      "Right there with you. National merit scholar now failing classes. Gpa from 3.7 to 2.5. Depression is a bitch. You just have to find what works for you study wise. For me, empty classrooms help. Space and isolation. You have to be strict about setting study days. It's incredibly difficult and I'm nowhere near succeeding, but it's given me some improvement.  Do it for yourself, not your family. \n",
      "===\n",
      "comment_id: 4 ADHD topic_id: 15 keywords: get, work, control, trying, job, stay, honestly, enjoy, time, using\n",
      "---\n",
      "I cannot put into words how much I love programming. So many avenues to go down, so many ways to innovate, and the very real possibility of changing the world just by doing it a lot. I work at a web development company and do independent game design in my off time, and rarely feel burnt out. If I could add an extra day to the week to program, I would do it twice.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "num_comments = 5\n",
    "for comment_id in range(num_comments):\n",
    "    topic_id = np.argsort(doctopic[comment_id])[::-1][0]\n",
    "    print('comment_id:',\n",
    "          comment_id,\n",
    "          subreddit_id_all[comment_id],\n",
    "          'topic_id:',\n",
    "          topic_id,\n",
    "          'keywords:',\n",
    "           ls_keywords[topic_id])\n",
    "    print('---')\n",
    "    print(corpus_all[comment_id])\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cool Visualizations \n",
    "    \n",
    "<img src = \"figs/supreme_court_topics.png\" style = \"width:100%;height:100%\"/>\n",
    "\n",
    "[Mental Health Subreddit Topics](http://geebioso.github.io/Subreddit-Topic-Visualization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Return to [TOC](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
