{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "**Garren Gaut with contributions from Avishek Kumar**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Table of Contents\n",
    " - [Data Source](#Data-Source:-Reddit-Comments-from-May-2015-in-JSON-format)\n",
    " - [Preprocess](#Preprocess-the-data)\n",
    " - [Supervised Learning](#Comment-Classification:-Supervised-Learning)\n",
    " - [Unsupervised Learning](#Comment-Similarity-and-Clustering:-Unsupervised-Learning)\n",
    " - [Dimensionality Reduction](#Topic-Modeling:-Dimensionality-Reduction )\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Text Analysis is used for summarizing or extracting useful information from large amounts of  unstructured text. Text analysis encompases a large array of tasks: \n",
    "\n",
    "<img src=\"figs/nlp_tasks.png\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Demos \n",
    "\n",
    " - [Stanford NLP](http://nlp.stanford.edu:8080/corenlp/process)    \n",
    " - [Illinois Cognitive Computation Group](http://cogcomp.cs.illinois.edu/page/demos/)    \n",
    " - [Event Search](http://eventregistry.org/searchEvents)    \n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why is Language Analysis Hard?\n",
    "\n",
    "- Ambiguity of Language\n",
    "- Commonsense knowledge is typically omitted from social communications\n",
    "- Abduction\n",
    "- Language is dynamic \n",
    "- Symbol grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a word? \n",
    "\n",
    "### One-hot Encoding\n",
    "\n",
    "`[0,0,0,...,0,1,0,...,0]`\n",
    "\n",
    "### Distributed Representation \n",
    "\n",
    "`[0.34, 0.51,...,0.49, 0.56, 0.72]`\n",
    "\n",
    "<!-- LSA, word embeddings, vector space embedding --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we represent a document? \n",
    "\n",
    "### Bag-of-words \n",
    "\n",
    "A bag of words is created by summing the one-hot encodings for all words in a document.     \n",
    "\n",
    "\n",
    "<img src=\"figs/scrabble-2133.jpg\" style=\"width: 50%; height: 50%\"/>\n",
    "\n",
    "\n",
    "### Document Embedding \n",
    "\n",
    "Document embeddings can be created by averaging over word embeddings. This assumes that words and document are 'occupying' the same semantic vector space. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "\n",
    "* **Document Classification**: Predicting which class label belongs to a document (in this tutorial comments are documents). \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ingnored. \n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. For instance, separating sentences using punctuation. \n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **Word (type) vs word token**: In text analysis, *word (type)* refers to a unique set of characters. A *word token* is any occurance of a *word (type)*. For example, the sentence 'Oh boy, that boy is impressive!' contains 6 *word tokens*, but only 5 *word (types)*.   \n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "- Cleaning & Tokenization\n",
    "- Stop Word Removal\n",
    "- Normalization: Stemming & Lemmatization\n",
    "- N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Processing Example \n",
    "\n",
    "**Original Sentence**    \n",
    "`Dude! I’m dying to go surfing. Let’s shred the gnar!` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Clearning and Tokenizing**    \n",
    "`[dude,im,dying,to,go,surfing,lets,shred,the,gnar]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stop Word Removal**    \n",
    "`[dying,surfing,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stemming and Lemmatization**    \n",
    "`[dy,surf,shred,gnar]   [die,surf,shred,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams**    \n",
    "`[die,die_surf,surf,surf_shred,shred,shred_gnar,gnar]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**N-grams using POS**    \n",
    "`[die,surf,shred,shred_gnar,gnar] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing In Scikit Learn \n",
    "\n",
    "<img src=\"figs/sklearn_preprocessing.png\" style=\"width: 75%; height: 75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sci-kit Learn and Nltk\n",
    "\n",
    "We are going to use sci-kit learn and nltk to process a dataset of comments from reddit. We will train machine learning classifiers to predict which subreddit a comment came from, and will cluster comments. First, we process comment text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Garren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering, spectral_clustering\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "from utils import load_reddit, plot_roc, print_variable_sizes, print_memory\n",
    "\n",
    "nltk.download('stopwords') # download the latest stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "### Data Source: Reddit Comments from May 2015 in JSON format\n",
    "\n",
    "For the superivised learning portion of the tutorial we will being attempting to classify whether reddit comments have come from /r/SucideWatch or /r/depression. These two subreddits are somewhat similiar so it poses a non-trivial challenge for a classifier.\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'SuicideWatch': 12609, 'depression': 24683})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab data from the following subreddits\n",
    "ls_subreddits = ['SuicideWatch', 'depression']\n",
    "[corpus, subreddit_id] = load_reddit('./data/RC_2015-05.json', ls_subreddits, MIN_CHAR=30)\n",
    "\n",
    "# count the number of comments in each subreddit \n",
    "Counter(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "In order to quantify our text, we will have to remove characters that we don't consider parts of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create an expression to remove from data \n",
    "RE_PREPROCESS = re.compile(r\"\"\" \\W + # one or more nonword characters\n",
    "                                |    # the or operator\n",
    "                                \\d+  # the decimal point\"\"\", re.VERBOSE)\n",
    "\n",
    "# remove anything matching RE_PREPROCESS and make lowercase \n",
    "processed_corpus = np.array( [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )\n",
    "#processed_corpus2 = np.array( [ re.sub(RE_PREPROCESS2, ' ', comment).lower() for comment in corpus] )\n",
    "\n",
    "# check out original and preprocessed data \n",
    "num_comment_to_view = 0\n",
    "for i in range(0, num_comment_to_view):\n",
    "    print(\"Original: subreddit: {}, comment: {}\".format(subreddit_id[i],corpus[i]))\n",
    "    print()\n",
    "    print(\"Processed: subreddit: {}, comment: {}\".format(subreddit_id[i],processed_corpus[i]))\n",
    "    \n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 1: Internet Stoic\n",
    "**10 mins**\n",
    "\n",
    "Write your own preprocessing routine that removes **only happy and sad emojis** from each comment in the corpus. Write a loop that prints comments `[1788, 2360, 9679, 18096]` to check that your choice of regular expression preprocesses the text as you intended. [Python regular expression documentation](https://docs.python.org/2/library/re.html) will be helpful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comment 1788:\n",
      " \n",
      ":/ this loneliness must be so hard then...I do hope you'll think about calling a friend if these feelings get stronger.  I don't want you to be scared. \n",
      ":/ this loneliness must be so hard then...I do hope you'll think about calling a friend if these feelings get stronger.  I don't want you to be scared. \n",
      "\n",
      "Comment 2360:\n",
      " \n",
      ":( You need to call someone...I had a panic attack right before I swallowed around 10 pills and saw the fucking police coming up the steps for me...I know that feeling.\n",
      "\n",
      "You can do serious damage and please don't take anymore pills. I care about you. :)\n",
      "\n",
      "~LG\n",
      "  You need to call someone...I had a panic attack right before I swallowed around 10 pills and saw the fucking police coming up the steps for me...I know that feeling.\n",
      "\n",
      "You can do serious damage and please don't take anymore pills. I care about you.  \n",
      "\n",
      "~LG\n",
      "\n",
      "Comment 9679:\n",
      " \n",
      ":| seriously? I wanted to get into building this pc as a **hobby**, something to get myself interested in. I already go for jogs and stuff, take pills, but I often feel I'm not doing anything productive with my life\n",
      ":| seriously? I wanted to get into building this pc as a **hobby**, something to get myself interested in. I already go for jogs and stuff, take pills, but I often feel I'm not doing anything productive with my life\n",
      "\n",
      "Comment 18096:\n",
      " \n",
      ":) thank you, Im working on it for sure or at least trying to\n",
      "  thank you, Im working on it for sure or at least trying to\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "comments_to_check = [1788, 2360, 9679, 18096]\n",
    "\n",
    "RE_PREPROCESS_EMOJI = re.compile(r'(:\\))|(:\\()')\n",
    "\n",
    "emojiless_corpus = np.array( [ re.sub(RE_PREPROCESS_EMOJI, ' ', comment) for comment in corpus] )\n",
    "for i in comments_to_check:\n",
    "    print(\"\\nComment {}:\\n \\n{}\\n{}\".format(i, corpus[i], emojiless_corpus[i]))\n",
    "\n",
    "# clear some memory\n",
    "emojiless_corpus = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Classification: Supervised Learning\n",
    "\n",
    "In this section we are going to train a classifier to properly tag the original subreddit in which the comment appeared. First we split our data into a testing and training set using the first 80% of the data as the training set and the remaining 20% as the testing set. \n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create Training/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Counter({'depression': 19382, 'SuicideWatch': 10451})\n",
      "Testing Labels Counter({'depression': 5301, 'SuicideWatch': 2158})\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and testing sets. \n",
    "#refactor this in the test train-split\n",
    "train_set_size = int(0.8*len(subreddit_id))\n",
    "train_idx = np.arange(0,train_set_size)\n",
    "test_idx = np.arange(train_set_size, len(subreddit_id))\n",
    "\n",
    "train_subreddit_id = subreddit_id[train_idx]\n",
    "train_corpus = processed_corpus[train_idx]\n",
    "\n",
    "test_subreddit_id = subreddit_id[test_idx]\n",
    "test_processed_corpus = processed_corpus[test_idx]\n",
    "test_corpus = corpus[test_idx]\n",
    "\n",
    "print('Training Labels', Counter(subreddit_id[train_idx]))\n",
    "print('Testing Labels', Counter((subreddit_id[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenize and stem to create features\n",
    "\n",
    "Now that we have the data and we have done a bit of preprocessing, we want to create features. We quantify words as **one-hot encodings** and create document word counts by summing encodings over words in a documents. Scikit-learn as a [CountVectorizer method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that makes this step simple. \n",
    "\n",
    "We first create a vectorizer object that stores the frequency of words in each of the documents in a **document-term count matrix**. We will discard very high and low frequency words. For example, the words *the* or *for* may appear often throughout a corpus but contain very little semantic information. Conversely a document may contain obscure words that do not occur anywhere in else in the corpus which could cause models to overfit. These cases are managed by setting a threshold for the Min and Max Document Frequency(DF).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for vectorizer \n",
    "ANALYZER = \"word\" # unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode' \n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) # Range for n-grams \n",
    "MIN_DF = 100/len(corpus) # Exclude words that are contained in less that x percent of documents \n",
    "MAX_DF = 0.8  # Exclude words that are contained in more than x percent of documents \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "                            # could add 'token_pattern' argument to specify what denotes a token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**TFIDF (Term Frequency Inverse Document Frequency)** transforms a count matrix--what we created above--into a TFIDF represenation. This is done by reweighting words that occur throughout the entire corpus to a lower weight due to empirically being found to be less discriminative. Intuitively, TFIDF counts the number of times each term occurs in each document and weights each term frequency by how rare that term is in the corpus. Rare words receive higher weight to quantify that rare words are more discriminative. \n",
    "\n",
    "Term frequency (TF) for a word $t$ and document $d$ is computed as:\n",
    "\n",
    "$$\n",
    "\\textrm{tf} (t,d)= \\frac{n_{t,d}}{\\sum\\limits_{t^\\prime \\in \\mathbb{T}} { n_{t^\\prime,d}}}\n",
    "$$\n",
    "\n",
    "where $n_{t,d}$ is the number of times term $t$ occurs in document $d$ and $\\mathbb{T}$ is the set of all terms in the corpus. The inverse document frequency (IDF) for a term and a set of documents $D$ is computed as:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathrm{idf}(t, D) &=  \\log \\frac{|D|}{1 + df(D,t)} \\\\ \n",
    " df(D,t) &= |\\{d \\in D: t \\in d\\}| \n",
    "\\end{align}\n",
    "\n",
    "where $|\\{d \\in D: t \\in d\\}|$ is the number of documents where term $t$ appears. The 1 added in the demoninator prevents division-by-zero. Then TFIDF is just the weighting of each term frequency by the inverse document frequency:\n",
    "\n",
    "$$ \n",
    "{\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)}\n",
    "$$ \n",
    "\n",
    "[TFIDF in sci-kit learn](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) differs from the textbook definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEZCAYAAAB1mUk3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22a32e080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot different IDF options\n",
    "document_freq = np.arange(0,10,1)\n",
    "n_d = 10\n",
    "\n",
    "default = np.log((1 + n_d)/(1 + document_freq)) + 1\n",
    "textbook = np.log( n_d / (1 + document_freq))\n",
    "smooth_idf_false = np.log( n_d/document_freq)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(document_freq, default, color='blue',\n",
    "     lw=lw, label='default')\n",
    "plt.plot(document_freq, textbook, color='red',\n",
    "     lw=lw, label='textbook')\n",
    "plt.plot(document_freq, smooth_idf_false, color='green',\n",
    "     lw=lw, label='smooth_idf_false')\n",
    "\n",
    "plt.xlabel('document frequency')\n",
    "plt.ylabel('inverse document frequency')\n",
    "plt.title('IDF')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 13.39s\n"
     ]
    }
   ],
   "source": [
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to weight the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words = vectorizer.transform( test_processed_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 2: Words are *Just Too Long*\n",
    "\n",
    "**20 mins**\n",
    "\n",
    "In text analysis, words are frequently **stemmed** or **lemmatized** in order to reduce the size of the vocubalary and treat words with similar semantic meaning as the same word. Stemming consists of removing parts of words to create a stem. For example, the word tokens `eat, eating, eatery, eaten, eater` would all be stemmed to the word `eat`. The CountVectorizer provides an easy way incorporate stemming into preprocessing via the *tokenizer* parameter which takes a function handle as an argument. For this activity, create a function to pass to the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will stem all words in your corpora. Use the CountVectorizer with this function as an argument to preprocess the corpus (this may take some time). Store the result in a variable called `stemmed_corpus`. Below is an example of a stemmer from the nltk toolkit that will be helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "x = 'eating'\n",
    "stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 12.03s\n"
     ]
    }
   ],
   "source": [
    "# ANSWER \n",
    "def tokenize_and_stem(comment, stemmer = stemmer):\n",
    "    \"\"\"\n",
    "    Takes a reddit comment and stemmer as input\n",
    "    and returns a list of stemmed words\n",
    "    \n",
    "    params: \n",
    "        str comment: comment to stem \n",
    "        nltk.stemmer stemmer: nltk stemmer object \n",
    "        \n",
    "    rtype: ls[str]\n",
    "    \"\"\"\n",
    "    stemmed_comment = [stemmer.stem(word) for word in comment]\n",
    "    return stemmed_comment\n",
    "    \n",
    "vectorizer_stem = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=tokenize_and_stem, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "start_time = time.time()\n",
    "train_bag_of_words_stem = vectorizer.fit_transform( train_corpus ) \n",
    "test_bag_of_words_stem = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf_stem = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf_stem = transformer.transform(test_bag_of_words)\n",
    "features_stem = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "# depression corresponds to a 1 \n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(subreddit_id)\n",
    "subreddit_id_binary = le.transform(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit and Evaluate a Supervised Model\n",
    "\n",
    "**Fit** \n",
    "We will fit a regularized logistic regression model that takes a comment as input and produces a subreddit classification (/r/SuicideWatch or /r/depression) as output. \n",
    "\n",
    "### Regularized Logistic Regression \n",
    "\n",
    "Regularized logistic regression solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\bf w}{\\frac{1}{2}||{\\bf w}||_d + C \\sum_{i=1}^{N}{\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})} }$$\n",
    "\n",
    "where $(x_i, y_i)$ are the features and label for the $i$th datapoint, $\\bf w$ is a vector of model coefficients (or weights), $C$ is a regularization parameter, and $d$ specifies a norm (usually l1 or l2). The l1 norm will force coefficients that are not useful for class dicrimination to zero. The loss function $\\textrm{log}(1+e^{−{y_i}{\\bf w}^{T}x_i})$ is derived from a probabilistic model and is referred to as the logistic loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a regularized logistic regression model  \n",
    "clf = LogisticRegression(penalty='l2')\n",
    "mdl = clf.fit(train_tfidf, \n",
    "              subreddit_id_binary[train_idx])\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Evaluate\n",
    "To evalute how our classifer performed we plot the Receiver Operating Characteristic (ROC) curve. The ROC curve is a way of lookin at overall classifier performance. For each test comment, the classifier produces a score indicating how confident the classifier is that the comment was posted in the depression subreddit. In order to produce classifications from these scores, we need to set a threshold above which a score is converted into a positive classificaiton. The ROC curve plots the false positive and true positive rate for every possible threshold we could set. We find the Area Under the Curve (AUC) which ranges from 0.5 (chance) to 1 (perfect prediction). The dashed line plots chance ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b1ffa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(subreddit_id_binary[test_idx], y_score[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Activity 3: \n",
    "\n",
    "**20mins** \n",
    "\n",
    "To understand what the model is doing, we want to see which features make the model more likely to classify a comment as a certain class. The CountVectorizer provides a list of features (using the `get_feature_names()` method) and model coefficients can be found in using the `coef_` method. For this activity, make a sorted list or dictionary of features and their importances (i.e., the corresponding coefficient value). Print the 5 most important features for classifying a comment as each of our class labels, SuicideWatch and depression. \n",
    "\n",
    "Note, this is pretty easy since we are using Logistic Regression. Feature importance may require more complicated analysis for other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuicideWatch:  ['lg', 'esteem', 'www reddit', 'https www', 'www youtube']\n",
      "depression:  ['youtube com', 'reddit com', 'depression', 'self esteem', 'productive']\n"
     ]
    }
   ],
   "source": [
    "# ANSWER \n",
    "coef = mdl.coef_.ravel()\n",
    "\n",
    "dict_feature_importances = dict( zip(features, coef) )\n",
    "orddict_feature_importances = OrderedDict( \n",
    "                                sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "                                # lambda is an anonymous function. in this case, we take the first \n",
    "                                # element of each item of the dictionary as the key to sort by\n",
    "        \n",
    "\n",
    "ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "num_features = 5\n",
    "suicidewatch_features = ls_sorted_features[:5] #SuicideWatch\n",
    "depression_features = ls_sorted_features[-5:] #depression\n",
    "print('SuicideWatch: ',suicidewatch_features)\n",
    "print('depression: ', depression_features[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examine Confident Predictions \n",
    "\n",
    "We want to see which comments the model scores as highly probable to belong to either class. This step can help to look for patterns in model classification and to check for any obvious errors. The predict probability refers to the probablity of a comment belonging to the depression subreddit. Therefore, comments belonging to the SucideWatch subreddit will have a low probablity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# maybe do something with this crazy indexing: this is python not C!\n",
    "num_comments = 5\n",
    "subreddit0_comment_idx = y_score[:,1].argsort()[:num_comments] #SuicideWatch\n",
    "subreddit1_comment_idx = y_score[:,1].argsort()[-num_comments:] #depression\n",
    "\n",
    "# convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([subreddit0_comment_idx, \n",
    "                                               subreddit1_comment_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuicideWatch:0.00021399175338538608\n",
      "---\n",
      "LOL wow...you actually searched that for me. \n",
      "\n",
      "Hmmm...I wouldn't know how to start. I would appreciate a nudge in the right direction though! I REALLY would like to do something like that to maybe help people out. My T even said that once I am more stable for a while that I should go do work advocating for people with mental health issues and maybe go around to schools and such to be sort of a motivational speaker and educator about mental illness--all told from the perspective of someone with severe depression, borderline personality disorder, and anorexia. Interesting...I think a blog or even a vlog would be a good start.\n",
      "\n",
      "&lt;3 I am feeling FANTASTIC right now and I want to share all the love I have in my heart with each person who is still hurting here tonight. I have been in your shoes, and I wouldn't be surprised if I find myself there again (again, just being real), so I understand and I have a lot of compassion and empathy for all of you.\n",
      "\n",
      "My T challenged me today, and maybe this would be a good question to ask everyone:\n",
      "\n",
      "He said \"Why is it that you can have so much compassion and empathy for everyone else (he knows I post here because I was honest with him that I had been still having a lot of thoughts and ideations and he is fine with that) but you can't have compassion and empathy for yourself?\n",
      "\n",
      "What a great question. I have to chew on that one tonight a little and see if I can't come up with an answer. Anyone else have any thoughts about themselves in regards to having compassion and empathy for others but not yourself? \n",
      "\n",
      "I don't mind if you hijack my thread. Please do...I want to hear from you. I don't mind listening to your stories. :) I am really interested in thoughts from the community on this one.\n",
      "\n",
      "Ta for now...going to check back in later. My video awaits. :)\n",
      "\n",
      "(YES...I am STILL procrastinating dang it!!)\n",
      "\n",
      "~LG\n",
      "===\n",
      "SuicideWatch:0.0007878345514254026\n",
      "---\n",
      ":) I am actually doing FANTASTIC tonight! I just posted if you wanna have a look. :)\n",
      "\n",
      "How are YOU doing?\n",
      "\n",
      "SMILE!!!\n",
      "\n",
      "~LG\n",
      "===\n",
      "SuicideWatch:0.0008365994882207714\n",
      "---\n",
      "Thanks everyone, I guess. I was stopped yet again (this time before I could even start) by my kids needing this, and this, and that. I know what you will all say, so save it. :( I am not looking for sympathy anymore (I was in the beginning, but now...I didn't even want to come here and reply because I don't want to waste any more of your time here)...I am making an outright promise. I wish I had a garage so I could just go out that way. That sounds like the BEST way ever to die...just drift off to eternal sleep on an odorless, colorless, tasteless gas. I don't have a garage though. We have street-parking or we use our tiny driveway if someone is in our spot, which happens a lot. \n",
      "\n",
      "I will exist no more. Don't cry for me all...especially you KG. I read your messages. I cried a bit thinking about how you were so lost and that night how we all were so concerned about you. I couldn't sleep at all that night and I tossed and turned as I worried about if you were alive or dead. Don't cry for me, though. I have had a long enough life on this planet and it is just time for me to go. When the pain outweighs the good, then it is time to let go of this life and go on to whatever lies beyond. I am not scared...well...maybe a little. I know that my kids will go on and change the world, and it will be up to them now to make a future for themselves and their future families. I read ALL of your messages, and they all made me tear up and cry.\n",
      "\n",
      "I will live on within each of them. I will be that snowbird outside the window on a cold February morning...I will be the hot sun in the middle of July...I will be the rain and the snow, and the wind that whips along the streets and howls through the pine trees. I will be that train whistle the kids hear, signalling a load coming into the city. I will be the grass under their feet. I will be here with them. Always. \n",
      "\n",
      "I will be here with you guys too. I will be that anonymous poster who can't live anymore...but then finds the strength to go on. I will be those words each of you type on the screen--words that seemingly come out of nowhere and make someone feel better. I will be each teardrop and each smile...I will be every cut that someone makes on their body, and every pill of medication that someone takes to feel better.\n",
      "\n",
      "I will be.\n",
      "\n",
      "Thanks all. I will prepare and sometime soon...just slip off into oblivion.\n",
      "\n",
      "~LG\n",
      "\n",
      "===\n",
      "SuicideWatch:0.002601239155180692\n",
      "---\n",
      "Hey OP, just letting you know I'm here, so I'll just carry over what you posted in your last post over here. I'll respond to that a little later, please give me a little moment. \n",
      "\n",
      "&gt; Original Post:\n",
      "This is my little way of venting and hopefully making myself feel better. Sort of my way of telling the whole world how I feel but not necessarily anyone will read it, if that makes sense. Well it does to me. The best way to describe it is that I prefer to have it here on reddit with the possibility of someone reading it than on microsoft word where I know no one will read it. I'm also typing what I feel none of this is planned so if it seems all over the place this is why. Like I said I just wrote this in attempt to make myself feel better. I'm tired and it's 5:51 am as im writing this so that explains any grammatical errors. Feel free to leave responses. I don't care. If I'm breaking any rules I'd greatly appreciate a PM mods, so I can edit it or if I have to copy it so I can post it somewhere else. Anyway....\n",
      "\n",
      "&gt; In short I want to die. That's it really. I just want to stop living. Not because I'm depressed from sadness, but because I'm tired. I'm only 19 years old and I feel like I'm 80. In high school teachers, counselors, etc. always use to ask my peers and I where we saw ourselves in ten years. I never knew the answer. I still don't. I can't picture my life in 10 years, or 5 years, or even next year. I hope I'm dead. Earlier I said I'm not depressed. That's not entirely true. Let me explain. I don't want to do anything with my life. The thought of going to college and having a career saddens me. Even the thought of having a simple job saddens me. The thought of interacting with strangers frightens me. The thought of living saddens me. I'm scum. I have the possibility to do anything with my life, and I don't have a hard life. My parent's didn't hit me. They didn't divorce. I have wonderful brothers and sisters, and yet I still feel this way. If I had the opportunity to not exist and somehow give my position to somebody who has had a hard life and suffered to them I would because at least they'd do something with it. I'd do it in a heartbeat. This is what depresses me. I am the definition of scum. I shouldn't be depressed, but I am. I don't feel like I deserve to be depressed but I am. And I hate myself even more because I am. I hold a small grudge towards my parents for bringing me into this world. I often find myself at night wishing they never met. Wishing I was never born. The thought of me not existing brings me such joy. But sadly I exist so the next best thing is death. And I would of killed myself by now but the thought of pain stops me. I'm too scared. Death is the only thing I want in life and once again the only person stopping me is me. I'm my own worst enemy. Sometimes I wonder if that's why I wanna kill myself. If somehow by committing suicide, I have killed the person who's made me feel the way I feel. I haven't told anyone the way I feel. No friends or family. I often joke to my friends about suicide but I'm positive they think I'm joking and have no idea how badly I want to die. This one time my friend casually told me at a party that he was kind of depressed, and he was describing exactly how I felt, and what did I do? I basically told him to shut up because no one wanted to hear that. I don't know why I told him that. I wonder if it was because he described my feelings and I felt like I couldn't accept it at the time. I regret it everyday of my life. It's another reason why I hate myself. My friend didn't commit suicide, and we've hung out since then, but he's never talked about it again and I haven't asked him. I'm not even good enough to be his friend. I've never felt connected to my family. At least not enough to ever tell them my feelings. I feel like my brothers and sisters were made for each other, if that makes sense. Like when you learn that they're related you think \"yeah, that makes sense\" but me, not so much. I'm nothing like them. We don't like the same things, and our personalities are so different that when we get together, like at a family gathering, I find that I don't talk to them much. and when we do they generally ask how's it going and I just say \"fine\". I've felt so lonely my entire life. Ever since I was a child. I've never had a girlfriend and at this point I don't even want one because I'm worried if I start dating someone and I tell her how I feel that she'll feel like she now has to stay in the relationship so I don't commit suicide. I often day dream of hanging out with this chick I had a crush on in one of my high school classes. We both love reggae music and we'd listen to it in class every now and then. I kind of gave signals that I was interested, but she never did anything. When I was attending community college she had the professor I had before me, and we'd talk for a few minutes. The first couple times was just catching up cause we didn't see each other for a while, and after that it was just me asking what we did in class. Now that I've dropped out of college I still think about her. I've even had dreams about her where we were just hanging out. I don't think that's healthy. I think I love her, but at the same time I just think I feel this way because I feel shitty. But when I dream of her I don't hurt so bad. But I'm just her friend, and I think I'm ok with that but it still hurts because she'll never know how I feel, but at the same time I know she doesn't feel the same. At this point I feel like I'm carrying a heavy weight and it just gets heavier and heavier. But I keep carrying it, and I want to drop it. But somehow I put a smile on my face, not because I'm happy but because the thought of losing this weight makes me laugh and the thought of me killing myself makes me laugh, because I know I'll carry it forever and I know I won't ever have the balls to kill myself, so I feel I'll always feel this way, but who knows. To me death sounds good all the time so maybe when I'm at my breaking point I'll do it. I have a lot more to say, but at this point I can't really think of a reason to keep going.\n",
      "===\n",
      "SuicideWatch:0.002648786639303404\n",
      "---\n",
      "It sounds like you're exhausted on many levels and are losing hope or have lost hope. I'm so sorry. \n",
      "\n",
      "For what it's worth, it's striking that you give equal weight in your post to your personal struggles and your existential despair at the state of the world. That's pretty remarkable - it speaks to your empathy. You are not worthless. I think that you want to help people. You say that you don't even know why you try - but it sounds like you do know. You care about the world so much that it hurts. Helping people gives life meaning, right? And sometimes jerks may deny that or try to take that from you, but we all have to fight them. \n",
      "\n",
      "I know that I can't imagine the added pain of being trans in a hostile world, though some of this, I get. I'm queer too and not in a good situation right now and it sounds like you've got something eating disordered type stuff going on - that certainly makes the world a harder place to live in. There is help out there though. Personally, I found support groups where people help me and I help them and they make my life not just survivable, but meaningful. Those may not be for everyone, but please don't stop believing that you deserve help. You are worth it and I'll be thinking about you.\n",
      "===\n",
      "depression:0.999964380296143\n",
      "---\n",
      "Sure! I'm taking 50mg of sertraline which is a fairly low dose. I started out at 25mg, they increased me to 50mg and I started seeing improvements. I started on medication and decided to stick with it because I wanted to pass my semester. I had previously dropped out of school and I really wanted to finish my degree. I experience a lot of classroom anxiety so I would avoid going to class. On the sertraline, boom, anxiety in the classroom gone. It took me about 3 months to get to that point - so give your medication time to work.\n",
      "\n",
      "I had no weight gain on the medication at all. In fact, it has probably helped my eating habits because I used to binge eat when I was very depressed. I have since stopped doing that. \n",
      "\n",
      "The side effects I do experience are issues acheiving orgasm. Possible TMI but last time I attempted an orgasm it took me 45 minutes! I still experience sexual satisfaction with my partner but I have just accepted that it will be more difficult for me now. I also experienced mild nausea when I first started taking the pills but that went away within a week. \n",
      "\n",
      "My advice - stick with it. SSRIs need a few months to kick in. But they gave me that little boost. I still struggle with depression - it isn't a magic cure all - but it seriously helped me. Go with what your doctor recommends. Good luck!\n",
      "===\n",
      "depression:0.9999792223876417\n",
      "---\n",
      "As the other commenter said, it works differently for everyone.\n",
      "\n",
      "That being said, it is a perfectly good med. AFAIK research hasn't shown it to be any better or any worse than other antidepressants. I own quite a few psychopharmacology books and one of them says that virtually all antidepressants have a 68% response rate (in other words, 2 out of 3 people who take it will respond in some way). I also recall reading about something called \"the rule of thirds\": this refers to how in clinical studies for a particular antidepressant, the pattern tends to be that 1/3 of patients will go into partial remission (i.e., they are almost completely better but have residual symptoms), 1/3 will go into full remission, and 1/3 will not show any response.\n",
      "\n",
      "Celexa (citalopram) is a very commonly used antidepressant. It's an SSRI (selective serotonin reuptake inhibitor), which is a safe and pretty effective class of meds. SSRIs are commonly used because they don't cause side effects as much or as severely as other types of antidepressants like tricyclics (TCAs) or monoamine oxidase inhibitors (MAOIs).\n",
      "\n",
      "If you'd like to read more about Celexa, I would recommend looking at this site: http://cdn.neiglobal.com/content/pg/live/citalopram.pdf\n",
      "\n",
      "That site has the same info that is in the book my doctor uses for prescribing info (it's called Stahl's Essential Psychopharmacology: The Prescriber's Guide). It explains what it's used for, how it works, notable side effects, how it causes side effects, what to do about side effects, best augmenting agents, dosing, best/worse candidates for taking this med, etc.\n",
      "\n",
      "Good luck!\n",
      "===\n",
      "depression:0.9999920510933308\n",
      "---\n",
      "Hi, I am not actually diagnosed with depression, sometimes I think it may be the case hence why I am browsing this subreddit, but I love the hiphops so I figured I would share with you.  Im not to familiar with Yelawolf, but that song you posted was pretty good.\n",
      "\n",
      "The song that usually sticks with me when it comes to songs about depression is Illogic's \"Hate is a Puddle.\"  I spent many times teary eyed listening to this song, it definitely hit a spot within my brain.  A choice quote, although the whole song is quotable.  Alot of other Illogic songs are real nice to.\n",
      "\n",
      "\"Cause inside he fights feelings that he was a mistake by God\n",
      "I see his confusion and self-deception\n",
      "Questions of relevance and intelligence\n",
      "He holds an illusion of self-acceptance\n",
      "That he shows to those outside lookin in\"\n",
      "\n",
      "https://www.youtube.com/watch?v=ICB-TbY96_8\n",
      "\n",
      "It was kind of a weird time to be a hiphop fan, that grew up with hiphop in the mid to late 90s, in the early 2000s.  Towards the end of the 90s early 2000s, Mase and Puffy and other terrible rappers seemed to be almost all you would hear.  Thanks to the internet I discovered underground hiphop.  The stuff that you would not see on Yo MTV Raps or BET.  Looking back alot of the stuff that I thought was deep and better than the mainstream hiphop did not hold up well (chalk it up to my late teens early 20s trying to be \"cool\" by not listening to any of that popular shit).  But there was still some decent stuff being done during the time, alot of the underground people's were trying to hide their lack of rapping ability/ear for beats with the fact that poured their hearts into their lyrics.  Sage Francis is one of those artists, alot of his stuff does not really hold up for me now a days, but one of his strongest songs that I feel still holds up is \"Inherited Scars.\"  Deals with his sister revealing that she cuts herself to him (dont know if it is a true story), doesnt deal with depression directly, but deals with mental health.\n",
      "\n",
      "https://www.youtube.com/watch?v=9VL0mB8ILeo\n",
      "\n",
      "Years later Sage Francis came out with his song \"The Best of Times.\"  Sage Francis was one of those \"deep\" early 2000s rappers who put out some stuff that held up and some other stuff that was kind of a \"this wasnt really that good.\"  But he did make stuff that seems to stick with me when it comes to songs I would like to listen to when I am not feeling the happiness of the world. \"The Best of Times\" is about loneliness, sadness, awkwardness but also about maybe things might be ok.\n",
      "\n",
      "https://www.youtube.com/watch?v=VA8hzUDXvtk\n",
      "\n",
      "Murs has been doing his thing since the late 90s, he has made alot of songs/albums, again some dont always hold up today, but alot of that has to do with the recording of the albums.  Recorded on a low technology/money so they can be kind of hard to listen to as the quality is not great.  His song \"Varsity Blues\" is more about getting older, but I can see some parallels between his lyrics that he uses to describe getting older and some of the symptoms some people experience with depression.  \n",
      "\n",
      "https://www.youtube.com/watch?v=36HPOA4mN8U\n",
      "\n",
      "My favorite producer to emerge in the last decade Apollo Brown got together with two emcees who werent really known to me, and I am not sure if either of them had a huge following before this album, but the 3 of them came together to produce my 2014 album of the year.  The Ugly Heroes album is an ode to the working man/woman in modern America.  While no songs directly deal with depression, the issue of depression is sprinkled throughout the album.  The double burdens of being part of the working poor and depression heavily influences the lyrics through this album.  The lyrics are backed up by some stellar production, seriously Apollo Brown is one of the greatest hiphop producers of all time.  I listen to this album alot.  \"Desperate\" starts the album off so that you know what you are about to get into, the despair and hopelessness so many in modern society feel yet usually keep to themselves.\n",
      "\n",
      "Verbal Kent - \n",
      "\"I'm in my flat flat broke, last smoke\n",
      "The voice in the back of my mind sighs have hope\n",
      "I have rope, I could end it all feelin' that\n",
      "Local rapper found hanging from a ceiling fan\n",
      "That would be the easy way out though\n",
      "But what would that amount to apart from a outro?\"\n",
      "\n",
      "https://www.youtube.com/watch?v=xxXzkm67yPw\n",
      "\n",
      "\"Long Drive Home\" continues the theme of the downtrodden working man.  Although this album's theme is depressing and dark, listening to this album makes me feel good, it might be the production or it might be the way the emcees were able to capture many of the feelings I have but cant put into such words.  Maybe its because \"every good hero need a theme song.\"\n",
      "\n",
      "Red Pill -\n",
      "\"These student loans, this stupid phones\n",
      "I rarely even answer, peoples wonder what I do at home\n",
      "I sit around with Captain, eat and drink a lot\n",
      "Cause I don't like to think a lot\n",
      "Cause when I think a tend to get myself in trouble\n",
      "My blood pressure doubles\n",
      "An anxiety is coupled\n",
      "With not so subtle hint to try to tell me\n",
      "What I'm doing isn't healthy\n",
      "When people try to help\n",
      "I just tell them \"go to hell\"\n",
      "Let me worry about myself\"\n",
      "\n",
      "https://www.youtube.com/watch?v=wuxysQ-qCAI\n",
      "\n",
      "In the weird early days of aughts there came a group Cunninlynguists, with a young producer name Kno.  The Cunninlynguists are a very capable group (check out all of their stuff, their album Dirty Acres is one of my favorite albums) but their standout is the production from Kno.  Over the years both the rappers and producer grew and got better at their craft.  Kno decided to work on his solo album with a theme.  The theme of course was death.  Now it may not seem that an album about death would be something you would want to listen to when you are depressed, but there is a beauty in it.  For an album about death there is a surprising amount of positivity sprinkled in the lyrics.  Maybe its just the excellent beats Kno crafted but there is something comforting to me about listening to this album about the cousin to sleep.\n",
      "\n",
      "Deacon -\n",
      "\"(They told me) not to fear living for eternity\n",
      "But, not even Heaven seemed pleasant it was burning me\n",
      "Raised with peasants in the crescents we were shadows\n",
      "When reality television was watching adults\n",
      "And, throughout the comedy was drama and alot of pain\n",
      "And that sentence it would run on 'til the comma came\n",
      "Until the eulogy and usually my mama sang\n",
      "About the pheasants and presence we no longer claim\n",
      "(they told me) that Heaven is forever\n",
      "But at times I'd find myself thinking I'd rather never\n",
      "Lived, I mean, my life was like a dream\n",
      "I had everything I wanted but, that ain't all it seems\"\n",
      "\n",
      "https://www.youtube.com/watch?v=1HeikQGmcpM\n",
      "\n",
      "As a little off topic, since you are familiar with Eminem, I was wondering if you were familiar with fellow Detroit emcee Royce Da 5'9, thats on a few of Em's songs.  He released an album with one of the greatest hiphop producers of all time DJ Premier a few months back.  May end up my album of the year.\n",
      "\n",
      "https://www.youtube.com/watch?v=zRZDVhs9xWc\n",
      "\n",
      "I hope this post maybe gives you more music to help you \"get past shit.\"  I am not sure why I was compelled to write this post the way I did, but maybe at the very least you find some more hiphop you like.  \n",
      "\n",
      "TLDR;  This post is long, sorry.  \n",
      "\n",
      "  .  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "SuicideWatch:0.999995901304597\n",
      "---\n",
      "##Looking for help?\n",
      "\n",
      "Mid twenties paedophile here.\n",
      "\n",
      "There **are** ethical options:\n",
      "- - -\n",
      "\n",
      "##Therapy &amp; Social Support\n",
      "\n",
      "[Various therapies](http://en.wikipedia.org/wiki/Sex_offender#Therapies) have been shown to reduce (often drastically) the instance of re-offense for sex offenders and (in my opinion) there is no reason or evidence to suggest it would be any different for those with difficult/perilous attractions, paedophilic or otherwise who haven't committed a crime.\n",
      "\n",
      "There are support groups for innocent paedophiles seeking to avoid dangerous or criminal behaviour, some being run by other self-titled \"[Virtuous Pedophiles](http://en.wikipedia.org/wiki/Virtuous_Pedophiles)\" and others by community outreach type groups like \"[Circles of Support and Accountability](http://en.wikipedia.org/wiki/Circles_of_Support_and_Accountability)\" and \"[B4U-ACT](http://www.b4u-act.org)\".\n",
      "\n",
      "\"[Stop It Now](http://www.stopitnow.org)\" is a good resource for survivors and reporting crimes in a general sense but also has [a page](http://www.stopitnow.org/treatment-resources-adults-risk-abuse-and-who-have-abused-0) dedicated specifically to helping us find help.\n",
      "\n",
      "On Reddit there's /r/pedtalk, /r/pedohelp , /r/pedophilia and the generic abuse prevention sub /r/abuseinterrupted that I've found or been referred to so far.\n",
      "\n",
      "In addition to such support, actual psychological treatment in the form of [behaviour modification](https://en.wikipedia.org/wiki/Behavior_modification) (whether through [operant conditioning](http://en.wikipedia.org/wiki/Operant_conditioning) or [respondent conditioning](http://en.wikipedia.org/wiki/Respondent_conditioning) including the ethically controversial [aversion therapy](http://en.wikipedia.org/wiki/Aversion_therapy)) has some substantial positive effect.\n",
      "- - -\n",
      "\n",
      "##Pharmaceutical Aid\n",
      "\n",
      "There is also [chemical castration](http://en.wikipedia.org/wiki/Chemical_castration) which attempts to reduce risk by lessening or eliminating sex drive entirely with varied (though often positive) results.\n",
      "\n",
      "Additionally, several other prescription medications (such as anti-depressants for example) list reduced or eliminated libido as a side effect (some without the more distressing side effects of standard chemical castration drugs) the use of which could allow someone to seek medication without \"outing\" themselves under the guise of someone suffering from something like depression, which is so common as to draw little or no attention, and has the side benefit of reducing the depression that often accompanies low libido and the social isolation associated with having a sexual identity that is incompatible with ethical sexual expression.\n",
      "\n",
      "I'm not certain which work on women but men at least should (very cautiously) research:\n",
      "\n",
      "**Antacids** (famotidine, ranitidine) can cause erectile dysfunction.\n",
      "\n",
      "**Anti-anxiety drugs** (alprazolam, clonazepam, lorazepam, diazepam) may result in lower libido, delayed ejaculation, and erectile dysfunction.\n",
      "\n",
      "**Antidepressants** (selective serotonin reuptake inhibitors [SSRIs], serotonin-norepinephrine reuptake inhibitor [SNRIs], monoamine oxidase inhibitors [MAOIs], tricyclic antidepressants [TCAs]) can lower libido and cause erectile dysfunction, delayed ejaculation, and sometimes painful ejaculation.\n",
      "\n",
      "**Antifungal drugs** (ketoconazole) can lower libido and cause erectile dysfunction.\n",
      "\n",
      "**Antipsychotics** (haloperidol, risperidone, fluphenazine, quetiapine, olanzapine, ziprasidone, clozapine) can result in lower libido, erectile dysfunction, and difficulty ejaculating.\n",
      "\n",
      "**Blood pressure medication** (beta-blockers, antiarrhythmic drugs, diuretics) can cause low libido, erectile dysfunction, and delayed ejaculation.\n",
      "\n",
      "**Cholesterol-lowering drugs** (statins, fibrates) can cause sexual side effects like low libido, and erectile dysfunction since cholesterol is needed to produce testosterone.\n",
      "\n",
      "**Chemotherapy drugs** can cause low libido and ejaculatory problems.\n",
      "\n",
      "**Prostate drugs** (finasteride, prazosin, tamsulosin) can cause erectile dysfunction. Terazosin may result in priapism. Anti-androgen drugs used to treat prostate cancer can also cause side effects because they are made to lower testosterone.\n",
      "- - -\n",
      "##Other Info\n",
      "\n",
      "**Additional Sources:** [I'm a Paedophile. Over ten years since my release without hurting anyone](http://www.reddit.com/r/IAmA/comments/1kpw5g/iama_paedophile_who_has_been_inactive_since_my/).\n",
      "\n",
      "**Other things I've found personally helpful in understanding and regulating my own behaviour in no particular order.**\n",
      "\n",
      "[Cognitive Biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases) are what our minds do to protect our opinions from being challenged.\n",
      "\n",
      "[Logical Fallacies](https://en.wikipedia.org/wiki/List_of_logical_fallacies) are the failures of reasoning that we use intentionally or accidentally to prop up ill-thought-out positions.\n",
      "\n",
      "[YouAreNotSoSmart](http://youarenotsosmart.com/) is a site that has articles on some of each group above.\n",
      "\n",
      "[Mental Disorder.](https://en.wikipedia.org/wiki/Mental_illness) Focus less on these as directly indicative of a mental disorder and more on the potential for someone's brain/mind to just fail to function in some fashion. Understanding this can give you the will to recognize when yours might do the same even if you don't have some specific illness.\n",
      "\n",
      "[Maslow's Hierarchy of needs](https://en.wikipedia.org/wiki/Maslows_hierarchy_of_needs) is a good framework from which to build understanding of your own needs and to strive for their responsible fulfillment and to help others do the same for themselves.\n",
      "\n",
      "[Mindfulness Meditation](http://marc.ucla.edu/) is a particular type of meditation that eschews the metaphysical bullshit in favor of common, secular practices and scientifically studied and evidenced medical and psychological benefits.\n",
      "\n",
      "[Here](http://marc.ucla.edu/body.cfm?id=22) are some links to guided mindfulness meditation sessions from the study of it at UCLA.\n",
      "\n",
      "[TheraminTrees](https://www.youtube.com/user/TheraminTrees) is a YouTube channel that makes some pretty interesting videos about thought, faith, ethics and such. I should warn you it is from a quite secular viewpoint which may be unsettling to you if you are religious.\n",
      "\n",
      "[QualiaSoup](https://www.youtube.com/user/QualiaSoup) is quite similar and works frequently with TheraminTrees.\n",
      "\n",
      "QualiaSoup's [video on a logical basis for morality](https://www.youtube.com/watch?v=T7xt5LtgsxQ&amp;list=SP0EFCB22DFCD4F2E7) is quite helpful in divorcing the notion of morality from your own personal feelings as is [Sam Harris' TED talk on the subject](https://www.youtube.com/watch?v=Hj9oB4zpHww).\n",
      "\n",
      "I'd also recommend Sam's video on [free will](https://www.youtube.com/watch?v=pCofmZlC72g) as it outlines some pretty interesting points.\n",
      "\n",
      "[Phil Hellenes](https://www.youtube.com/user/philhellenes) has some food for thought/secular inspirational videos and is well worth checking out.\n",
      "\n",
      "[This](http://youtu.be/0qDtHdloK44) is a poem titled \"Shake The Dust\" written and performed by \"Anis Mojgani\" at the 2010 \"Heavy and Light\" concert/poetry slam hosted by the suicide/self-harm prevention advocacy group \"To Write Love On Her Arms\" that I've found particularly empowering when I'm struggling.\n",
      "- - -\n",
      "\n",
      "Once again, I derive a lot of my positions from very secular sources (because they're equally available to anyone regardless of belief system) so if you're religious they might put you off a bit.\n",
      "\n",
      "Just remember: You're not alone, you're not evil and you're not doomed.\n",
      "\n",
      "We're all here for you so talk to us.\n",
      "===\n",
      "depression:0.9999991147342128\n",
      "---\n",
      "Nothing is wrong with you.  *hug*  You're just one of us!\n",
      "\n",
      "You don't have a perfect life;  you're afflicted with depression.  If you and I were born naturally happy people, we'd be barely affected by a lousy life situation.  Man, wouldn't that be nice?\n",
      "\n",
      "But depression doesn't much care how good our life is.  My absolute worst year happened when my life was \"perfect\"...on paper.  Still sucked. So so so much.\n",
      "\n",
      "Eek!  Did you decide by yourself to stop taking antidepressants or did you talk to a doctor?  I understand the \"gray\" feeling can be really unpleasant, but maybe you would have better luck trying a different AD med.\n",
      "\n",
      "The guilt is the worst part of depression. Feeling bad about feeling bad:  it's a cycle that so many of us get stuck in.  Do you like logic?  I love logic, so recognizing that guilt is a completely illogical emotion helped me better identify and deal with it.  I'm not guilt free but at least when it hits me, I can take a step back from it and say \"This is stupid but I'm feeling it and that's okay and it's still stupid.\"\n",
      "\n",
      "Is it going to get better?  It did for me. I don't think we ever \"beat\" depression, but I've kept mine well under control for seven years now.  For me, exercise is key.  When I hit rock bottom nine years ago, I set a solid goal to do a triathlon like I always wanted. Crossing that finish line...man, I get goosebumps right now thinking about it. Antidepressants got me through the absolute worst of times, and some major life changes (job, significant other)  got me out of a really bad rut as well.  \n",
      "\n",
      "As for your significant other, it's too bad he doesn't understand.  I've learned that I can't really have deep relationships with people who don't understand depression at all; they're just insensitive about it, and sometimes I find their take on happiness to be pretty painfully simple.  That said, he might still be able to be a great partner to you.  Does he add more good to your life than bad?  Also, he doesn't have to know what to say if you tell him what support you need. My significant other knows when I'm having a down day (which I always recognize could be the start of a slipper slope), he should encourage me to go for a walk somewhere beautiful, and then cook me a healthy meal we can eat while watching comedy.  No real, deep conversational words of wisdom required;  just the anti-depression algorithm I've developed for myself:  exercise, healthy food, and comedy.  Maybe you can develop your own algorithm?\n",
      "\n",
      "I'll repeat: there's nothing wrong with you. *super hug* \n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_subreddit_id[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As can be seen from the comments for the three highest probablities for SucideWatch and depression, the classifier does a good job. Note the last entry in the list of SuicideWatch top comments is miscategorized. This is most likely due to references to depression, depression medication, and psychological treatment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.111460864\n"
     ]
    }
   ],
   "source": [
    "# clear some memory \n",
    "train_corpus = None\n",
    "test_corpus = None\n",
    "\n",
    "print_memory() # prints memory usage in Gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comment Similarity and Clustering: Unsupervised Learning\n",
    "\n",
    "The first step in grouping documents is to define a measure of similarity between documents. A common measure of similarity for non-text data is euclidean distance. However the euclidean distance between document-term vectors ignore the length of the document. For example, if document one consists of a single sentence and document two consists of that same sentence repeated multiple times, the euclidean distance will be nonzero. To overcome this issue documents are often compared using cosine similarity, which measures the angle between two count vectors. \n",
    "\n",
    "### Cosine Similarity \n",
    "\n",
    "The cosine similarity between two vectors ${\\bf x}$ and ${\\bf y}$ is defined as: \n",
    "\n",
    "$$\\textrm{cos}({\\bf x},{\\bf y}) = \\frac{{\\bf x} \\bullet {\\bf y}}{||\\bf x|| \\; || \\bf y ||}$$. \n",
    "\n",
    "The common conceptual interpretations of the cosine similarity is the angle between two vectors. In text analysis each document is represented as a point in a $V$-dimensional, where $V$ is the size of the vocabulary. Thus by normalizing we are looking at the percent of a document that each word compromises instead of the raw counts of word tokens. \n",
    "\n",
    "\n",
    "Below is an example illustrating why we use cosine distance. The example computes the euclidean and cosine distances between repeated sentences. \n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance\n",
      " [[ 0.    2.83  5.66]\n",
      " [ 2.83  0.    2.83]\n",
      " [ 5.66  2.83  0.  ]]\n",
      "\n",
      "Cosine Distance\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['The girl went to the store', \n",
    "             'The girl went to the store. The girl went to the store',\n",
    "             'The girl went to the store. The girl went to the store. The girl went to the store']\n",
    "\n",
    "vectorizer_sim = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=(0,1)\n",
    "                            )\n",
    "\n",
    "sentence_count = vectorizer_sim.fit_transform( sentences ) \n",
    "\n",
    "\n",
    "euclidean_distance = euclidean_distances(sentence_count, Y = None)# sentence_count, Y=None, dense_output=True, 'l2')\n",
    "cosine_distance = cosine_distances(sentence_count, Y=None )\n",
    "\n",
    "print(\"Euclidean Distance\\n\", euclidean_distance.round(2))\n",
    "print(\"\\nCosine Distance\\n\", cosine_distance.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define count vectorizer for clustering \n",
    "#MIN_DF = 0.001\n",
    "#MAX_DF = 0.8\n",
    "vectorizer_clust = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "num_comments = 1000\n",
    "bag_of_words_clust = vectorizer_clust.fit_transform( processed_corpus[0:(num_comments-1)] ) \n",
    "processed_corpus_clust = processed_corpus[0:(num_comments-1)]\n",
    "corpus_clust = corpus[0:(num_comments-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #del corpus_clust[396]\n",
    "# def delete_row_csr(mat, i):\n",
    "#     \"\"\"\n",
    "#     Function to remove a row from a scipy.sparse.csr_matrix. \n",
    "#     Taken from \n",
    "#     http://stackoverflow.com/questions/13077527/is-there-a-numpy-delete-equivalent-for-sparse-matrices\n",
    "    \n",
    "#     params: \n",
    "#         scipy.sparse.csr_matrix mat\n",
    "#         int i: row to delete \n",
    "#     \"\"\"\n",
    "#     if not isinstance(mat, scipy.sparse.csr_matrix):\n",
    "#         raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "#     n = mat.indptr[i+1] - mat.indptr[i]\n",
    "#     if n > 0:\n",
    "#         mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "#         mat.data = mat.data[:-n]\n",
    "#         mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "#         mat.indices = mat.indices[:-n]\n",
    "#     mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "#     mat.indptr[i:] -= n\n",
    "#     mat.indptr = mat.indptr[:-1]\n",
    "#     mat._shape = (mat._shape[0]-1, mat._shape[1])\n",
    "    \n",
    "# corpus_clust = np.delete(corpus_clust, 396)\n",
    "# processed_corpus_clust = np.delete(corpus_clust, 396)\n",
    "# delete_row_csr(bag_of_words_clust, 396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering \n",
    "\n",
    "The idea behind agglomerative clustering is to cluster nearby data points together in a bottom up fashion. Suppose we have $n$ data points $\\{x_1, ..., x_N\\}$. First, we pre-compute the similarity between all data points using our choice of distance metric (affinity in sci-kit learn). We start with a collection of clusters $C$ that contains $N$ singleton clusters (cluster $c_i = {x_i}$). We repeat the following process until we have $K$ clusters: \n",
    "\n",
    "1. Find the closest pair of clusters ( $\\min_{i,j} D(c_i, c_j)$ ). $D$ depends on the similarity values between each element of each cluster.     \n",
    "2. Add a new cluster $c_{i+j}$ to collection $C$ such that $c_{i+j}= c_i \\cup c_j$.    \n",
    "3. Remove clusters $c_i$ and $c_j$ from $C$.    \n",
    "\n",
    "The distance $D$ is defined between clusters. The following distances are supported in sci-kit learn: \n",
    "\n",
    "* The *ward distance* minimizes the variance of the clusters being merged. I.e., it computes the sum of squares error. \n",
    "* The *average* distance uses the average of the distances of each observation of the two sets.\n",
    "* The *complete or maximum* distance uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "\n",
    "\\* Material is from this [tutorial](https://www.youtube.com/watch?v=XJ3194AmH40). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters = 4, affinity=\"cosine\", linkage =\"average\")\n",
    "cluster.fit(bag_of_words_clust.toarray())\n",
    "labels = cluster.labels_\n",
    "\n",
    "label_counter = Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activity 4: Cosine Deception \n",
    "**20 mins**\n",
    "\n",
    "Use the code above to find clusters in the data using [Agglomerative Clusting](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Examine the comments that are assigned to each cluster using the function `examine_comment_labels` defined below. Notice that the comments in Group 0 are all short comments. Wasn't the point of using cosine distance to prevent short comments from being similar?  \n",
    "\n",
    "See if you can figure out why these comments are being grouped together. What are some ways to fix this error? \n",
    "\n",
    "**Hint**: look at the comments after converting them into bags of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 997, 0: 1, 3: 1})\n",
      "False\n",
      "\n",
      "-------Group0-------\n",
      "\n",
      "\n",
      "Comment 396: Why are you doing this to yourself? \n",
      "\n",
      "\n",
      "-------Group1-------\n",
      "\n",
      "\n",
      "Comment 0: Systems of living have been built around us to make us miserable.  Idk where you are but if you can.  Go shooting.   Trust me on this. Something about shooting guns helps with that feeling.  \n",
      "\n",
      "Comment 1: Just logged back onto my alt. Thank you. That means a lot to me, I know I'm late and all but ... thanks.\n",
      "\n",
      "Comment 2: Hey, do you know what it is that makes you feel such hatred towards these things? Does the violence make your thoughts go in an unwanted direction? And when you say these things make them bad people, do you mean you think your friends are bad people for enjoying the shows/games, or that the people who make them are bad? It sounds like it could easily be a depression thing tbh.\n",
      "\n",
      "Comment 3: Right there with you. National merit scholar now failing classes. Gpa from 3.7 to 2.5. Depression is a bitch. You just have to find what works for you study wise. For me, empty classrooms help. Space and isolation. You have to be strict about setting study days. It's incredibly difficult and I'm nowhere near succeeding, but it's given me some improvement.  Do it for yourself, not your family. \n",
      "\n",
      "\n",
      "-------Group3-------\n",
      "\n",
      "\n",
      "Comment 261: prohibition of pornography generally increases incidence of rape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def examine_comment_labels(num_comments_to_view, label_counter, corpus_clust, num_comments):\n",
    "    \"\"\"\n",
    "    Function that prints comments and their cluster labels. The function can print\n",
    "    the raw comments, processed comments, or bag of words. \n",
    "    \n",
    "    params: \n",
    "        int num_comments_to_view: the number of comments to view for \n",
    "            each group\n",
    "        Counter label_counter: contains the cluster labels for each item \n",
    "            in the dataset\n",
    "        numpy.ndarray or scipy.sparse.csr.csr_matrix corpus: the corpus \n",
    "            processed corpus, or bag of words representation of our data \n",
    "        int num_comments: number of comments contained in corpus\n",
    "    \"\"\"\n",
    "    for key, value in label_counter.items():\n",
    "        count = 0\n",
    "        print(\"\\n-------Group{}-------\\n\\n\".format(key))\n",
    "        for i in range(0, num_comments-1):\n",
    "            if (labels[i] == key) & (count < num_comments_to_view):\n",
    "                print(\"Comment {0}: {1}\\n\".format(i, corpus_clust[i]))\n",
    "                #print(\"Comment {0}: {1}\\n\".format(i, processed_corpus[i]))\n",
    "                count += 1\n",
    "\n",
    "                \n",
    "# examine the comments that fall in each group\n",
    "# what defines each group?\n",
    "print(label_counter)\n",
    "num_comments_to_view = 4\n",
    "print('prohibition' in stopwords.words('english'))\n",
    "examine_comment_labels(num_comments_to_view, label_counter, corpus_clust, num_comments-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# clear memory \n",
    "corpus = None\n",
    "processed_corpus = None\n",
    "test_processed_corpus = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Dimensionality Reduction\n",
    "\n",
    "In this portion of the tutorial we will be extracting topics in the form of commonly co-occuring words from the corpus of data. A topic model is a generative model of document creation used for automatic summarization of text into topics, or mixtures of words. While many variants exist (e.g., supervised, unsupervised, dynamic, hierarchical, structural), this tutorial will cover the standard topic model. The standard topic model makes three assumptions: \n",
    "\n",
    "- Bag-of-words: semantic meaning can be extracted from word co-occurance\n",
    "- Dimensionality reduction is essential to understanding semantic information \n",
    "- Topics (lower dimensional units) can be expressed as a probability distribution over words \n",
    "\n",
    "\n",
    "<img src=\"figs/topic_model.png\" style=\"width :85%; height: 85%\"/>\n",
    "\n",
    "\n",
    "Return to [TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topic Model IO\n",
    "\n",
    "### Input     \n",
    "\n",
    "Document term count matrix    \n",
    "\n",
    "### Output \n",
    "\n",
    "**Topics**--the probability of each word being expressed in a given topic    \n",
    "**Document Mixtures**--the probability of each topic being expressed in a document  \n",
    "\n",
    "<img src=\"figs/topic_model_io.png\" style=\"width:85%; height:85%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Topics \n",
    "\n",
    "<img src=\"figs/example_topics.png\" style=\"width:85%; height:85%\"/>\n",
    "\n",
    "\\* These topics were extracted from a *supervised* topic model. In this case, the topics correspond to each subreddit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load and Preprocess Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading takes 4.061205s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "corpus_all, subreddit_id_all = load_reddit('./data/RC_2015-05.json',MIN_CHAR=250)\n",
    "end = time.time()\n",
    "print('Loading takes {0:2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for vectorizer \n",
    "ANALYZER = \"word\" # unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode' \n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,1) # Range for n-grams \n",
    "MIN_DF = 10/len(corpus_all) # Exclude words that are contained in less that x percent of documents \n",
    "MAX_DF = 0.9  # Exclude words that are contained in more than x percent of documents \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing took 21.793128967285156s\n"
     ]
    }
   ],
   "source": [
    "# Get rid of punctuation and set to lowercase  \n",
    "start = time.time()\n",
    "processed_corpus_all = [ re.sub( RE_PREPROCESS, ' ', comment).lower() for comment in corpus_all]\n",
    "\n",
    "#tokenzie the words\n",
    "bag_of_words_all = vectorizer.fit_transform( processed_corpus_all ) \n",
    "end = time.time() \n",
    "#grab the features/vocabulary\n",
    "features_all = vectorizer.get_feature_names()\n",
    "print(\"Processing took {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13832 words in vocabulary\n",
      "\n",
      "Counter({'depression': 12079, 'offmychest': 10661, 'stopdrinking': 6958, 'SuicideWatch': 6098, 'ADHD': 5674, 'GetMotivated': 4799, 'Anxiety': 4466, 'Meditation': 3608, 'stopsmoking': 1867, 'leaves': 1629, 'BPD': 1618, 'BipolarReddit': 1515, 'OpiatesRecovery': 1391, 'socialanxiety': 1221, 'StopGaming': 946, 'schizophrenia': 898, 'mentalhealth': 572, 'ptsd': 566, 'MMFB': 483, 'alcoholism': 378, 'rapecounseling': 283, 'BipolarSOs': 272, 'AlAnon': 267, 'ZenHabits': 237, 'alcoholicsanonymous': 219, 'getting_over_it': 208, 'EatingDisorders': 203, 'Anger': 165, 'selfhelp': 150, 'StopSelfHarm': 136, 'dpdr': 105, 'survivorsofabuse': 96, 'helpmecope': 45, 'AtheistTwelveSteppers': 29, 'problemgambling': 17, 'buddhistrecovery': 12, 'MaladaptiveDreaming': 9, 'PanicAttack': 8, 'psychoticreddit': 8, 'secularsobriety': 6, 'SMARTRecovery': 5, 'secondary_survivors': 5, 'hardshipmates': 4, 'PanicParty': 4, 'afterthesilence': 3, 'feelgood': 2, 'Existential_crisis': 1, 'jessiesparents': 1})\n",
      "69927\n"
     ]
    }
   ],
   "source": [
    "print(\"{} words in vocabulary\\n\".format(len(features_all)))\n",
    "\n",
    "# print subreddits and number of topics \n",
    "print(Counter(subreddit_id_all))\n",
    "print(len(subreddit_id_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit a Topic Model\n",
    "\n",
    "To create our topics we will use Latent Dirichlet Allocation (LDA, i.e., the standard topic model). Both [scikit learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and [gensim](https://radimrehurek.com/gensim/) provide software for fitting LDA. Both modules use [online variational inference](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf) to fit the model, but methods such as [Gibbs sampling](https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/steyvers06probabilistic.pdf) are also popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing took 442.95932483673096s\n",
      "Shape of document topic matrix: (69927, 50)\n",
      "Shape of topic word matrix: (50, 13832)\n"
     ]
    }
   ],
   "source": [
    "# Options\n",
    "N_TOPICS = 50\n",
    "N_TOP_WORDS = 10\n",
    "\n",
    "start = time.time()\n",
    "lda = LatentDirichletAllocation( n_topics = N_TOPICS )\n",
    "#lda.fit(bag_of_words_all)\n",
    "doctopic = lda.fit_transform(bag_of_words_all)\n",
    "end = time.time() \n",
    "print(\"Processing took {}s\".format(end- start)) # ~255s for ~1000 features, , ~475 for 13000\n",
    "\n",
    "print(\"Shape of document topic matrix: {}\".format(doctopic.shape))\n",
    "print(\"Shape of topic word matrix: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Display the top ten words for each topics \n",
    "\n",
    "Qualitatively examining the topic output is the most common method for assessing fit of a topic model. Ideally, most topics will be interpretable. Topic interpretability tends to increase with the number of documents used during fitting. Since this topic model is unsupervised, topic names (in this case integers $1,...,T$) are arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 meditation, mind, practice, body, eat, breath, mindfulness, breathing, gym, dog\n",
      "1 mental, health, book, treatment, kill, interested, hospital, state, called, coffee\n",
      "2 quite, must, news, movie, patterns, role, chemical, hello, ice, email\n",
      "3 stay, control, hate, get, worry, goes, easier, heart, husband, definitely\n",
      "4 meds, system, response, expect, information, insurance, memory, throw, handle, usually\n",
      "5 job, work, working, success, top, position, company, business, useful, career\n",
      "6 day, time, days, get, week, first, one, like, months, much\n",
      "7 years, life, never, got, back, still, let, went, home, guess\n",
      "8 answer, question, asking, kid, putting, research, doubt, terms, clear, bring\n",
      "9 money, using, jobs, higher, eye, spending, doc, attitude, hobby, picture\n",
      "10 wonder, opposite, strange, faith, occasionally, complicated, insight, hardly, master, agreed\n",
      "11 friends, relationship, friend, love, reading, girl, true, words, meet, person\n",
      "12 honestly, yet, lose, sex, weight, knows, healthy, ex, trust, wow\n",
      "13 society, upon, law, vent, sent, statement, ignorant, fantasy, prison, presence\n",
      "14 depressed, god, anti, disease, correct, smile, motivated, religion, air, credit\n",
      "15 told, one, mom, would, said, issue, asked, put, tried, room\n",
      "16 literally, reality, minutes, sitting, hour, fight, thought, meditate, sit, meditating\n",
      "17 today, aware, bpd, cancer, tend, overwhelming, episode, paid, complex, manic\n",
      "18 stress, horrible, knowing, moved, bullshit, difference, new, differently, voice, says\n",
      "19 guy, love, women, hold, guys, men, please, woman, someone, sorry\n",
      "20 thanks, giving, experiences, helping, choice, medical, mine, keeping, perfect, dude\n",
      "21 year, half, shit, fuck, changes, death, sucks, fucking, changed, girls\n",
      "22 pain, short, term, long, certainly, option, gain, ultimately, aspect, chronic\n",
      "23 sleep, effects, worked, exercise, side, food, eating, dose, water, energy\n",
      "24 emotionally, cases, catch, source, zero, permanent, picking, perception, remain, influence\n",
      "25 kids, play, suicide, gone, happiness, phone, awesome, writing, playing, online\n",
      "26 com, http, www, amp, reddit, wiki, https, org, en, wikipedia\n",
      "27 watch, listen, music, video, youtube, fighting, rock, temporary, favorite, nights\n",
      "28 smoking, quit, smoke, addiction, steps, currently, gaming, goals, nicotine, allow\n",
      "29 big, wife, car, study, test, forever, home, incredibly, drive, near\n",
      "30 help, depression, anxiety, doctor, also, therapy, therapist, get, medication, issues\n",
      "31 game, common, program, behavior, cannot, avoid, purpose, therapists, rarely, rate\n",
      "32 self, panic, continue, abuse, tomorrow, attack, proud, attacks, dr, harm\n",
      "33 normal, im, dont, easily, inside, speak, guess, listening, nearly, thats\n",
      "34 honest, decision, house, cool, normally, provide, except, seeking, anywhere, visit\n",
      "35 link, note, studies, idk, carry, related, valuable, notes, articles, use\n",
      "36 adhd, games, brain, lack, sister, use, several, also, dopamine, environment\n",
      "37 post, meeting, sorry, disorder, definitely, based, diagnosed, op, bipolar, diagnosis\n",
      "38 thoughts, positive, negative, recovery, hang, agree, supposed, step, believe, power\n",
      "39 social, hours, morning, focus, check, clean, motivation, day, gt, begin\n",
      "40 mg, tired, take, adderall, pills, allowed, rules, per, schizophrenia, save\n",
      "41 peace, focused, cost, paying, attention, trauma, expectations, acceptance, breaks, unhealthy\n",
      "42 read, books, skills, chat, science, fairly, stuff, teach, pull, individual\n",
      "43 drinking, drink, sober, alcohol, aa, drugs, sobriety, meetings, drunk, alcoholic\n",
      "44 like, people, know, think, feel, get, things, want, really, something\n",
      "45 please, automatically, message, subreddit, action, contact, tags, questions, concerns, compose\n",
      "46 child, area, present, different, certain, looked, brain, immediately, body, part\n",
      "47 kind, totally, share, sub, internet, whenever, appointment, resources, strangers, character\n",
      "48 thank, group, support, available, across, sign, apply, quiet, breathe, powerful\n",
      "49 school, parents, college, high, class, pay, job, degree, classes, university\n"
     ]
    }
   ],
   "source": [
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "    keywords = ', '.join( features_all[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Most Prevalent Topic in Each Comment\n",
    "\n",
    "We will examine some of the comments and list which topic comprises the largest portion of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_id: 0 offmychest topic_id: 44 \n",
      "keywords: like, people, know, think, feel, get, things, want, really, something\n",
      "---\n",
      "It's not your \u001b[31mfault\u001b[0m don't \u001b[31mthink\u001b[0m that. \u001b[31mHey\u001b[0m if this \u001b[31mhelps\u001b[0m it does then if it doesn't \u001b[31mwell.\u001b[0m At \u001b[31mleast\u001b[0m I can \u001b[31mtalk\u001b[0m about it. I \u001b[31mshot\u001b[0m my dog. I had him for 10 years. I can home from partying and it was \u001b[31mNew\u001b[0m Years so I was \u001b[31mgoing\u001b[0m to \u001b[31mshoot\u001b[0m off my .40 so... \u001b[31mWell\u001b[0m I was loading it \u001b[31moutside\u001b[0m and I \u001b[31mshot\u001b[0m it off accidentally. \u001b[31mRight\u001b[0m into my dog. Me having about 5-7 tequila shots into me \u001b[31mthough\u001b[0m if I \u001b[31mgo\u001b[0m to sleep I \u001b[31mwould\u001b[0m wake up and he \u001b[31mwould\u001b[0m be \u001b[31mokay.\u001b[0m \u001b[31mWell\u001b[0m if I \u001b[31mcalled\u001b[0m a vet when I \u001b[31mshot\u001b[0m him I \u001b[31mcould\u001b[0m have saved him. Or at \u001b[31mleast\u001b[0m from his pain. So I woke up \u001b[31msaw\u001b[0m a \u001b[31mdead\u001b[0m dog in my yard… i cried for hours. That \u001b[31mold\u001b[0m dog \u001b[31mstill\u001b[0m had a few \u001b[31mgood\u001b[0m years in him. That was my \u001b[31mfault\u001b[0m and you sitting on a \u001b[31mlittle\u001b[0m \u001b[31mtiny\u001b[0m mouse is more \u001b[31munderstandable\u001b[0m than a \u001b[31mman\u001b[0m discharging a firearm into a shitzu. Don't \u001b[31mfeel\u001b[0m to \u001b[31mbad.\u001b[0m\n",
      "===\n",
      "comment_id: 1 offmychest topic_id: 11 \n",
      "keywords: friends, relationship, friend, love, reading, girl, true, words, meet, person\n",
      "---\n",
      "You do have a \u001b[31mvalid\u001b[0m point--I've \u001b[31malways\u001b[0m been a \u001b[31mbit\u001b[0m of a \u001b[31mbehind\u001b[0m the \u001b[31mscenes\u001b[0m \u001b[31mperson.\u001b[0m While I'm \u001b[31msuper\u001b[0m \u001b[31mbubbly\u001b[0m and extraverted \u001b[31mIRL,\u001b[0m I'm not too \u001b[31mbig\u001b[0m on \u001b[31mbringing\u001b[0m \u001b[31mmuch\u001b[0m \u001b[31mattention\u001b[0m to myself. I don't \u001b[31meven\u001b[0m have a \u001b[31mFacebook\u001b[0m because I just \u001b[31mfound\u001b[0m it so \u001b[31mweird\u001b[0m over the \u001b[31myears.\u001b[0m \u001b[31mPerhaps\u001b[0m I \u001b[31mkinda\u001b[0m \u001b[31minadvertently\u001b[0m \u001b[31mset\u001b[0m a \u001b[31mstandard\u001b[0m \u001b[31mwithout\u001b[0m \u001b[31mrealizing\u001b[0m it, and \u001b[31mreading\u001b[0m this \u001b[31mmade\u001b[0m me \u001b[31mreflect.\u001b[0m I \u001b[31mreached\u001b[0m out to some \u001b[31mpeople\u001b[0m today after \u001b[31mreading\u001b[0m this \u001b[31mlast\u001b[0m \u001b[31mnight\u001b[0m and \u001b[31mthinking\u001b[0m \u001b[31mthings\u001b[0m through. Some \u001b[31mpeople\u001b[0m are \u001b[31mstill\u001b[0m jerks--but that's just who they are and I \u001b[31mknow\u001b[0m to just \u001b[31mstay\u001b[0m \u001b[31maway\u001b[0m from them. \u001b[31mOthers\u001b[0m did \u001b[31mcare.\u001b[0m Again, my \u001b[31mfamily\u001b[0m and my \u001b[31mboyfriend's\u001b[0m (he's \u001b[31mcoming\u001b[0m with) don't \u001b[31mreally,\u001b[0m and that \u001b[31mcuts\u001b[0m \u001b[31mreal\u001b[0m \u001b[31mdeep,\u001b[0m but what are \u001b[31mya\u001b[0m \u001b[31mgonna\u001b[0m do? I do \u001b[31magree\u001b[0m with the other commenter \u001b[31mthough\u001b[0m as \u001b[31mwell.\u001b[0m I \u001b[31mthink\u001b[0m it's \u001b[31mone\u001b[0m \u001b[31mthing\u001b[0m to \u001b[31msuggest\u001b[0m \u001b[31msomething,\u001b[0m as in \u001b[31m\"Hey\u001b[0m \u001b[31mmaybe\u001b[0m we should do this \u001b[31msmall\u001b[0m \u001b[31mthing\u001b[0m Saturday for my \u001b[31mbirthday\",\u001b[0m but in my \u001b[31mexperience,\u001b[0m I've \u001b[31mknown\u001b[0m \u001b[31mway\u001b[0m too \u001b[31mmany\u001b[0m \u001b[31mpeople\u001b[0m who, when they \u001b[31msuggest\u001b[0m such \u001b[31mthings,\u001b[0m are very over the \u001b[31mtop\u001b[0m and it just \u001b[31malways\u001b[0m \u001b[31mcame\u001b[0m to me as tacky, \u001b[31mespecially\u001b[0m (to me) if they haven't \u001b[31mreally\u001b[0m \u001b[31machieved\u001b[0m \u001b[31manything.\u001b[0m\n",
      "===\n",
      "comment_id: 2 depression topic_id: 44 \n",
      "keywords: like, people, know, think, feel, get, things, want, really, something\n",
      "---\n",
      "\u001b[31mHey,\u001b[0m do you \u001b[31mknow\u001b[0m what it is that \u001b[31mmakes\u001b[0m you \u001b[31mfeel\u001b[0m such hatred towards these \u001b[31mthings?\u001b[0m Does the violence \u001b[31mmake\u001b[0m your thoughts \u001b[31mgo\u001b[0m in an unwanted direction? And when you \u001b[31msay\u001b[0m these \u001b[31mthings\u001b[0m \u001b[31mmake\u001b[0m them \u001b[31mbad\u001b[0m \u001b[31mpeople,\u001b[0m do you \u001b[31mmean\u001b[0m you \u001b[31mthink\u001b[0m your \u001b[31mfriends\u001b[0m are \u001b[31mbad\u001b[0m \u001b[31mpeople\u001b[0m for enjoying the shows/games, or that the \u001b[31mpeople\u001b[0m who \u001b[31mmake\u001b[0m them are \u001b[31mbad?\u001b[0m It \u001b[31msounds\u001b[0m \u001b[31mlike\u001b[0m it \u001b[31mcould\u001b[0m easily be a depression \u001b[31mthing\u001b[0m tbh.\n",
      "===\n",
      "comment_id: 3 depression topic_id: 29 \n",
      "keywords: big, wife, car, study, test, forever, home, incredibly, drive, near\n",
      "---\n",
      "\u001b[31mRight\u001b[0m there with you. \u001b[31mNational\u001b[0m \u001b[31mmerit\u001b[0m \u001b[31mscholar\u001b[0m now \u001b[31mfailing\u001b[0m classes. Gpa from 3.7 to 2.5. \u001b[31mDepression\u001b[0m is a \u001b[31mbitch.\u001b[0m You just have to \u001b[31mfind\u001b[0m what \u001b[31mworks\u001b[0m for you \u001b[31mstudy\u001b[0m \u001b[31mwise.\u001b[0m For me, \u001b[31mempty\u001b[0m classrooms \u001b[31mhelp.\u001b[0m \u001b[31mSpace\u001b[0m and \u001b[31misolation.\u001b[0m You have to be \u001b[31mstrict\u001b[0m about \u001b[31msetting\u001b[0m \u001b[31mstudy\u001b[0m \u001b[31mdays.\u001b[0m It's \u001b[31mincredibly\u001b[0m \u001b[31mdifficult\u001b[0m and I'm \u001b[31mnowhere\u001b[0m \u001b[31mnear\u001b[0m \u001b[31msucceeding,\u001b[0m but it's \u001b[31mgiven\u001b[0m me some \u001b[31mimprovement.\u001b[0m Do it for yourself, not your \u001b[31mfamily.\u001b[0m\n",
      "===\n",
      "comment_id: 4 ADHD topic_id: 31 \n",
      "keywords: game, common, program, behavior, cannot, avoid, purpose, therapists, rarely, rate\n",
      "---\n",
      "I \u001b[31mcannot\u001b[0m \u001b[31mput\u001b[0m into \u001b[31mwords\u001b[0m how \u001b[31mmuch\u001b[0m I \u001b[31mlove\u001b[0m \u001b[31mprogramming.\u001b[0m So \u001b[31mmany\u001b[0m \u001b[31mavenues\u001b[0m to \u001b[31mgo\u001b[0m down, so \u001b[31mmany\u001b[0m \u001b[31mways\u001b[0m to innovate, and the very \u001b[31mreal\u001b[0m \u001b[31mpossibility\u001b[0m of \u001b[31mchanging\u001b[0m the \u001b[31mworld\u001b[0m just by doing it a \u001b[31mlot.\u001b[0m I \u001b[31mwork\u001b[0m at a \u001b[31mweb\u001b[0m \u001b[31mdevelopment\u001b[0m \u001b[31mcompany\u001b[0m and do \u001b[31mindependent\u001b[0m \u001b[31mgame\u001b[0m \u001b[31mdesign\u001b[0m in my off \u001b[31mtime,\u001b[0m and \u001b[31mrarely\u001b[0m \u001b[31mfeel\u001b[0m burnt out. If I \u001b[31mcould\u001b[0m add an \u001b[31mextra\u001b[0m \u001b[31mday\u001b[0m to the \u001b[31mweek\u001b[0m to \u001b[31mprogram,\u001b[0m I \u001b[31mwould\u001b[0m do it \u001b[31mtwice.\u001b[0m\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "p = 95 # percentile of words to highlight \n",
    "num_comments = 5\n",
    "for comment_id in range(num_comments):\n",
    "    \n",
    "    # get most prevalent topic \n",
    "    topic_id = np.argsort(doctopic[comment_id])[N_TOPICS-1]\n",
    "    \n",
    "    # find most prevalent words in the most prevalent topic\n",
    "    topic = lda.components_[topic_id]\n",
    "    feat_idx = np.argsort(topic)[::-1]\n",
    "    num_words = (topic > np.percentile(topic, p)).sum()\n",
    "    feat_idx = feat_idx[0:num_words]\n",
    "    top_features = [features_all[i] for i in feat_idx]\n",
    "    \n",
    "    # create highlighted version of the comment \n",
    "    comment_color = []\n",
    "    comment = corpus_all[comment_id].split()\n",
    "    \n",
    "    for word in comment: \n",
    "        if re.sub(RE_PREPROCESS, '', word).lower() in top_features:\n",
    "            comment_color.append('\\x1b[31m' + word + '\\x1b[0m') # makes the word print in red \n",
    "        else:\n",
    "            comment_color.append(word)\n",
    "    \n",
    "    print('comment_id:',\n",
    "          comment_id,\n",
    "          subreddit_id_all[comment_id],\n",
    "          'topic_id:',\n",
    "          topic_id,\n",
    "          '\\nkeywords:',\n",
    "           ls_keywords[topic_id])\n",
    "    print('---')\n",
    "    print(' '.join(comment_color))\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comment with Largest Proportion of Each Topic\n",
    "\n",
    "We will iterate through topics and find the comment that most expresses that topic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_id: 53965 stopsmoking topic_id: 6 \n",
      "keywords: day, time, days, get, week, first, one, like, months, much\n",
      "---\n",
      "I have been \u001b[31msmoking\u001b[0m a \u001b[31mpack\u001b[0m a \u001b[31mday\u001b[0m \u001b[31msince\u001b[0m my \u001b[31mearly\u001b[0m 20s. I'm 45 now. I \u001b[31mquit\u001b[0m 309 \u001b[31mdays\u001b[0m \u001b[31mago,\u001b[0m and I will \u001b[31mnever\u001b[0m touch \u001b[31manother\u001b[0m \u001b[31mcigarette\u001b[0m again. It's been the \u001b[31mbest\u001b[0m decision in my \u001b[31mlife.\u001b[0m This sub did it for me. The short version: you're addicted to nicotine, the \u001b[31maddiction\u001b[0m is real, junkie-you will tell yourself anything to \u001b[31mget\u001b[0m \u001b[31mback\u001b[0m to \u001b[31msmoking,\u001b[0m don't do it. It \u001b[31mmay\u001b[0m seem \u001b[31mdifficult\u001b[0m now, but you basically have a nicotine hangover. It will \u001b[31mgo\u001b[0m \u001b[31maway\u001b[0m if you don't \u001b[31msmoke\u001b[0m (stay sober). It's worth it, \u001b[31msmoking\u001b[0m is silly, you don't need nicotine. The \u001b[31mlong\u001b[0m version: Here are my thoughts on \u001b[31mquitting,\u001b[0m rehashed from a post I \u001b[31mmade\u001b[0m when I was 30 \u001b[31mdays\u001b[0m in. I \u001b[31mknow\u001b[0m there are other thoughts, but these are mine. **How / when to \u001b[31mquit:**\u001b[0m * \u001b[31mQuitting\u001b[0m is easy, just \u001b[31mstop\u001b[0m \u001b[31msmoking.\u001b[0m \u001b[31mGetting\u001b[0m the nicotine out of your \u001b[31mbody\u001b[0m only takes three \u001b[31mdays.\u001b[0m * \u001b[31mCold\u001b[0m \u001b[31mturkey\u001b[0m is the \u001b[31mbest\u001b[0m \u001b[31mway\u001b[0m to \u001b[31mquit.\u001b[0m Gum, patches, ecigs, just prolong the suffering and \u001b[31mkeep\u001b[0m you addicted to, and thinking about, nicotine for a \u001b[31mlong\u001b[0m \u001b[31mtime.\u001b[0m They \u001b[31malso\u001b[0m increase the \u001b[31mchance\u001b[0m of relapse. I failed many \u001b[31mtimes\u001b[0m to \u001b[31mquit\u001b[0m using NRT. * However, \u001b[31meverything\u001b[0m is \u001b[31mbetter\u001b[0m than \u001b[31msmoking,\u001b[0m so if NRT does \u001b[31mhelp\u001b[0m you to \u001b[31mstop,\u001b[0m or if it shows you that you'll be \u001b[31mfine\u001b[0m \u001b[31mwithout\u001b[0m \u001b[31msmoking,\u001b[0m or if it will \u001b[31mhelp\u001b[0m you \u001b[31mget\u001b[0m \u001b[31mrid\u001b[0m of the \u001b[31mmental\u001b[0m \u001b[31maddiction\u001b[0m to cigarettes, you should do it. Just \u001b[31mremember\u001b[0m that if you fail to \u001b[31mquit\u001b[0m that \u001b[31mway,\u001b[0m \u001b[31mcold\u001b[0m \u001b[31mturkey\u001b[0m is always an option * Don't be afraid if you \u001b[31mquit.\u001b[0m The \u001b[31mpanic\u001b[0m is just the addict inside of you \u001b[31mgoing\u001b[0m \u001b[31mwithout\u001b[0m the nicotine. It will \u001b[31mgo\u001b[0m \u001b[31maway\u001b[0m very soon once you realize the freedom you just gained. * \u001b[31mRemember\u001b[0m that nicotine is just a poison, not a nutrient. You'll be \u001b[31mfine\u001b[0m \u001b[31mwithout\u001b[0m it. * Don't make a ritual out of your \u001b[31mquit.\u001b[0m Don't \u001b[31mset\u001b[0m a date to \u001b[31mquit,\u001b[0m just do it today. Setting a date will only give you \u001b[31mtime\u001b[0m to come up with reasons to not do it. * There's no such \u001b[31mthing\u001b[0m as a \u001b[31mbad\u001b[0m \u001b[31mtime\u001b[0m to \u001b[31mquit.\u001b[0m It doesn't exist. Now is the \u001b[31mbest\u001b[0m \u001b[31mtime.\u001b[0m Anything else is just the nicotine junkie talking. Don't let it come up with a real \u001b[31mgood\u001b[0m reason to not \u001b[31mquit,\u001b[0m or not \u001b[31mquit\u001b[0m \u001b[31mright\u001b[0m now. \u001b[31mSee\u001b[0m the junkie for who he/she is. * Have that \u001b[31mlast\u001b[0m \u001b[31msmoke\u001b[0m if you \u001b[31mwant\u001b[0m to, but realize you're not giving \u001b[31msomething\u001b[0m up of value. You're just no \u001b[31mlonger\u001b[0m systematically choking yourself. \u001b[31m**Get\u001b[0m support and track your progress** * \u001b[31mGet\u001b[0m support, and burn all bridges. \u001b[31mGet\u001b[0m a badge here, tell your story. Tell family, \u001b[31mfriends,\u001b[0m colleagues and ask for their support. Hang a X \u001b[31mdays\u001b[0m smoke-free paper on your \u001b[31mwork\u001b[0m desk. Bring cake to \u001b[31mwork\u001b[0m after being nicotine free for \u001b[31mtwo\u001b[0m \u001b[31mweeks\u001b[0m / \u001b[31mone\u001b[0m \u001b[31mmonth.\u001b[0m Tattoo your \u001b[31mquit\u001b[0m date on your forehead. * Tell the \u001b[31mpeople\u001b[0m where you buy your smokes that you have \u001b[31mquit,\u001b[0m and that under no circumstance should they sell you any more. * Download an app to \u001b[31msee\u001b[0m how many \u001b[31mdays,\u001b[0m hours, mins, and secs have \u001b[31mpassed\u001b[0m \u001b[31msince\u001b[0m your \u001b[31mlast\u001b[0m \u001b[31msmoke.\u001b[0m Track the number of cigarettes not \u001b[31msmoked,\u001b[0m and money saved. * \u001b[31mRead\u001b[0m up on health \u001b[31mbenefits\u001b[0m of not \u001b[31msmoking,\u001b[0m so you understand what to expect and when. * I \u001b[31mmade\u001b[0m a wager with \u001b[31meveryone\u001b[0m that \u001b[31mwanted\u001b[0m \u001b[31mone.\u001b[0m I will pay 10 to 1 if I \u001b[31msmoke\u001b[0m again \u001b[31mwithin\u001b[0m a year. If I do, I will loose approximately 5000 dollars. If I don't I will win 500 dollars (for charity). * I mean you're \u001b[31mserious\u001b[0m about \u001b[31mquitting,\u001b[0m \u001b[31mright?\u001b[0m You're not just \u001b[31m\"trying\u001b[0m to \u001b[31mquit\",\u001b[0m but you will be a non-smoker. Decide how \u001b[31mserious\u001b[0m you are, and burn those bridges, and then nuke them to make \u001b[31msure.\u001b[0m **The detox phase** * If you \u001b[31mgo\u001b[0m \u001b[31mcold\u001b[0m \u001b[31mturkey,\u001b[0m it should only \u001b[31mlast\u001b[0m a \u001b[31mcouple\u001b[0m of \u001b[31mdays.\u001b[0m \u001b[31mFirst\u001b[0m \u001b[31mcouple\u001b[0m of \u001b[31mdays\u001b[0m can be a \u001b[31mlittle\u001b[0m annoying, but it will \u001b[31mget\u001b[0m easier very soon. * \u001b[31mCravings,\u001b[0m should you have them (I didn't this \u001b[31mtime),\u001b[0m only \u001b[31mlast\u001b[0m 3 mins max. Watch a timer on your phone if you have to. \u001b[31mEvery\u001b[0m \u001b[31mcraving\u001b[0m is a reminder of how awesome you are to have \u001b[31mstopped.\u001b[0m * Drink lots of \u001b[31mfruit\u001b[0m juice the \u001b[31mfirst\u001b[0m \u001b[31mcouple\u001b[0m of \u001b[31mdays,\u001b[0m breathe. You \u001b[31mmay\u001b[0m \u001b[31mfeel\u001b[0m down a \u001b[31mlittle.\u001b[0m Don't let it \u001b[31mget\u001b[0m to you. * Don't \u001b[31mkeep\u001b[0m cigarettes around, and \u001b[31mavoid\u001b[0m smokers and \u001b[31mstrong\u001b[0m triggers \u001b[31mlike\u001b[0m alcohol until you \u001b[31mknow\u001b[0m you'll be \u001b[31mfine\u001b[0m around them. * Don't listen to yourself talking you into \u001b[31msmoking\u001b[0m again. This includes: \"now is not a \u001b[31mgood\u001b[0m \u001b[31mtime\u001b[0m to \u001b[31mstop\",\u001b[0m \"it's too \u001b[31mdifficult\u001b[0m for me\", \"I can't \u001b[31mstop\u001b[0m this \u001b[31mway\",\u001b[0m \"I can just cut \u001b[31mback\u001b[0m slowly\", \"I should have just \u001b[31mone\u001b[0m to make me \u001b[31mfeel\u001b[0m \u001b[31mbetter\",\u001b[0m \"I will \u001b[31mquit\u001b[0m \u001b[31manother\u001b[0m \u001b[31mtime\".\u001b[0m \u001b[31metc.\u001b[0m Junkie alarm, typical addict behavior to \u001b[31mthink\u001b[0m \u001b[31mlike\u001b[0m that! * \u001b[31mStop\u001b[0m being a wuss, and just \u001b[31mget\u001b[0m it over with. Stay in \u001b[31mbed\u001b[0m eating ice cream if you \u001b[31mwant\u001b[0m to, but don't \u001b[31msmoke.\u001b[0m **The battle inside your \u001b[31mhead**\u001b[0m * The biggest battle is in the mind, not in the detox. Make \u001b[31msure\u001b[0m you \u001b[31mreally\u001b[0m \u001b[31mwant\u001b[0m to \u001b[31mquit\u001b[0m before you do it. Make \u001b[31msure\u001b[0m you're ready to fight the addict inside of you. * \u001b[31mKnow\u001b[0m that \u001b[31msmoking\u001b[0m is an \u001b[31maddiction\u001b[0m and not a habit. Realize that your own \u001b[31mbrain\u001b[0m is \u001b[31mtrying\u001b[0m to trick you into believing you need to, or you can, \u001b[31msmoke\u001b[0m again. * All \u001b[31mpeople\u001b[0m that \u001b[31msmoke\u001b[0m on a regular basis are nicotine junkies. \u001b[31mEven\u001b[0m those that \u001b[31msay\u001b[0m they can \u001b[31mgo\u001b[0m for \u001b[31mdays\u001b[0m \u001b[31mwithout.\u001b[0m There's no so \u001b[31mthing\u001b[0m as casual \u001b[31msmoking,\u001b[0m social \u001b[31msmoking,\u001b[0m not buying your own smokes, or just \u001b[31msmoking\u001b[0m when you drink beer. That's just the junkie talking. These \u001b[31mthings\u001b[0m aren't real, they're just excuses for \u001b[31msmoking.\u001b[0m * Understand that \u001b[31msmoking\u001b[0m brings \u001b[31mnothing\u001b[0m of value to you. It doesn't relax you, it doesn't \u001b[31mhelp\u001b[0m with concentration, it doesn't \u001b[31msmell\u001b[0m or taste \u001b[31mgood,\u001b[0m it doesn't \u001b[31meven\u001b[0m give you a \u001b[31mhigh.\u001b[0m It just fills the hole \u001b[31mleft\u001b[0m by the \u001b[31mlast\u001b[0m \u001b[31mcigarette.\u001b[0m Watch the Alan Carr video (google it). * Realize fully that you're not giving \u001b[31msomething\u001b[0m up, you're not sacrificing \u001b[31msomething\u001b[0m by not \u001b[31msmoking.\u001b[0m This is \u001b[31mimportant.\u001b[0m Don't \u001b[31mromanticize\u001b[0m the smokes, don't become sentimental about your nicotine \u001b[31maddiction.\u001b[0m That's all just typical junkie talk. * \u001b[31mRead\u001b[0m up on all \u001b[31mbenefits\u001b[0m of being a non-smoker. * Be a non-smoker, and do and \u001b[31mfeel\u001b[0m all the \u001b[31mthings\u001b[0m non-smokers do and \u001b[31mfeel.\u001b[0m Non-smokers \u001b[31mwould\u001b[0m \u001b[31mnever\u001b[0m \u001b[31mreach\u001b[0m for a \u001b[31msmoke\u001b[0m if they \u001b[31mfeel\u001b[0m \u001b[31mbad,\u001b[0m stressed, \u001b[31mgood,\u001b[0m bored. * You're a \u001b[31mnon\u001b[0m \u001b[31msmoker.\u001b[0m You have \u001b[31mreally\u001b[0m \u001b[31mstopped.\u001b[0m \u001b[31mNever\u001b[0m have that \u001b[31mfirst\u001b[0m \u001b[31mcigarette.\u001b[0m Now that you're a non-smoker, don't \u001b[31mstart\u001b[0m \u001b[31msmoking.\u001b[0m Nicotine is highly \u001b[31maddictive\u001b[0m (more so than crack and heroin), and \u001b[31mone\u001b[0m \u001b[31mcigarette\u001b[0m can/will \u001b[31mget\u001b[0m you \u001b[31mhooked\u001b[0m again. Don't make the same mistake you \u001b[31mmade\u001b[0m when you \u001b[31mfirst\u001b[0m \u001b[31mstarted.\u001b[0m * Three options: (1) \u001b[31msmoke\u001b[0m just \u001b[31mone\u001b[0m more and be a \u001b[31msmoker\u001b[0m for the rest of your live, (2) \u001b[31msmoke\u001b[0m just \u001b[31mone\u001b[0m more and \u001b[31mgo\u001b[0m through detox / \u001b[31mcravings\u001b[0m again, and (3) don't have \u001b[31meven\u001b[0m just \u001b[31mone.\u001b[0m You chose which \u001b[31mone\u001b[0m you \u001b[31mwant.\u001b[0m * When you're \u001b[31mfeeling\u001b[0m \u001b[31mgood,\u001b[0m don't become arrogant and \u001b[31mthink\u001b[0m you've \u001b[31mgot\u001b[0m this or that \u001b[31mquitting\u001b[0m is easy for you. You can't just have \u001b[31mone,\u001b[0m not \u001b[31meven\u001b[0m when you're \u001b[31mfeeling\u001b[0m you're in control, because you \u001b[31mnever\u001b[0m are with \u001b[31msomething\u001b[0m as \u001b[31maddictive\u001b[0m as nicotine. \u001b[31mRemember\u001b[0m what it was \u001b[31mlike\u001b[0m to be a junkie. Thanks all , goodbye and I hope that you all succeed and lead \u001b[31mlong,\u001b[0m happy, and healthy lives.\n",
      "===\n",
      "comment_id: 61416 depression topic_id: 25 \n",
      "keywords: kids, play, suicide, gone, happiness, phone, awesome, writing, playing, online\n",
      "---\n",
      "\u001b[31mHi,\u001b[0m I am not \u001b[31mactually\u001b[0m diagnosed with \u001b[31mdepression,\u001b[0m \u001b[31msometimes\u001b[0m I \u001b[31mthink\u001b[0m it \u001b[31mmay\u001b[0m be the case hence why I am browsing this subreddit, but I \u001b[31mlove\u001b[0m the hiphops so I figured I would share with you. \u001b[31mIm\u001b[0m not to familiar with Yelawolf, but that \u001b[31msong\u001b[0m you posted was pretty \u001b[31mgood.\u001b[0m The \u001b[31msong\u001b[0m that \u001b[31musually\u001b[0m sticks with me when it comes to songs about \u001b[31mdepression\u001b[0m is Illogic's \"Hate is a Puddle.\" I spent many \u001b[31mtimes\u001b[0m teary eyed listening to this \u001b[31msong,\u001b[0m it definitely hit a spot within my brain. A choice quote, although the whole \u001b[31msong\u001b[0m is quotable. \u001b[31mAlot\u001b[0m of other Illogic songs are \u001b[31mreal\u001b[0m \u001b[31mnice\u001b[0m to. \"Cause inside he fights \u001b[31mfeelings\u001b[0m that he was a mistake by God I \u001b[31msee\u001b[0m his confusion and self-deception Questions of relevance and intelligence He holds an illusion of self-acceptance That he shows to those outside lookin in\" https://www.youtube.com/watch?v=ICB-TbY96_8 It was kind of a weird \u001b[31mtime\u001b[0m to be a hiphop fan, that grew up with hiphop in the mid to \u001b[31mlate\u001b[0m 90s, in the \u001b[31mearly\u001b[0m 2000s. Towards the end of the 90s \u001b[31mearly\u001b[0m 2000s, Mase and Puffy and other terrible rappers \u001b[31mseemed\u001b[0m to be almost all you would hear. \u001b[31mThanks\u001b[0m to the \u001b[31minternet\u001b[0m I \u001b[31mdiscovered\u001b[0m underground hiphop. The stuff that you would not \u001b[31msee\u001b[0m on Yo MTV Raps or BET. \u001b[31mLooking\u001b[0m \u001b[31mback\u001b[0m \u001b[31malot\u001b[0m of the stuff that I thought was \u001b[31mdeep\u001b[0m and \u001b[31mbetter\u001b[0m than the mainstream hiphop did not hold up \u001b[31mwell\u001b[0m (chalk it up to my \u001b[31mlate\u001b[0m teens \u001b[31mearly\u001b[0m 20s trying to be \"cool\" by not listening to any of that popular \u001b[31mshit).\u001b[0m But there was \u001b[31mstill\u001b[0m some decent stuff being \u001b[31mdone\u001b[0m during the \u001b[31mtime,\u001b[0m \u001b[31malot\u001b[0m of the underground people's were trying to hide their lack of rapping ability/ear for \u001b[31mbeats\u001b[0m with the fact that poured their hearts into their lyrics. Sage Francis is \u001b[31mone\u001b[0m of those artists, \u001b[31malot\u001b[0m of his stuff does not \u001b[31mreally\u001b[0m hold up for me now a \u001b[31mdays,\u001b[0m but \u001b[31mone\u001b[0m of his strongest songs that I feel \u001b[31mstill\u001b[0m holds up is \"Inherited Scars.\" Deals with his sister revealing that she cuts herself to him (dont \u001b[31mknow\u001b[0m if it is a \u001b[31mtrue\u001b[0m story), doesnt \u001b[31mdeal\u001b[0m with \u001b[31mdepression\u001b[0m directly, but deals with \u001b[31mmental\u001b[0m \u001b[31mhealth.\u001b[0m https://www.youtube.com/watch?v=9VL0mB8ILeo Years later Sage Francis came out with his \u001b[31msong\u001b[0m \"The \u001b[31mBest\u001b[0m of \u001b[31mTimes.\"\u001b[0m Sage Francis was \u001b[31mone\u001b[0m of those \u001b[31m\"deep\"\u001b[0m \u001b[31mearly\u001b[0m 2000s rappers who put out some stuff that held up and some other stuff that was kind of a \"this wasnt \u001b[31mreally\u001b[0m that \u001b[31mgood.\"\u001b[0m But he did \u001b[31mmake\u001b[0m stuff that seems to stick with me when it comes to songs I would \u001b[31mlike\u001b[0m to listen to when I am not \u001b[31mfeeling\u001b[0m the \u001b[31mhappiness\u001b[0m of the \u001b[31mworld.\u001b[0m \"The \u001b[31mBest\u001b[0m of \u001b[31mTimes\"\u001b[0m is about loneliness, sadness, awkwardness but \u001b[31malso\u001b[0m about maybe \u001b[31mthings\u001b[0m might be ok. https://www.youtube.com/watch?v=VA8hzUDXvtk Murs has been doing his \u001b[31mthing\u001b[0m \u001b[31msince\u001b[0m the \u001b[31mlate\u001b[0m 90s, he has \u001b[31mmade\u001b[0m \u001b[31malot\u001b[0m of songs/albums, again some dont \u001b[31malways\u001b[0m hold up today, but \u001b[31malot\u001b[0m of that has to do with the \u001b[31mrecording\u001b[0m of the albums. Recorded on a low technology/money so they can be kind of \u001b[31mhard\u001b[0m to listen to as the quality is not \u001b[31mgreat.\u001b[0m His \u001b[31msong\u001b[0m \"Varsity Blues\" is more about \u001b[31mgetting\u001b[0m older, but I can \u001b[31msee\u001b[0m some parallels between his lyrics that he \u001b[31muses\u001b[0m to describe \u001b[31mgetting\u001b[0m older and some of the symptoms some \u001b[31mpeople\u001b[0m experience with \u001b[31mdepression.\u001b[0m https://www.youtube.com/watch?v=36HPOA4mN8U My \u001b[31mfavorite\u001b[0m producer to emerge in the \u001b[31mlast\u001b[0m decade Apollo Brown \u001b[31mgot\u001b[0m together with \u001b[31mtwo\u001b[0m emcees who werent \u001b[31mreally\u001b[0m known to me, and I am not \u001b[31msure\u001b[0m if either of them had a huge \u001b[31mfollowing\u001b[0m before this \u001b[31malbum,\u001b[0m but the 3 of them came together to produce my 2014 \u001b[31malbum\u001b[0m of the year. The Ugly Heroes \u001b[31malbum\u001b[0m is an ode to the \u001b[31mworking\u001b[0m man/woman in modern America. While no songs directly \u001b[31mdeal\u001b[0m with \u001b[31mdepression,\u001b[0m the \u001b[31missue\u001b[0m of \u001b[31mdepression\u001b[0m is sprinkled throughout the \u001b[31malbum.\u001b[0m The double burdens of being part of the \u001b[31mworking\u001b[0m poor and \u001b[31mdepression\u001b[0m heavily influences the lyrics through this \u001b[31malbum.\u001b[0m The lyrics are \u001b[31mbacked\u001b[0m up by some stellar production, seriously Apollo Brown is \u001b[31mone\u001b[0m of the greatest hiphop producers of all \u001b[31mtime.\u001b[0m I listen to this \u001b[31malbum\u001b[0m \u001b[31malot.\u001b[0m \"Desperate\" starts the \u001b[31malbum\u001b[0m off so that you \u001b[31mknow\u001b[0m what you are about to \u001b[31mget\u001b[0m into, the despair and hopelessness so many in modern society feel yet \u001b[31musually\u001b[0m \u001b[31mkeep\u001b[0m to themselves. Verbal Kent - \u001b[31m\"I'm\u001b[0m in my flat flat broke, \u001b[31mlast\u001b[0m smoke The voice in the \u001b[31mback\u001b[0m of my mind sighs have hope I have rope, I \u001b[31mcould\u001b[0m end it all feelin' that Local rapper found hanging from a ceiling fan That would be the \u001b[31measy\u001b[0m \u001b[31mway\u001b[0m out \u001b[31mthough\u001b[0m But what would that amount to apart from a outro?\" https://www.youtube.com/watch?v=xxXzkm67yPw \"Long \u001b[31mDrive\u001b[0m Home\" continues the theme of the downtrodden \u001b[31mworking\u001b[0m \u001b[31mman.\u001b[0m Although this album's theme is depressing and \u001b[31mdark,\u001b[0m listening to this \u001b[31malbum\u001b[0m makes me feel \u001b[31mgood,\u001b[0m it might be the production or it might be the \u001b[31mway\u001b[0m the emcees were \u001b[31mable\u001b[0m to capture many of the \u001b[31mfeelings\u001b[0m I have but cant put into such words. Maybe its because \u001b[31m\"every\u001b[0m \u001b[31mgood\u001b[0m hero \u001b[31mneed\u001b[0m a theme \u001b[31msong.\"\u001b[0m Red Pill - \"These student loans, this stupid phones I rarely \u001b[31meven\u001b[0m answer, peoples wonder what I do at home I sit \u001b[31maround\u001b[0m with Captain, eat and drink a \u001b[31mlot\u001b[0m Cause I don't \u001b[31mlike\u001b[0m to \u001b[31mthink\u001b[0m a \u001b[31mlot\u001b[0m Cause when I \u001b[31mthink\u001b[0m a tend to \u001b[31mget\u001b[0m myself in trouble My blood \u001b[31mpressure\u001b[0m doubles An anxiety is coupled With not so subtle hint to try to tell me What \u001b[31mI'm\u001b[0m doing isn't healthy When \u001b[31mpeople\u001b[0m try to help I just tell them \u001b[31m\"go\u001b[0m to hell\" Let me worry about myself\" https://www.youtube.com/watch?v=wuxysQ-qCAI In the weird \u001b[31mearly\u001b[0m \u001b[31mdays\u001b[0m of aughts there came a \u001b[31mgroup\u001b[0m Cunninlynguists, with a young producer \u001b[31mname\u001b[0m Kno. The Cunninlynguists are a very capable \u001b[31mgroup\u001b[0m (check out all of their stuff, their \u001b[31malbum\u001b[0m Dirty Acres is \u001b[31mone\u001b[0m of my \u001b[31mfavorite\u001b[0m albums) but their standout is the production from Kno. Over the years both the rappers and producer grew and \u001b[31mgot\u001b[0m \u001b[31mbetter\u001b[0m at their craft. Kno decided to \u001b[31mwork\u001b[0m on his solo \u001b[31malbum\u001b[0m with a theme. The theme of course was death. Now it \u001b[31mmay\u001b[0m not \u001b[31mseem\u001b[0m that an \u001b[31malbum\u001b[0m about death would be \u001b[31msomething\u001b[0m you would \u001b[31mwant\u001b[0m to listen to when you are depressed, but there is a beauty in it. For an \u001b[31malbum\u001b[0m about death there is a surprising amount of positivity sprinkled in the lyrics. Maybe its just the excellent \u001b[31mbeats\u001b[0m Kno crafted but there is \u001b[31msomething\u001b[0m comforting to me about listening to this \u001b[31malbum\u001b[0m about the cousin to sleep. Deacon - \"(They told me) not to fear \u001b[31mliving\u001b[0m for eternity But, not \u001b[31meven\u001b[0m Heaven \u001b[31mseemed\u001b[0m pleasant it was burning me Raised with peasants in the crescents we were shadows When reality television was \u001b[31mwatching\u001b[0m adults And, throughout the comedy was drama and \u001b[31malot\u001b[0m of pain And that \u001b[31msentence\u001b[0m it would run on 'til the comma came Until the eulogy and \u001b[31musually\u001b[0m my mama sang About the pheasants and presence we no longer claim (they told me) that Heaven is forever But at \u001b[31mtimes\u001b[0m I'd \u001b[31mfind\u001b[0m myself thinking I'd rather never Lived, I \u001b[31mmean,\u001b[0m my \u001b[31mlife\u001b[0m was \u001b[31mlike\u001b[0m a dream I had \u001b[31meverything\u001b[0m I wanted but, that ain't all it seems\" https://www.youtube.com/watch?v=1HeikQGmcpM As a little off topic, \u001b[31msince\u001b[0m you are familiar with Eminem, I was wondering if you were familiar with fellow Detroit emcee Royce Da 5'9, thats on a few of Em's songs. He released an \u001b[31malbum\u001b[0m with \u001b[31mone\u001b[0m of the greatest hiphop producers of all \u001b[31mtime\u001b[0m DJ Premier a few months \u001b[31mback.\u001b[0m \u001b[31mMay\u001b[0m end up my \u001b[31malbum\u001b[0m of the year. https://www.youtube.com/watch?v=zRZDVhs9xWc I hope this \u001b[31mpost\u001b[0m maybe gives you more music to help you \u001b[31m\"get\u001b[0m past \u001b[31mshit.\"\u001b[0m I am not \u001b[31msure\u001b[0m why I was compelled to \u001b[31mwrite\u001b[0m this \u001b[31mpost\u001b[0m the \u001b[31mway\u001b[0m I did, but maybe at the very least you \u001b[31mfind\u001b[0m some more hiphop you \u001b[31mlike.\u001b[0m TLDR; This \u001b[31mpost\u001b[0m is long, \u001b[31msorry.\u001b[0m .\n",
      "===\n",
      "comment_id: 53965 stopsmoking topic_id: 28 \n",
      "keywords: smoking, quit, smoke, addiction, steps, currently, gaming, goals, nicotine, allow\n",
      "---\n",
      "I have been \u001b[31msmoking\u001b[0m a pack a \u001b[31mday\u001b[0m since my early 20s. I'm 45 now. I \u001b[31mquit\u001b[0m 309 \u001b[31mdays\u001b[0m ago, and I will never touch another cigarette again. It's been the best decision in my life. This sub did it for me. The short version: you're addicted to \u001b[31mnicotine,\u001b[0m the \u001b[31maddiction\u001b[0m is real, junkie-you will tell yourself anything to get \u001b[31mback\u001b[0m to \u001b[31msmoking,\u001b[0m don't do it. It may seem difficult now, but you basically have a \u001b[31mnicotine\u001b[0m hangover. It will go away if you don't \u001b[31msmoke\u001b[0m (stay sober). It's worth it, \u001b[31msmoking\u001b[0m is silly, you don't need \u001b[31mnicotine.\u001b[0m The long version: Here are my thoughts on quitting, rehashed from a post I made when I was 30 \u001b[31mdays\u001b[0m in. I \u001b[31mknow\u001b[0m there are other thoughts, but these are mine. **How / when to \u001b[31mquit:**\u001b[0m * Quitting is easy, just stop \u001b[31msmoking.\u001b[0m \u001b[31mGetting\u001b[0m the \u001b[31mnicotine\u001b[0m out of your body only takes three \u001b[31mdays.\u001b[0m * Cold turkey is the best way to \u001b[31mquit.\u001b[0m \u001b[31mGum,\u001b[0m patches, \u001b[31mecigs,\u001b[0m just \u001b[31mprolong\u001b[0m the suffering and keep you addicted to, and thinking about, \u001b[31mnicotine\u001b[0m for a long time. They \u001b[31malso\u001b[0m increase the chance of relapse. I failed many times to \u001b[31mquit\u001b[0m using NRT. * However, everything is better than \u001b[31msmoking,\u001b[0m so if NRT does \u001b[31mhelp\u001b[0m you to stop, or if it shows you that you'll be fine without \u001b[31msmoking,\u001b[0m or if it will \u001b[31mhelp\u001b[0m you get rid of the mental \u001b[31maddiction\u001b[0m to \u001b[31mcigarettes,\u001b[0m you should do it. Just remember that if you fail to \u001b[31mquit\u001b[0m that way, cold turkey is always an option * Don't be afraid if you \u001b[31mquit.\u001b[0m The panic is just the \u001b[31maddict\u001b[0m inside of you going without the \u001b[31mnicotine.\u001b[0m It will go away very soon once you realize the freedom you just gained. * Remember that \u001b[31mnicotine\u001b[0m is just a poison, not a \u001b[31mnutrient.\u001b[0m You'll be fine without it. * Don't make a \u001b[31mritual\u001b[0m out of your \u001b[31mquit.\u001b[0m Don't \u001b[31mset\u001b[0m a date to \u001b[31mquit,\u001b[0m just do it today. Setting a date will only give you time to come up with reasons to not do it. * There's no such thing as a bad time to \u001b[31mquit.\u001b[0m It doesn't exist. Now is the best time. Anything else is just the \u001b[31mnicotine\u001b[0m junkie talking. Don't let it come up with a real good reason to not \u001b[31mquit,\u001b[0m or not \u001b[31mquit\u001b[0m right now. See the junkie for who he/she is. * Have that last \u001b[31msmoke\u001b[0m if you \u001b[31mwant\u001b[0m to, but realize you're not giving something up of value. You're just no longer \u001b[31msystematically\u001b[0m choking yourself. **Get support and \u001b[31mtrack\u001b[0m your progress** * Get support, and burn all bridges. Get a badge here, tell your story. Tell family, \u001b[31mfriends,\u001b[0m colleagues and ask for their support. Hang a X \u001b[31mdays\u001b[0m smoke-free paper on your work desk. Bring cake to work after being \u001b[31mnicotine\u001b[0m free for two weeks / one month. Tattoo your \u001b[31mquit\u001b[0m date on your forehead. * Tell the people where you buy your smokes that you have \u001b[31mquit,\u001b[0m and that under no circumstance should they sell you any more. * Download an app to see how many \u001b[31mdays,\u001b[0m hours, mins, and secs have passed since your last \u001b[31msmoke.\u001b[0m \u001b[31mTrack\u001b[0m the number of \u001b[31mcigarettes\u001b[0m not \u001b[31msmoked,\u001b[0m and money saved. * Read up on health benefits of not \u001b[31msmoking,\u001b[0m so you understand what to expect and when. * I made a wager with everyone that wanted one. I will pay 10 to 1 if I \u001b[31msmoke\u001b[0m again within a year. If I do, I will loose \u001b[31mapproximately\u001b[0m 5000 dollars. If I don't I will win 500 dollars (for charity). * I mean you're serious about quitting, right? You're not just \"trying to \u001b[31mquit\",\u001b[0m but you will be a non-smoker. \u001b[31mDecide\u001b[0m how serious you are, and burn those bridges, and then \u001b[31mnuke\u001b[0m them to make sure. **The detox phase** * If you go cold turkey, it should only last a \u001b[31mcouple\u001b[0m of \u001b[31mdays.\u001b[0m First \u001b[31mcouple\u001b[0m of \u001b[31mdays\u001b[0m can be a \u001b[31mlittle\u001b[0m annoying, but it will get easier very soon. * Cravings, should you have them (I didn't this time), only last 3 mins max. Watch a \u001b[31mtimer\u001b[0m on your phone if you have to. \u001b[31mEvery\u001b[0m craving is a reminder of how awesome you are to have stopped. * Drink lots of fruit juice the first \u001b[31mcouple\u001b[0m of \u001b[31mdays,\u001b[0m breathe. You may feel down a \u001b[31mlittle.\u001b[0m Don't let it get to you. * Don't keep \u001b[31mcigarettes\u001b[0m around, and avoid \u001b[31msmokers\u001b[0m and strong triggers \u001b[31mlike\u001b[0m alcohol until you \u001b[31mknow\u001b[0m you'll be fine around them. * Don't listen to yourself talking you into \u001b[31msmoking\u001b[0m again. This includes: \"now is not a good time to stop\", \"it's too difficult for me\", \"I can't stop this way\", \"I can just cut \u001b[31mback\u001b[0m slowly\", \"I should have just one to make me feel better\", \"I will \u001b[31mquit\u001b[0m another time\". etc. Junkie \u001b[31malarm,\u001b[0m typical \u001b[31maddict\u001b[0m behavior to think \u001b[31mlike\u001b[0m that! * Stop being a wuss, and just get it over with. Stay in bed eating ice cream if you \u001b[31mwant\u001b[0m to, but don't \u001b[31msmoke.\u001b[0m **The battle inside your head** * The biggest battle is in the mind, not in the detox. Make sure you really \u001b[31mwant\u001b[0m to \u001b[31mquit\u001b[0m before you do it. Make sure you're ready to fight the \u001b[31maddict\u001b[0m inside of you. * \u001b[31mKnow\u001b[0m that \u001b[31msmoking\u001b[0m is an \u001b[31maddiction\u001b[0m and not a \u001b[31mhabit.\u001b[0m Realize that your own brain is trying to trick you into believing you need to, or you can, \u001b[31msmoke\u001b[0m again. * All people that \u001b[31msmoke\u001b[0m on a regular basis are \u001b[31mnicotine\u001b[0m junkies. Even those that say they can go for \u001b[31mdays\u001b[0m without. There's no so thing as casual \u001b[31msmoking,\u001b[0m social \u001b[31msmoking,\u001b[0m not buying your own smokes, or just \u001b[31msmoking\u001b[0m when you drink beer. That's just the junkie talking. These things aren't real, they're just excuses for \u001b[31msmoking.\u001b[0m * Understand that \u001b[31msmoking\u001b[0m brings nothing of value to you. It doesn't relax you, it doesn't \u001b[31mhelp\u001b[0m with concentration, it doesn't smell or taste good, it doesn't even give you a high. It just fills the hole left by the last cigarette. Watch the \u001b[31mAlan\u001b[0m \u001b[31mCarr\u001b[0m video (google it). * Realize fully that you're not giving something up, you're not sacrificing something by not \u001b[31msmoking.\u001b[0m This is important. Don't romanticize the smokes, don't become \u001b[31msentimental\u001b[0m about your \u001b[31mnicotine\u001b[0m \u001b[31maddiction.\u001b[0m That's all just typical junkie talk. * Read up on all benefits of being a non-smoker. * Be a non-smoker, and do and feel all the things non-smokers do and feel. Non-smokers would never reach for a \u001b[31msmoke\u001b[0m if they feel bad, stressed, good, bored. * You're a non smoker. You have really stopped. Never have that first cigarette. Now that you're a non-smoker, don't start \u001b[31msmoking.\u001b[0m \u001b[31mNicotine\u001b[0m is highly \u001b[31maddictive\u001b[0m (more so than crack and \u001b[31mheroin),\u001b[0m and one cigarette can/will get you hooked again. Don't make the same mistake you made when you first started. * Three options: (1) \u001b[31msmoke\u001b[0m just one more and be a smoker for the rest of your live, (2) \u001b[31msmoke\u001b[0m just one more and go through detox / cravings again, and (3) don't have even just one. You chose which one you \u001b[31mwant.\u001b[0m * When you're feeling good, don't become arrogant and think you've got this or that quitting is easy for you. You can't just have one, not even when you're feeling you're in control, because you never are with something as \u001b[31maddictive\u001b[0m as \u001b[31mnicotine.\u001b[0m Remember what it was \u001b[31mlike\u001b[0m to be a junkie. Thanks all , goodbye and I hope that you all \u001b[31msucceed\u001b[0m and lead long, happy, and healthy lives.\n",
      "===\n",
      "comment_id: 642 alcoholism topic_id: 41 \n",
      "keywords: peace, focused, cost, paying, attention, trauma, expectations, acceptance, breaks, unhealthy\n",
      "---\n",
      "That's what is great about this subreddit. We don't have a lot of mobs coming in a wanting to watch the world burn. Most of the repeat participants have healthy conversations. I \u001b[31mreally\u001b[0m \u001b[31mlike\u001b[0m what you typed. You touch a couple of \u001b[31mthings\u001b[0m that a lot say and that I have said, and I \u001b[31mwant\u001b[0m to give a few \u001b[31mpoints\u001b[0m for those who may be on the \u001b[31mfence.\u001b[0m ----- AA is free, and they \u001b[31mtake\u001b[0m anonymity \u001b[31mserious.\u001b[0m So, if you are on the \u001b[31mfence\u001b[0m and you have a \u001b[31mproblem\u001b[0m with alcohol or drugs, go to a meeting on the other side of town and use a fake name. It won't \u001b[31mcost\u001b[0m you anything. Also, go to a SMART Recovery meeting. Then go to more. Each meeting and each group is different. AA has a proven track record. AA works enough that it self perpetuates, has continued to do so for 75 plus years, and continues to have \u001b[31mgroups\u001b[0m nationwide and worldwide. Sure, it isn't everything to \u001b[31meveryone,\u001b[0m but it works for some \u001b[31mpeople.\u001b[0m It works enough to continue to exist even in today's world. AA is no \u001b[31mcost\u001b[0m because it is a completely volunteer organization. \u001b[31mPeople\u001b[0m who have found AA helpful, come back. They volunteer and make themselves available to \u001b[31mpeople\u001b[0m who \u001b[31mwant\u001b[0m to \u001b[31mtry\u001b[0m \u001b[31msomething\u001b[0m different. There are thousands of \u001b[31mgroups\u001b[0m all over the world because volunteers offer their free time to help \u001b[31mpeople\u001b[0m \u001b[31mlike\u001b[0m themselves. Now. These next three \u001b[31mpoints\u001b[0m drive \u001b[31mpeople\u001b[0m nuts. Most of us cannot explain it \u001b[31mlet\u001b[0m alone understand it. 1) The Big Book, the book that AA uses to help an individual get sober, is an average self help book written by an average human. 2) AA is structured to so that average volunteers can run their \u001b[31mgroups\u001b[0m based on their average \u001b[31minterpretation\u001b[0m of that book. 3) There is no specific central governance and each group is \u001b[31mbasically\u001b[0m autonomous, volunteers running their \u001b[31mgroups\u001b[0m as \u001b[31mbest\u001b[0m as average \u001b[31mpeople\u001b[0m can: following an average book written by an average \u001b[31mperson.\u001b[0m That's the rub. Average \u001b[31mpeople\u001b[0m helping. ---- Now. I understand more than my words are showing how \u001b[31mflawed\u001b[0m AA is. Yet I do not grok how \u001b[31mpeople\u001b[0m criticize AA as they \u001b[31mwould\u001b[0m criticize an scientific knowledge based organization or system. They are totally different types of organizations. Yes. Everything has to be \u001b[31mexamined,\u001b[0m but I can rarely imagine how a close examination of AA, NA, CA, MA, \u001b[31mSA,\u001b[0m GA, OA, \u001b[31met\u001b[0m al merits \u001b[31mlengthy\u001b[0m critiques or analyses once its true nature is \u001b[31mrealized.\u001b[0m AA and all of these organizations are volunteer: volunteers of average humans. They are run by volunteer average humans. The system is based on a \u001b[31msomething\u001b[0m developed by volunteer average humans. Of \u001b[31mcourse\u001b[0m it is going to have problems.. Then AA, NA, CA, MA, \u001b[31met\u001b[0m al get criticized that they do not do more, do not lead the field, do not change fast enough, they aren't culpable, \u001b[31met\u001b[0m al. But they do all of these \u001b[31mthings.\u001b[0m They just do them as tiny independent \u001b[31mgroups.\u001b[0m AA, NA, CA, \u001b[31met\u001b[0m al. are not big organizations that \u001b[31mpeople\u001b[0m think they are. They are structured to avoid being a proactive big, national, world wide organization. And this is a common misunderstanding. Sure, there are other organizations that do those \u001b[31mthings:\u001b[0m National Council on Drugs and Alcohol and more. AA is a group name for thousands of tiny local \u001b[31mgroups.\u001b[0m Then, I get real again. This issue isn't academic or \u001b[31msomething\u001b[0m just on a chalkboard. Addiction and chemical abuse is an \u001b[31menormous\u001b[0m \u001b[31mproblem.\u001b[0m It is horribly fatal to the users and \u001b[31mpeople\u001b[0m around them. The numbers of \u001b[31mpeople\u001b[0m dying \u001b[31mevery\u001b[0m day are frigging horrible. AA and the others are trying to help. Shouldn't they get a bit of slack? I \u001b[31mmean,\u001b[0m they are no \u001b[31mcost.\u001b[0m \u001b[31mPeople\u001b[0m forget to stop and think about that. NO \u001b[31mCOST.\u001b[0m All of the science based solutions \u001b[31mcost.\u001b[0m Addiction and alcohol has a tendency to damage people's economic position or power. AA is no \u001b[31mcost.\u001b[0m But, I can understand a lot of the issues. AA is run by average \u001b[31mpeople\u001b[0m a majority have no college education or organizational experience. In my 12 plus years of going in and out of the rooms, I have never experienced many of your \u001b[31mpoints\u001b[0m \"supremcy of AA\", \"unwilling to change or address problems in the organization\", or \"blame game\". I have been lucky to be around AA \u001b[31mpeople\u001b[0m that \u001b[31mknow\u001b[0m and share and promote our \u001b[31mimperfect\u001b[0m solution. Yet, I \u001b[31mknow\u001b[0m those issues exist in other \u001b[31mgroups\u001b[0m or areas. Again, I \u001b[31mlike\u001b[0m what you typed. Usually, I don't respond to these issues, because \u001b[31mlike\u001b[0m you typed, \"diminishing \u001b[31mreturns\",\u001b[0m but I step into the waters sometimes. AA and all the anonymous \u001b[31mgroups\u001b[0m are \u001b[31mflawed,\u001b[0m but they are volunteer organizations of average humans trying to help average humans who are having \u001b[31mBAD\u001b[0m and FATAL issues. They are not for \u001b[31meveryone,\u001b[0m but they have a solution that is free and that works for enough \u001b[31mpeople\u001b[0m that it is still here today. If you are having a \u001b[31mproblem,\u001b[0m give them a \u001b[31mtry.\u001b[0m It certainly won't \u001b[31mcost\u001b[0m you anything.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "p = 95 # percentile of words to highlight \n",
    "topics_to_view = [6, 25, 28, 41] \n",
    "for topic_id in topics_to_view:\n",
    "    \n",
    "    # find the comment that most expresses that topic\n",
    "    comment_id = doctopic[:,topic_id].argmax()\n",
    "    \n",
    "    # find most prevalent words in the most prevalent topic\n",
    "    topic = lda.components_[topic_id]\n",
    "    feat_idx = np.argsort(topic)[::-1]\n",
    "    num_words = (topic > np.percentile(topic, p)).sum()\n",
    "    feat_idx = feat_idx[0:num_words]\n",
    "    top_features = [features_all[i] for i in feat_idx]\n",
    "    \n",
    "    # create highlighted version of the comment \n",
    "    comment_color = []\n",
    "    comment = corpus_all[comment_id].split()\n",
    "    \n",
    "    for word in comment: \n",
    "        if re.sub(RE_PREPROCESS, '', word).lower() in top_features:\n",
    "            comment_color.append('\\x1b[31m' + word + '\\x1b[0m') # makes the word print in red \n",
    "        else:\n",
    "            comment_color.append(word)\n",
    "    \n",
    "    print('comment_id:',\n",
    "          comment_id,\n",
    "          subreddit_id_all[comment_id],\n",
    "          'topic_id:',\n",
    "          topic_id,\n",
    "          '\\nkeywords:',\n",
    "           ls_keywords[topic_id])\n",
    "    print('---')\n",
    "    print(' '.join(comment_color))\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cool Visualizations \n",
    "    \n",
    "<img src=\"figs/supreme_court_topics.jpg\"/> \n",
    "\n",
    "[Mental Health Subreddit Topics](http://geebioso.github.io/Subreddit-Topic-Visualization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Return to [TOC](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.89865984\n"
     ]
    }
   ],
   "source": [
    "print_memory() # prints memory usage in Gb "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
